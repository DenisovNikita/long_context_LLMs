{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fd1ea6-4c85-457f-8dcc-f3e3a696ee96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdea8f-48ea-49ee-a487-5bf5de98deb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5998e0e-0188-4d18-88b6-a634a5d69ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T12:00:33.501102Z",
     "iopub.status.busy": "2024-05-12T12:00:33.500452Z",
     "iopub.status.idle": "2024-05-12T12:00:33.516512Z",
     "shell.execute_reply": "2024-05-12T12:00:33.515779Z",
     "shell.execute_reply.started": "2024-05-12T12:00:33.501074Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"`do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ab3461b-a57e-4bf5-b9a2-6f5438346c6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T12:00:35.810821Z",
     "iopub.status.busy": "2024-05-12T12:00:35.810191Z",
     "iopub.status.idle": "2024-05-12T12:00:36.376679Z",
     "shell.execute_reply": "2024-05-12T12:00:36.376015Z",
     "shell.execute_reply.started": "2024-05-12T12:00:35.810794Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /tmp/xdg_cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "login(os.environ['hf-read-token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb725155-72a6-403d-9c18-e1da14092bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T12:00:38.147247Z",
     "iopub.status.busy": "2024-05-12T12:00:38.146625Z",
     "iopub.status.idle": "2024-05-12T12:00:56.429110Z",
     "shell.execute_reply": "2024-05-12T12:00:56.428308Z",
     "shell.execute_reply.started": "2024-05-12T12:00:38.147219Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../utils\")\n",
    "from definitions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c446f708-d8ae-415e-bc88-cc77e234a2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T12:00:56.431162Z",
     "iopub.status.busy": "2024-05-12T12:00:56.430407Z",
     "iopub.status.idle": "2024-05-12T12:00:56.445189Z",
     "shell.execute_reply": "2024-05-12T12:00:56.444495Z",
     "shell.execute_reply.started": "2024-05-12T12:00:56.431131Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(\"../../../../cache/\")\n",
    "DATASET_DIR = Path(\"/home/jupyter/mnt/datasets/diplomas/russian_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db3f5d5d-9db1-4862-8308-3b80e2dfdd56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:27:59.769572Z",
     "iopub.status.busy": "2024-05-11T10:27:59.769265Z",
     "iopub.status.idle": "2024-05-11T10:27:59.779092Z",
     "shell.execute_reply": "2024-05-11T10:27:59.778544Z",
     "shell.execute_reply.started": "2024-05-11T10:27:59.769552Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_EXPERIMENT_NAME = \"learnt_16k\"\n",
    "\n",
    "MODEL_MAX_LENGTH = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6e454b1-e40f-4cce-bad9-aec790a0d10a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T12:00:56.446491Z",
     "iopub.status.busy": "2024-05-12T12:00:56.446196Z",
     "iopub.status.idle": "2024-05-12T12:00:56.456740Z",
     "shell.execute_reply": "2024-05-12T12:00:56.456185Z",
     "shell.execute_reply.started": "2024-05-12T12:00:56.446472Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../../LongLoRA-diploma-research\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5567796b-dd01-4aaf-a49c-a49a14f0a068",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Prompt Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a41412f-7e20-476c-8c6d-6ce1f74791c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:33:44.892400Z",
     "iopub.status.busy": "2024-05-11T10:33:44.892085Z",
     "iopub.status.idle": "2024-05-11T10:33:44.901611Z",
     "shell.execute_reply": "2024-05-11T10:33:44.901092Z",
     "shell.execute_reply.started": "2024-05-11T10:33:44.892381Z"
    }
   },
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input_llama2\":(\n",
    "        \"[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\"\n",
    "        \"<</SYS>> \\n\\n {instruction} [/INST]\"\n",
    "    ),\n",
    "    \"prompt_input_llama2\": (\n",
    "        \"[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\"\n",
    "        \"<</SYS>> \\n\\n {instruction} \\n{input} [/INST]\"\n",
    "    ),\n",
    "    \"prompt_llama2\": \"[INST]{instruction}[/INST]\",\n",
    "    \"prompt_input_diploma_special\":(\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\nBelow is a diploma text. Your task is to generate abstract of this diploma.\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f34b3f-f3e1-4375-a566-dcec395755d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71bcadb8-237e-4220-9439-168c392f394d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T22:22:38.996921Z",
     "iopub.status.busy": "2024-05-11T22:22:38.995914Z",
     "iopub.status.idle": "2024-05-11T22:23:52.495437Z",
     "shell.execute_reply": "2024-05-11T22:23:52.494227Z",
     "shell.execute_reply.started": "2024-05-11T22:22:38.996873Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n",
      "Loaded tokenizer\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[LLAMA_2_7B], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    # config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[LLAMA_2_7B],\n",
    "    cache_dir=CACHE_DIR,\n",
    "    # model_max_length=MODEL_MAX_LENGTH,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "print(\"Loaded tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10864846-5a3e-41b7-85e1-deeedffa6036",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Load peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e56e8e-d59f-4c38-b0b1-489d83518a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T22:09:06.522410Z",
     "iopub.status.busy": "2024-05-10T22:09:06.521763Z",
     "iopub.status.idle": "2024-05-10T22:13:55.627520Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 22:09:11.677550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [03:40<00:00, 110.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "from llama_attn_replace_sft import replace_llama_attn\n",
    "from gptneox_attn_replace import replace_gpt_neox_attn\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.distributed import barrier\n",
    "\n",
    "\n",
    "model_name = LLAMA_2_7B\n",
    "\n",
    "replace_llama_attn(True, False, inference=True)\n",
    "\n",
    "# Set RoPE scaling factor\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name],\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "\n",
    "orig_rope_scaling = getattr(config, \"rope_scaling\", None)\n",
    "if orig_rope_scaling is None:\n",
    "    orig_rope_scaling = {\"factor\": 1}\n",
    "orig_rope_scaling_factor = orig_rope_scaling[\"factor\"] if \"factor\" in orig_rope_scaling.keys() else 1\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len:\n",
    "    orig_ctx_len *= orig_rope_scaling_factor\n",
    "    if MODEL_MAX_LENGTH > orig_ctx_len:\n",
    "        scaling_factor = float(math.ceil(MODEL_MAX_LENGTH / orig_ctx_len))\n",
    "        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "\n",
    "print(\"Created config\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name],\n",
    "    cache_dir=CACHE_DIR,\n",
    "    model_max_length=MODEL_MAX_LENGTH,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "print(\"Loaded tokenizer\")\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "\n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "# if training_args.low_rank_training:\n",
    "#     if model_args.model_type == \"gpt-neox\":\n",
    "#         # added `dense` to match with llama as the basic LoRA would only target 'query_key_value'\n",
    "#         targets = [\"query_key_value\", \"dense\"]\n",
    "#     else:\n",
    "#         targets=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "#     config = LoraConfig(\n",
    "#         r=8,\n",
    "#         lora_alpha=16,\n",
    "#         target_modules=targets,\n",
    "#         lora_dropout=0,\n",
    "#         bias=\"none\",\n",
    "#         task_type=\"CAUSAL_LM\",\n",
    "#     )\n",
    "#     model = get_peft_model(model, config)\n",
    "#     # enable trainable params\n",
    "#     [p.requires_grad_() for n, p in model.named_parameters() if any([k in n for k in training_args.trainable_params.split(\",\")])]\n",
    "\n",
    "# model.config.use_cache = False         # required for gradient checkpointing\n",
    "# model.enable_input_require_grads()     # required for gradient checkpointing\n",
    "# model.gradient_checkpointing_enable()  # enable gradient checkpointing\n",
    "\n",
    "# print(\"Prepared model to learn\")\n",
    "\n",
    "###\n",
    "\n",
    "# model_name = LLAMA_2_7B\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "#     cache_dir=CACHE_DIR, \n",
    "#     device_map='auto'\n",
    "# )\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model.to(device)\n",
    "\n",
    "# MODEL_MAX_LENGTH = 16384\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "#     cache_dir=CACHE_DIR, \n",
    "#     model_max_length=MODEL_MAX_LENGTH,\n",
    "#     padding_side=\"right\",\n",
    "#     use_fast=True\n",
    "# )\n",
    "\n",
    "# IGNORE_INDEX = -100\n",
    "# DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "# DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "# DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "# DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "# def smart_tokenizer_and_embedding_resize(\n",
    "#     special_tokens_dict: Dict,\n",
    "#     tokenizer: transformers.PreTrainedTokenizer,\n",
    "#     model: transformers.PreTrainedModel,\n",
    "# ):\n",
    "#     \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "#     Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "#     \"\"\"\n",
    "#     num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "#     model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#     if num_new_tokens > 0:\n",
    "#         input_embeddings = model.get_input_embeddings().weight.data\n",
    "#         output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "#         input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "#         output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "#         input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "#         output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "# special_tokens_dict = dict()\n",
    "# if tokenizer.pad_token is None:\n",
    "#     special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "# if tokenizer.eos_token is None:\n",
    "#     special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "# if tokenizer.bos_token is None:\n",
    "#     special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "# if tokenizer.unk_token is None:\n",
    "#     special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "# smart_tokenizer_and_embedding_resize(\n",
    "#     special_tokens_dict=special_tokens_dict,\n",
    "#     tokenizer=tokenizer,\n",
    "#     model=model,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56302f6-7c25-43f2-bf2b-0d4aa811931c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T22:13:55.630010Z",
     "iopub.status.busy": "2024-05-10T22:13:55.629229Z",
     "iopub.status.idle": "2024-05-10T22:14:02.517727Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "        \n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=special_tokens_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001686d-9a4b-4eb5-8db6-1bd51622d329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T22:14:02.519226Z",
     "iopub.status.busy": "2024-05-10T22:14:02.518969Z",
     "iopub.status.idle": "2024-05-10T22:14:22.669158Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading adapter_config.json: 100%|██████████| 674/674 [00:00<00:00, 1.75MB/s]\n",
      "Downloading adapter_model.bin: 100%|██████████| 541M/541M [00:08<00:00, 60.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"nvdenisov2002/llama-longLoRA-v3-16k-5000-samples-220-iterations\"\n",
    "peft_model = PeftModel.from_pretrained(model, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47199b-36fd-4a22-b2c2-ace036f3abff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference model on tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1ca3ac-8666-406e-b551-49d6c151fa8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Try inference smth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae6e8d60-326e-438d-b6cf-a12b5ae65e95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:51:48.256514Z",
     "iopub.status.busy": "2024-05-11T10:51:48.256125Z",
     "iopub.status.idle": "2024-05-11T10:51:48.266060Z",
     "shell.execute_reply": "2024-05-11T10:51:48.265518Z",
     "shell.execute_reply.started": "2024-05-11T10:51:48.256490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kek\n"
     ]
    }
   ],
   "source": [
    "print(\"kek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ba9c1af-1004-4240-9dcb-2b220e528ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:14:16.057275Z",
     "iopub.status.busy": "2024-05-11T12:14:16.056667Z",
     "iopub.status.idle": "2024-05-11T12:14:16.070190Z",
     "shell.execute_reply": "2024-05-11T12:14:16.069604Z",
     "shell.execute_reply.started": "2024-05-11T12:14:16.057249Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"How to write diploma work?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "275099ab-394b-45cc-90c3-9fe23dad1224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:30:17.099060Z",
     "iopub.status.busy": "2024-05-11T12:30:17.098482Z",
     "iopub.status.idle": "2024-05-11T12:30:17.112726Z",
     "shell.execute_reply": "2024-05-11T12:30:17.112170Z",
     "shell.execute_reply.started": "2024-05-11T12:30:17.099036Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.use_default_system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "999cb904-e920-40f8-871e-94749782a60b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:30:25.651252Z",
     "iopub.status.busy": "2024-05-11T12:30:25.650665Z",
     "iopub.status.idle": "2024-05-11T12:30:25.660817Z",
     "shell.execute_reply": "2024-05-11T12:30:25.660235Z",
     "shell.execute_reply.started": "2024-05-11T12:30:25.651227Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(text, return_tensors=\"pt\").to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f37b9d61-0e25-45d8-8fb0-c333e89eafed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:30:27.049804Z",
     "iopub.status.busy": "2024-05-11T12:30:27.049125Z",
     "iopub.status.idle": "2024-05-11T12:30:27.065556Z",
     "shell.execute_reply": "2024-05-11T12:30:27.064993Z",
     "shell.execute_reply.started": "2024-05-11T12:30:27.049774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  1128,   304,  2436,   652,   572,  4125,   664, 29973]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed655ff6-36e9-42a1-85d0-f0d0fd1ec4a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:31:01.709274Z",
     "iopub.status.busy": "2024-05-11T12:31:01.708642Z",
     "iopub.status.idle": "2024-05-11T12:31:01.727116Z",
     "shell.execute_reply": "2024-05-11T12:31:01.726548Z",
     "shell.execute_reply.started": "2024-05-11T12:31:01.709237Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> How to write diploma work?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(model_inputs[\"input_ids\"].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a219f93f-d7c6-4b15-b2ad-246d4e4096a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:17:25.525827Z",
     "iopub.status.busy": "2024-05-11T12:17:25.525154Z",
     "iopub.status.idle": "2024-05-11T12:17:25.542148Z",
     "shell.execute_reply": "2024-05-11T12:17:25.541553Z",
     "shell.execute_reply.started": "2024-05-11T12:17:25.525789Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = transformers.GenerationConfig(max_new_tokens=512, max_time=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dffa4759-3a9a-42e4-822d-91848c709f09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:24:21.664012Z",
     "iopub.status.busy": "2024-05-11T12:24:21.663231Z",
     "iopub.status.idle": "2024-05-11T12:24:36.831443Z",
     "shell.execute_reply": "2024-05-11T12:24:36.830021Z",
     "shell.execute_reply.started": "2024-05-11T12:24:21.663982Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generated = model.generate(**model_inputs, generation_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "746e96dc-def4-4409-a897-649fc66f224c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:24:36.832862Z",
     "iopub.status.busy": "2024-05-11T12:24:36.832429Z",
     "iopub.status.idle": "2024-05-11T12:24:36.846525Z",
     "shell.execute_reply": "2024-05-11T12:24:36.845985Z",
     "shell.execute_reply.started": "2024-05-11T12:24:36.832832Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 105])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d3ec904-65ff-4b50-ba45-fa2a6cc79e84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:24:36.847787Z",
     "iopub.status.busy": "2024-05-11T12:24:36.847422Z",
     "iopub.status.idle": "2024-05-11T12:24:36.874269Z",
     "shell.execute_reply": "2024-05-11T12:24:36.873745Z",
     "shell.execute_reply.started": "2024-05-11T12:24:36.847768Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> How to write diploma work?\n",
      "The diploma work is a scientific research work, which is written by the student in the last year of study. The diploma work is a mandatory part of the educational program. The diploma work is a scientific research work, which is written by the student in the last year of study. The diploma work is a mandatory part of the educational program.\n",
      "The diploma work is a scientific research work, which is written by the\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cded816-742e-4a21-9224-ed9773baeaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cfd6a19-e0e0-43e4-9882-b0a898b6e6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:37:35.444810Z",
     "iopub.status.busy": "2024-05-11T10:37:35.444159Z",
     "iopub.status.idle": "2024-05-11T10:51:48.222236Z",
     "shell.execute_reply": "2024-05-11T10:51:48.221105Z",
     "shell.execute_reply.started": "2024-05-11T10:37:35.444782Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PROMPT_DICT[\"prompt_no_input_llama2\"].format(instruction=text)\n",
    "prompt_tokens = torch.tensor(tokenizer(prompt)[\"input_ids\"]).to(device)\n",
    "generated_tokens = model.generate(input_ids=prompt_tokens.reshape((1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "405fc380-b0bd-4b13-a064-268313450de5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:04:37.249058Z",
     "iopub.status.busy": "2024-05-11T12:04:37.248425Z",
     "iopub.status.idle": "2024-05-11T12:04:37.258460Z",
     "shell.execute_reply": "2024-05-11T12:04:37.257861Z",
     "shell.execute_reply.started": "2024-05-11T12:04:37.249023Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e52704c-3bd6-4ddb-a570-15458bc11cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:10:21.423060Z",
     "iopub.status.busy": "2024-05-11T12:10:21.422417Z",
     "iopub.status.idle": "2024-05-11T12:10:21.440183Z",
     "shell.execute_reply": "2024-05-11T12:10:21.439581Z",
     "shell.execute_reply.started": "2024-05-11T12:10:21.423024Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>> \n",
      "\n",
      " How to write diploma work? [/INST]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(prompt_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e06bbbe2-0ae4-426d-b0d8-1e53714f1a2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:02:39.862943Z",
     "iopub.status.busy": "2024-05-11T12:02:39.862257Z",
     "iopub.status.idle": "2024-05-11T12:02:39.886352Z",
     "shell.execute_reply": "2024-05-11T12:02:39.885728Z",
     "shell.execute_reply.started": "2024-05-11T12:02:39.862908Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_tokens.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ddf886d-9ffc-4dfe-9cc7-6cd7ddcd81a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:04:14.278313Z",
     "iopub.status.busy": "2024-05-11T12:04:14.277613Z",
     "iopub.status.idle": "2024-05-11T12:04:14.288926Z",
     "shell.execute_reply": "2024-05-11T12:04:14.288311Z",
     "shell.execute_reply.started": "2024-05-11T12:04:14.278277Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_part = tokenizer.decode(generated_tokens.flatten()[:200])\n",
    "decoded_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60a7b30e-c79c-4178-857d-2e3701557925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T10:51:48.242121Z",
     "iopub.status.busy": "2024-05-11T10:51:48.241741Z",
     "iopub.status.idle": "2024-05-11T10:51:48.255288Z",
     "shell.execute_reply": "2024-05-11T10:51:48.254714Z",
     "shell.execute_reply.started": "2024-05-11T10:51:48.242095Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> [INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>> \\n\\n How to write diploma work? [/INST]\\n\\n[INST] <<SYS>>\\n\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = tokenizer.decode(generated_tokens.flatten())\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65c90e71-af8e-466b-a43c-6fa2be0ed61a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T21:06:02.962459Z",
     "iopub.status.busy": "2024-05-10T21:06:02.962189Z",
     "iopub.status.idle": "2024-05-10T21:06:02.973695Z",
     "shell.execute_reply": "2024-05-10T21:06:02.973053Z",
     "shell.execute_reply.started": "2024-05-10T21:06:02.962434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   263,  6635,  3290,   373,   263, 29871], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_tokens = torch.tensor(tokenizer(\"a cat sat on a \")[\"input_ids\"], dtype=torch.int)\n",
    "prefix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5ef59dd-3d04-43ea-9305-4752248f5534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T21:06:19.705891Z",
     "iopub.status.busy": "2024-05-10T21:06:19.705414Z",
     "iopub.status.idle": "2024-05-10T21:09:14.599396Z",
     "shell.execute_reply": "2024-05-10T21:09:14.598643Z",
     "shell.execute_reply.started": "2024-05-10T21:06:19.705870Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "generated = peft_model.generate(input_ids=prefix_tokens.reshape((1, -1)).to(device), do_sample=False, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c7df453-c022-491b-82e9-02d95bd45d33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T21:09:14.600864Z",
     "iopub.status.busy": "2024-05-10T21:09:14.600482Z",
     "iopub.status.idle": "2024-05-10T21:09:14.620736Z",
     "shell.execute_reply": "2024-05-10T21:09:14.620104Z",
     "shell.execute_reply.started": "2024-05-10T21:09:14.600845Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> a cat sat on a 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(generated.to('cpu').flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e93972-797e-4751-a9ad-90090081d081",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Define some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468c29dd-047e-4fbf-9090-ac9f10b50cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T01:10:37.756575Z",
     "iopub.status.busy": "2024-05-11T01:10:37.756166Z",
     "iopub.status.idle": "2024-05-11T01:10:37.765160Z",
     "shell.execute_reply": "2024-05-11T01:10:37.764679Z",
     "shell.execute_reply.started": "2024-05-11T01:10:37.756557Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_k = int(1e5)\n",
    "INF = int(1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a7157-5a13-4b1a-bfcd-d44ca832c094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T22:14:22.682796Z",
     "iopub.status.busy": "2024-05-10T22:14:22.682542Z",
     "iopub.status.idle": "2024-05-10T22:14:22.697220Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 10000000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_k, INF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2b5e4-04d8-456e-a2d1-21421982f2e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Try inference model on diplomas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ab84f31-e0a9-48e5-89d7-8bb89a9bab4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T21:16:17.307128Z",
     "iopub.status.busy": "2024-05-10T21:16:17.306892Z",
     "iopub.status.idle": "2024-05-10T21:16:17.415708Z",
     "shell.execute_reply": "2024-05-10T21:16:17.415026Z",
     "shell.execute_reply.started": "2024-05-10T21:16:17.307112Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>diploma</th>\n",
       "      <th>abstract</th>\n",
       "      <th>study_field</th>\n",
       "      <th>degree</th>\n",
       "      <th>original_diploma_extension</th>\n",
       "      <th>raw_model</th>\n",
       "      <th>learnt</th>\n",
       "      <th>learnt_8k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>45042</td>\n",
       "      <td>2023</td>\n",
       "      <td>АЙВАЗЬЯН Аршак Владимирович\\nВыпускная квалифи...</td>\n",
       "      <td>В этой работе мы строим правую трансферную мод...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "      <td>В данной работе рассматривается модельная стру...</td>\n",
       "      <td>В работе рассматриваются алгебраические теории...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>45043</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Пусть 𝐾 выпуклое тело в ℝ^𝑛. Определим 𝑑𝑛,𝑛−1(...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Плотность решетки трансляций - это минимальная...</td>\n",
       "      <td>В работе рассматриваются плотности решеток тра...</td>\n",
       "      <td>В</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>45044</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Работа посвящена повышению производительности ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе представлены результаты исслед...</td>\n",
       "      <td>В работе рассматривается задача булевой выполн...</td>\n",
       "      <td>In this work we propose a method for improving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>45046</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются классы случайных проц...</td>\n",
       "      <td>В данной работе рассматривается энергетически-...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>45047</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы рассматривается подход ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе рассматривается задача настраи...</td>\n",
       "      <td>В данной работе рассматривается задача добавле...</td>\n",
       "      <td>В работе рассматривается применение добавления...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  ...                                          learnt_8k\n",
       "0             0  ...  В работе рассматриваются алгебраические теории...\n",
       "1             1  ...                                                  В\n",
       "2             2  ...  In this work we propose a method for improving...\n",
       "3             3  ...  В работе мы обобщаем результаты об энергии нат...\n",
       "4             4  ...  В работе рассматривается применение добавления...\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(ARTIFACTS_DIR_PATH.joinpath(\"diplomas_abstracts/mcs_raw_learnt_abstract_learnt8k.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73c605cd-0001-41bf-b284-43941d3c17b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T21:17:44.173342Z",
     "iopub.status.busy": "2024-05-10T21:17:44.172976Z",
     "iopub.status.idle": "2024-05-10T21:17:44.270366Z",
     "shell.execute_reply": "2024-05-10T21:17:44.269714Z",
     "shell.execute_reply.started": "2024-05-10T21:17:44.173324Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578c4bba5d264031974ebf81b72b0a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64f6de8b65a40d8a8fe344bb76474c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/long_context_LLMs/src/notebooks/metrics_calculation/../../utils/llm_helpers_fixed_tokenizer.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_id = torch.tensor(res, dtype=torch.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13700, tensor([    1, 13866,   338,  ...,    15,    13,    13]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_helpers_fixed_tokenizer import get_prefix_len_and_tokens\n",
    "\n",
    "prefix_len, prefix_tokens = get_prefix_len_and_tokens(tokenizer, df.loc[0], INF)\n",
    "prefix_len, prefix_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "197db719-4c1d-4b1e-b811-bf4b80906896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T21:17:50.089238Z",
     "iopub.status.busy": "2024-05-10T21:17:50.088802Z",
     "iopub.status.idle": "2024-05-10T21:17:56.672430Z",
     "shell.execute_reply": "2024-05-10T21:17:56.671495Z",
     "shell.execute_reply.started": "2024-05-10T21:17:50.089220Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 13700, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 536.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c2c87e9a9bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpeft_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpeft_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgenerated_continue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerated_continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m             return self.beam_sample(\n\u001b[0m\u001b[1;32m   1723\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3427\u001b[0m             )\n\u001b[1;32m   3428\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3429\u001b[0;31m                 \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reorder_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_in_generate\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moutput_scores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m_reorder_cache\u001b[0;34m(past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_past\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             reordered_past += (\n\u001b[0;32m-> 1119\u001b[0;31m                 \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpast_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreordered_past\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_past\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             reordered_past += (\n\u001b[0;32m-> 1119\u001b[0;31m                 \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpast_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreordered_past\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 536.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "peft_model.eval()\n",
    "generated = peft_model.generate(input_ids=prefix_tokens.reshape((1, -1)).to(device), do_sample=True, num_beams=5)\n",
    "generated_continue = tokenizer.decode(generated.to('cpu').flatten()[prefix_len:])\n",
    "generated_continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b29d6e5-95d5-4740-a1f8-3a24cf583f23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T21:11:01.870938Z",
     "iopub.status.busy": "2024-05-10T21:11:01.870562Z",
     "iopub.status.idle": "2024-05-10T21:11:01.974874Z",
     "shell.execute_reply": "2024-05-10T21:11:01.973893Z",
     "shell.execute_reply.started": "2024-05-10T21:11:01.870920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nBelow is a diploma text. Your task is to generate abstract of this diploma.\\n\\n### Input:\\nАЙВАЗЬЯН Аршак Владимирович\\nВыпускная квалификационная работа\\n\\nМодельная структура на категории алгебр над\\nобогащенной теорией Ловера\\nОбразовательная программа бакалавриат «Математика»\\nНаправление и код: 01.03.01 «Математика»\\nШифр ОП: СВ.5000.2019\\n\\nНаучный руководитель:\\nд.ф.-м.н., доцент\\nБондарко Михаил Владимирович\\nРецензент:\\nк.ф.-м.н., Пекинский институт\\nматематических наук и приложений\\nу озера Янчи\\nИванов Сергей Олегович\\n\\nСанкт-Петербург\\n2023 год\\n\\n\\x0c1\\n\\nВведение\\n\\nКлассически, алгебраическая теория – это следующий набор данных:\\n• множество носителей (или сортов) S\\n• множество операций O, имеющих dom вида S1 × ... × Sn (формальное произведение; может\\nбыть пустым — 1) и cod вида Si .\\n• множество эквациональных аксиом A, т. е. тождеств связывающих операции.\\nНапример, группа – это теория с тремя операциями:\\n• носитель G\\n• операции · : G × G → G, −1 : G → G, e : 1 → G\\n• аксиомы a(bc) = (ab)c, aa−1 = e, a−1 a = e, ae = a, ea = a\\nЯсно, что также многие широко известные алгебраические понятия (например, моноиды, кольца,\\nмодули) являются алгебраическими теориями. Чуть менее очевидный пример алгебраической теории: симплициальные множества (равно как и предпучки на любой малой категории). При этом,\\nнапример, области целостности и поля не являются алгебраическими теориями, так как определяющие их аксиомы невозможно записать в виде соотношений на операции. Последнее связано с\\nтем фактом, что категории полей и областей целостности плохие: например, в них нет почти никаких пределов и копределов (в том числе даже произведений и копроизведений). Тем временем\\nлюбая категория алгебр алгебраической теории биполна, точна (в частности, регулярна), локально\\nпредставима и имеет проективное генерирующее семейство (cм. [Bor94]).\\nЕстественная среда интерпретации алгебраических теорий – категории с конечными произведениями: носители рассматриваются как объекты, операции как морфизмы, аксиомы как коммутативности соответствующих диаграмм. Легко видеть, что на самом деле каждая алгебраическая\\nтеория T определяет функтор T : CatProd → Cat, где CatProd – 2-категория категорий1 , имеющих\\nконечные произведения, и функторов, сохраняющих произведения. Примеры:\\n1. Группы в категории множеств – обычные группы, Group[Set] = Group.\\n2. Группы в категории топ. пространств – топологические группы, Group[Top] = TopGroup.\\n3. Группы в категории гладких многобразиий – группы Ли, Group[Diff] = LieGroup.\\n4. Группы в категории групп – абелевы группы, Group[Group] = Ab (аргумент Экмана-Хилтона).\\n1\\n\\nМы принимаем аксиому универсумов Гротендика и языковое соглашение typical ambiguity with universe\\n\\npolymorphism.\\n\\n2\\n\\n\\x0c5. Функтор Group[π1 ] : Group[Top] → Ab так как π1 сохраняет произведения.\\nКонечно, приятность возникающих категорий непосредственно ограничена приятностью подлежащих категорий: свойства, выше упомянутые для категорий алгебраических объектов в Set, не\\nимеют места в общем. Математика, о которой можно говорить в произвольной категории с конечными произведениями, – это в точности алгебра в смысле, определенном выше2 . Здесь следует также отметить, что хотя в произвольной категории с произведениями и удается определить\\nлюбое алгебраическое понятие, мало интересно можно сказать про них. В виду дикости произвольных категорий с конечными произведениями, это представляется скорее бесплодной землей,\\nне содержащей ни одного содержательного вопроса. Как минимум от категории математических\\nструктур, по-видимому, естественно требовать локально-представимости — это с одной стороны\\nнаделяет её множеством категорных совершенств (которые, например, оказываются витальными\\nдля теории гомотопий), а, с другой стороны, покрывает, по-видимому, подавляющее большинство\\nреальных категорий, в которых протекает математика (с точностью до замены \"поломанных\"\\nкатегорией, таких как Top или (особенно) SmoothManifold на гораздо более полезные категории\\nдельта-порожденных пространств DeltaSp и гладких множеств SmoothSet — здесь обе являются\\nлокально-представимыми и декартово-замкнутами, а вторая, более того, является топосом Гротендика). Локально представимые категории будут основным контекстом в этой работе. Условие\\nлокальной-представимости само по себе недостаточно для того, чтобы интерепртация декартовых алгебраических теорий (как они определены выше) в категории приводила к продуктивному\\nпонятию (что, например, видно в категории R-Mod, где плодотворна моноидальная структура\\nтензорного произведения, а не декартового), но об этом мы скажем подробнее несколько позже.\\nКлассическое определение алгебраической теории неестественно выделяет некоторое множество исходных операций. Так, эквивалентные по совершенно синтаксическим причинам алгебраические теории будут рассматриваться им как разные. Поэтому естественно сформировать по\\nданной алгебраической теории категорию, объекты которой суть формальные произведения C1 ×\\n.. × Cn , а морфизмы – все формальные операции между ними, и рассматривать её как инвариантное понятие алгебраической теории. Это наблюдение ведет к современному пониманию термина\\nалгебраическая теория:\\nОпределение 1.1. Финитарной алгебраической теорией или теорией Ловера (Lawvere) с множеством сортов S называется малая категория T , снабженная существенно биективным сохраняющим\\nпроизведения функтором (finSet/S)op → T . Для алгебраической теории T и категории с конечными\\nпроизведениями C, категорией T -объектов в C называется категория сохраняющих произведения\\nфункторов T → C и естественных преобразований между ними.\\n2\\n\\nТакже как математика в произвольном топосе с объектом натуральных чисел — это конструктивная математика,\\n\\nа математика в Set – классическая математика; подробнее об этом см. статью internal logic на nlab и литературу\\nтам.\\n\\n3\\n\\n\\x0cФинитарная здесь указывает на конечную арность всех операций (можно говорить также о\\nалгебраических теориях ранга λ, где λ регулярный кардинал). T -алгеброй мы будем называть\\nT -объект в Set. Легко видеть, что алгебраическая теория индуцирует финитарную (то есть сохраняющую фильтрованные копределы) монаду на SetS и монадическое сопряжение Forget : T [Set] ⇄\\nSetS : Free. Существенный образ правого функтора называется свободными T -алгебрами, а существенный образ (под его действием) подкатегории кортежей конечных множеств называется\\nсвободными конечнопорожденными T -алгебрами. Так, (как было фактически отмечено в мотивации к определению) алгебраическая теория T это просто двойственная категория к категории\\nконечнопорожденных свободных T -алгебр.\\nНа самом деле, 2-категория S-сортных алгебраических теорий эквивалентна 2-категории финитарных монад на SetS . В этом смысле понятие алгебраический теории, как оно определено выше,\\nдействительно захватывает всю (финитарную) алгебру доступную в SetS . Вот некоторые обычные\\nатрибуты финитарного монадического сопряжения ([Bor94], раздел 4):\\n• сопряжение Forget − Free, где оба функтора финитарны (что содержательно только для правого Forget, конечно)\\n• забывающий функтор унивалентен (= инъективен на Hom-множествах) и консервативен (=\\nотражает изоморфизмы)\\n• категория алгебр биполна (предполагая, что подлежащая категория такова, разумеется —\\nнапример, если она локально-представима)\\nНо это очень далеко от истины в других категориях. Если потребовать, для категории с конечными произведениям C потребовать кополноту и дистрибутивность конечные произведений\\nотносительно всех копределов, тогда алгебраическая теория по-прежнему индуцирует монадическое сопряжение между категорией T -объектов в C и C S 3 . Примером такой категории является\\nлюбая кополная декартово-замкнутая категория. Но, во-первых, и такие категории вообще говоря\\nимеют финитарные монады, не описываемые алгебраическими теориями. Во-вторых (как было\\nотмечено ранее с R-Mod) есть много естественных категорий, где это условие не выполняется и\\nалгебраические теории (в смысле определенном выше) на самом деле не определяют монад.\\nТакже во контекстах на множествах операций алгебраических объектов имеется естественная\\nдополнительная структура (скажем, это может быть симплициальное множество, (что-то похожее\\nна) топологическое пространство, гладкое многообразие или граф, последнее, например, оказывается полезно в операционной семантике см. [BW20]) и её интересно было бы захватывать.\\nЭти два наблюдения ведут к понятию V -обогащенной алгебраической теории над базой A,\\nгде V замкнутая симметричная моноидальная локально конечно представимая категория, а A\\n3\\n\\nсм. nlab, multisorted Lawvere theories\\n\\n4\\n\\n\\x0cV -обогащенная локально конечно представимая категория. Оно было введено Джоном Пауром и\\nKoki Nishizawa в их работе [PN09], где показано, что 2-категории алгебраических V -теорий на\\nA и финитарных V -монад на A канонически эквивалентны. Локально конечная представимость,\\nконечно, является существенным, нежелательным ограничением (например, исключая большую\\nчасть топосов Гротендика, в частности, разного рода топосы пространств (гладких, голоморфных,\\nалгебраических)), но для произвольных локально представимых категорий теорию алгебраических\\nтеорий над базой ещё предстоит установить (мы планируем сделать это в дальнейшем).\\nЭто работа устроена следующим образом:\\n• В секции \"Теория категорий\" мы излагаем стандартный материал о локально-представимых\\nкатегориях.\\n• В секции \"Обогащение\" мы (следуя Пауэру) определяем категории, над которыми будет\\nобогащаться. Мы предлагаем конструкции, дающие много примеров категорий и указываем\\nпока не покрытые ими примеры — это оригинальные результа работы. Далее мы (следуя\\nупомянутой выше работе) определяем обогащенные алгебраическии теории.\\n• В секции \"Гомотопическая теория категорий\" мы излагаем стандартный материал о модельных категориях и правом трансфере модельной структуры.\\n• В последней секции мы строим обогащенную модельную структуру на категории алгебр для\\nобогащенной финитарной монады.\\nТочка зрения алгебраических теорий имеет преимущество над финитарными монадами (и поэтому требует ограничиваться контекстом Пауэра, а не работать в произвольном) в том, что дает\\nвозможность определить понятие также гомотопических алгебр, на ряду с обычными. Гомотопические алгебры естественно возникают во многих контекстах, как об этом прекрасно написано в\\nвведении работы [Bad02], в которой устанавливается теорема жесткости: категория гомотопических алгебр симплициальной алгебраической теории в sSet канонично Квиллен-эквивалентна её\\nкатегории ординарных алгебр. В дальнейшем мы планируем устанавить аналогичный результат\\nдля категорий обогащенных в V .\\nТакже в дальнейшем планируется определить обогащенные алгебраическии теории для ∞категорий и устанивить связь конструкций.\\n\\n2\\n\\nТеория категорий\\n\\nЛокально-представимые категории имеют много полезных равносильных определений (каноничный учебник [AR94]). Мы укажем здесь одно из них: синтаксическое. Понятие алгебраической\\nтеории 1.1 имеет естественное обобщение, в которым операции не обязаны быть тотальными, но\\n5\\n\\n\\x0cих области определения контролируются равенствами операций. Например, таково определение\\nтеории Cat:\\n• носители: объекты Ob и морфизмы Mor\\n• операции: dom, cod : Mor → Ob, id : Ob → Mor, ◦ : Mor ×Ob Mor → Mor\\n• аксиомы: (стандартные)\\nЗдесь область определения операция композиции описывается на Mor × Mor равенством cod(pr1 ) =\\ndom(pr2 ) (что можно рассматривать также как пулбек Mor → Ob ← Mor). Также как естественная\\nобласть интерпретации финитарных алгебраических теорий — категории с конечными произведениями, естественная область интерпретации финитарных существенно алгебраических теорий —\\nкатегориях с конечными пределами. Равенста, описывающие область определения операций, интерпретируются как уравнители.\\nТакже как и для алгебраических теорий, чтобы избавиться от привязки к выделенному набору\\nопераций, естественно сформировать категорию, предоставляющую инвариантную точку зрения\\nна понятие существенно алгебраической категории. Но определение в том же стиле дать не удается, потому что в отличии от теории произведений, имеющих один и тот же каркас объектов (и\\nотличающихся именно операциями между ними), предельные теории имеют разные формы в этом\\nсмысле.\\nОпределение 2.1. Финитарной предельной теорией с множеством сортов S называется малая конечно полная категория T , снабженная функтором S → T , образ которого порождает все объекты\\nT конечными пределами. Для предельной теории T и категории с конечными произведениями\\nC, категорий T объектов в C называется категория сохраняющих конечные пределы функторов\\nT → C и естественных преобразований между ними.\\nАналогично определяется предельная теория ранга λ, где λ регулярный кардинал, с заменой\\nконечных пределов на λ-малые пределы (то есть индексная категориях которых имеет мощность\\nменьше λ, где мощность малой категории это мощность множества её морфизмов, конечно). T алгеброй мы также будем называть T -объект в Set.\\nОпределение 2.2. Локально λ-представимая категория это категория эквивалентная категории\\nалгебр существенной алгебраической теории ранга λ.\\nОбратно по локально λ-представимой категории C, её предельная теория восстанавливается\\nкак двойственная категория к полной подкатегории λ-представимых объектов: то есть объектов\\nA ∈ C таких, что функтор Hom(A, −) сохраняет λ-фильтрованные копределы 4 . Но множество\\n4\\n\\nТо есть копределы, индексная категория которых имеет конус над каждой λ-малой диаграммой. В случае когда\\n\\nλ = ℵ0 это воспроизводит обычное понятие фильтрованной категории, по определению имеющей конус над каждой\\nконечной диаграммой.\\n\\n6\\n\\n\\x0cсортов S и функтор S → T восстановить по категории алгебр невозможно — например, различные Морита-эквивалентные кольца определяют разным предельные теориям с выделенным\\nмножеством сортов, но их категории алгебр эквивалентны. Так явление уже имеет место уже для\\nобычных алгебраических теорий. Указание множества сортов — это дополнительная структура,\\nкоторая позволяет определить забывающий функтор T [C] → C S как прекомпозицию с функтором\\nS → T.\\nДля финитарных алгебраических теорий конечно представимые объекты (как они определены\\nвыше) — это в точности то, что называется конечно представимыми объектами в универсальной\\nалгебре (объекты заданные конечными множествами образующих и соотношений). Так, например,\\nпредельная теория SyntGroup это просто двойственная категория к категории конечнопредставимых групп. Классическому синтаксическому описанию теории групп (как в начале секции для Cat)\\nсоответствует одноэлементное множество сортов S и функтор S → SyntGroup выбирающий свободную группу на одном образующим (и функтор Hom(Z, −) возвращает подлежащее множество\\nгруппы). В общем случае, аналогично классическое синтаксическое описание существенно алгебраической теории определяет множество сортов S как множество носителей и функтор S → T\\nотправляющий носитель Ai в FreeAlg(∅, ..., ∅, 1, ∅, .., ∅) где одноэлементное множество 1 стоит на\\nпозиции i, соответствующей носителю. Например, для Cat образ S состоит из категорий • и • → •\\n— это свободные категории на Ob = 1, Mor = ∅ и Ob = ∅, Mor = 1 соответственно (и соответствующие Hom-функторы возвращают множество объектов и множество морфизмов категории\\nсоответственно)\\nТак структура множества сортов на теории соответствует структуре забывающего функтора\\nна категории алгебр. В действительности, для чистых теорий T ранга λ (без указания множества\\nсортов) описанные выше конструкции устанавливают антиэквивалентность синтаксиса и семантики.\\nТеорема 2.1 (Двойственность Габриэля-Ульмера). 2-категория предельных теорий ранга λ антиэквивалентна 2-категории локально λ представимых категорий\\nλ-LimitTheoryop → λ-LocPresentCat\\nT\\n\\n7→\\n\\nT [Set]\\n\\n(Cλ )op\\n\\n←[\\n\\nC\\n\\nгде λ-LimitTheoryop 2-категория\\n\\nLocPresentCat 2-категория\\n\\n• предельных теорий ранга λ\\n\\n• локально λ-представимых категорий\\n\\n• конечно непрерывных функторов\\n\\n• правых сопряженных функторов ранга λ\\n\\n• естественных преобразований\\n\\n• естественных преобразований\\n7\\n\\n\\x0cЛокально представимые категории обладают множеством категорных совершенств. Мы укажем\\nтолько некоторые, которые будем использовать в работе.\\nТеорема 2.2. Пусть C локально представимая категория, тогда C биполна.\\nТеорема 2.3. Пусть C, D локально представимые категория, F : C → D имеет ранг (то есть сохраняет λ-фильтрованные пределы для некоторого регулярного λ). Тогда если F сохраняет все\\n(малые) пределы, тогда F правый сопряженный, а если сохраняет все копределы, то левый сопряженный.\\nТак в версии для левого функтора, требование о ранге становится лишним: функтор сохраняющий все копределы, в частности, просто уже финитарен\\n\\n3\\n\\nОбогащение\\n\\nПусть V замкнутая симметричная моноидальная локально конечно представимая категория. Мы\\n(как это оказывается естественно в обогащенной теории локально конечно представимых категорий [Kel82]) требуем согласованности структур: операции моноидальной структуры сохраняют\\nконечнопредставимость (то есть единица I конечно-представима и x ⊗ y конечнопредставимо, если\\nтаковы x, y). Далее мы будем ссылаться на это понятие коротко: моноидальная lfp категория.\\nУтверждение 3.1. Пусть C локально конечно представимая декартово-замкнутая сбалансированная категория, в которой терминальный объект 1 является сепаратором (последнее свойство\\nназывается well pointed категория). Если 1 является конечно-представимым, то C декартово моноидальная lfp.\\nДоказательство. Для X конечно представимого объекта C рассмотрим функтор Hom(X, −) : C →\\nC. Так как Hom(1, Hom(X, −)) = Hom(X, −), а функтор Hom(1, −) консервативен (т.к. унивалентен и C сбалансированная) и финитарен, то и Hom(X, −) финитарен ([Kel82], 1.3). Теперь для\\nX, Y конечно представимых объектов Hom(X × Y, −) = Hom(X, −) ◦ Hom(Y, −) финитарен. Таким\\nобразом Hom(X × Y, −) = Hom(1, Hom(X × Y, 1)) финитарен, что и требовать доказать.\\nСледствие 3.1. Пусть K малая категория, если терминальный объект 1 в топосе предпучков\\nPSh(K) конечо-представим, то PSh(K) декартово моноидальная lfp.\\nДоказательство. Топосы декартово-замкнуты и сбалансированы, а топосы предпучков локально\\nконечно представимы и well-pointed.\\nУтверждение 3.2. Если K содержит терминальный объект или K конечная категория, то PSh(K)\\nдекартово моноидальная lfp.\\n8\\n\\n\\x0cДоказательство. Если K содержит терминальный объект 1, то терминальный объект топоса предпучков представим HomK (−, 1) и следовательно является конечно представимым.\\nВ любом топосе предпучков каждый предпучок копределом диаграммы представимых предпучков отображающихся в него. Если K конечная категория, то множества морфизмов между\\nпредпучками конечных множеств конечны. Значит для предпучка конечных множеств эта диаграмма конечна, следовательно он является конечным копределом представимых, следовательно\\nявляется конечно представимым. В частности, предпучок одноэлементных множеств (терминальный объект) конечно представим.\\nСледствие 3.2. Категория симплициальных множеств sSet декартово моноидальная lfp\\nУтверждение 3.3. Категория G-множеств G-Set для конечно представимой группы G является\\nдекартово моноидальной lfp.\\nУтверждение 3.4. Если K, L такие категории, что топосы предпучков над ними декартово мо`\\nноидальные lfp, то и PSh(K L) декартово моноидальная lfp.\\nДоказательство. Терминальный объект является копроизведением терминальных объектов PSh(K)\\nи PSh(L), которые конечно представимы по условию, следовательно он является конечно представимым.\\nУтверждение 3.5. Пусть T коммутативная финитарная монада на моноидальной lfp категории\\nC, тогда категория Alg T является моноидальной lfp.\\nДоказательство. Категория алгебр финитарной монады на локально конечно представимой категории является локально конечно представимой ([AR94], 2.78, замечание в конце). Тензорное\\nпроизведение алгебр (A, a), (B, b) определяется как коуравнитель:\\n(T ∇)◦µ\\n\\nT (T A ⊗ T B) ⇒ T (A ⊗ B) → A ⊠ B\\nT (a⊗b)\\n\\nгде µ — умножение в монаде, ∇ — естественное преобразование из lax моноидальной структуры\\n(которой обладает каждая коммутативная монада на моноидальной категории), ⊗ тензорное произведение в подлежащей категории и композиция (T ∇) ◦ µ : T (T A ⊗ T B) → T T (A ⊗ B) → T (A ⊗ B)\\nзаписана в прямой нотации. Моноидальная единица определяется как T 1, где 1 единица подлежащей категории. Это действительно определяет структуру замкнутой симметричной моноидальной категории (результат восходит к работе Андреса Кока [Koc71], но более удобное изложение [Bra14], 6.4.12, учитывая 6.3.12. Более общие результаты о моноидальных структурах на\\nкатегориях моноидальных монад [Sea13]). Так как монада финитарна, то её забывающий функтор финитарен, следовательно его левый сопряженный (функтор свободной алгебры T ) сохраняет\\nконечно-представимые объекты, следовательно T 1 конечно-представимо. Также (в виду того, что\\n9\\n\\n\\x0cподлежащая категория моноидальная lfp и T сохраняет конечно представимые объекты) оба объекта из диаграммы уравнителя, определяющей ⊠, являются конечно представимыми, следовательно\\nеё копредел конечно представим.\\nСледствие 3.3. Категория пунктированных объектов в подхо дящем топосе предпучков, моноидальная lfp. Категория абелевых объектов (и более обще: модулей над конечно-представимым\\nкольцом) в подходящем топосе предпучков моноидальная lfp. В частности, таковы классические\\nкатегории Set∗ , R-Mod.\\nЗамечание 3.1. Категория малых категорий Cat, группоидов Grpd и предпорядков Preord являются декартово моноидальными lfp ([Kel82]).\\nНапомним, что подлежащая категория V -обогащенной категории A определяется применением\\nк каждому Hom(X, Y ) для X, Y ∈ A функтора Hom(I, −) : V → Set. Мы будем всегда обозначать\\nподлежащую категорию как A0 и таким образом (следуя [Kel82]), когда это удобно, использовать\\nслова \"категория \"функтор\" и т.п. в значении V -категория, V -функтор и т.п., когда V ясно из\\nконтекста.\\nЗамечание 3.2. Так как Hom(I, −) : V → Set сохраняет все пределы (как любой Hom-функтор) и\\nфильтрованные копределы (т.к. I конечно-представима), то по теореме о сопряженном функторе\\nдля локально-представимых категорий, он является правым сопряженным.\\nV -категория A называется конечно экспоненцируемой, если для каждого конечно представимого x ∈ V и каждого Z ∈ A задан объект Z x такой, что [x, A(Z, −)] ∼\\n= A(−, Z x ). Для V = Set объект\\nZ x это категорное произведение соответствующего множества копий Z с самим собой. Мы пользуемся стандартными понятиями взвешенного и конического предела в обогащенной категории (см.\\n[Kel05], Chapter 3)5\\nУтверждение 3.6 ([Kel05], 3.73). V -категория A имеет все конечные обогащенные пределы тогда\\nи только тогда, когда она имеет все конечные конические пределы и конечно экспоненцируема. В\\nэтом случае также функтор F : A → B сохраняет все конечные пределы ⇐⇒ F сохраняет конечные\\nконические пределы и конечное экспоненцирование.\\nОпределение 3.1. Фильтрованным копределом в V -категории называется (обогащенный) конический копредел по ординарной фильтрованной диаграмме. V -функтор называется финитарным,\\nесли он сохраняет все фильтрованные копределы. Так финитаность функтора равносильна финитарности его подлещего. Объект X V -категории A называется локально конечно представимым,\\nесли Hom(X, −) : A → V финитарен.\\n5\\n\\nчитатель также может найти очень полезным ознакомиться с мотивацией этих понятий, прекрасно изложенной\\n\\nв статье weighted limit на nlab\\n\\n10\\n\\n\\x0cОпределение 3.2. Категория A называется локально конечно представимой, если существует\\nмалая категория G и правый сопряженный, финитарный, консервативный функтор A → [Go p, V ]\\nЭто одно из равносильных определений локально конечно представимой категории, о которых\\nмы упоминали. Так в случае Set это означает, что локально представимые категории являются\\nопределенного рода рефлексивными подкатегориями топосов предпучков.\\nПусть A локально конечно представимая (в обогащенном смысле см. [Kel82], 3.2) конечно обогащено полная категория, как обычно Af полная подкатегория конечно представимых объектов.\\nОпределение 3.3 ([PN09], 2.1). Алгебраической теорией на A (или над базой A) называется\\nстрого биективный строго сохраняющий конечные обогащенные пределы V -функтор T : Aop\\nf → T,\\nгде T малая V -обогащенная категория.\\nОбъекты T (иначе говоря, объекты Aop\\nf ) кодируют арности теории, а морфизмы — операции. В случае V = Set,\\nA = SetS мы получаем классическую финитарную алгебраическую теорию с множеством сортов S. В случае\\nV = A = sSet мы получаем хорошо известные в теории гомотопий симплициальные алгебраические теории.\\n\\nОбозначим как ι̃ : A → [Aop\\nf , V ] функтор получаемый композицией обогащенного вложения\\nЙонеды и ограничения.\\nОпределение 3.4 ([PN09], 2.2). Категорией алгебр алгебраической теории T называется описанный ниже пулбэк в категории локально малых V -категорий V -Cat:\\nAlg(T )\\n\\nPT\\n\\n[T, V ]\\n\\n⌟\\nUT\\n\\nA\\n\\nι̃\\n\\n[Aop\\nf ,V ]\\n\\nТак алгебра задается объектом из A, снабженным дополнительной структурой и аксиомами, приходящими из\\nморфизмов в T , которых нет в Aop\\nf .\\n\\nПонятие обогащенной алгебраической теории на A действительно точно захватывает финитарную алгебру доступную в A:\\nТеорема 3.1 ([PN09], 5.2). Категории алгебраических V -теорий на A и финитарных V -монад на\\nA канонически эквивалентны\\nAlgTheory(A) ≃ FinMonad(A)\\nT 7→ Monad(Alg(T ) → A)\\nфунктор забывающий структуру алгебры является финитарно V -монадичным\\n\\n11\\n\\n\\x0cop\\n(Aop\\nf → M (Af ) ) ←[ M\\n\\nестественный функтор Aop\\nf 7→ (дуал категории к.п. свободных алгебр) является алгебраической теорией\\n\\nПри этой эквивалентности соответстветствующие V -категории алгебр канонически эквивалентны.\\n\\n4\\n\\nГомотопическая теория категорий\\n\\nОпределение 4.1. Категорией со слабыми эквивалентностями (C, W ) называется категория C\\nснабженная классом морфизмов W (называемых слабыми эквивалентностями), содержащим все\\nизоморфизмы, замкнутым относительно композции и удовлетворяющим правилу 2 из 3: если композиция двух морфизмов и один из них лежит в W , то и второй лежит.\\nКатегории со слабыми эквивалентностями можно рассматривать как данные задающие (через локализацию Двайера-Кана) контексты для абстрактной теории гомотопий (то есть (∞, 1)категории). Но в тех случаях, когда категория со слабыми эквивалентностями снабжается некоторой дополнительной структурой (аксиоматизирующей расслоения и корасслоения пространств),\\nвозникают существенно более эффективные методы для управления и вычислений в соответствующей теории гомотопий.\\nОпределение 4.2. Модельной структурой на биполной категории со слабыми эквивалентностями\\n(M, W ) называется пара классов морфизмов C, F (называемых кофибрациями и фибрациями) и\\nдве слабо ортогональные функториальные системы факторизации (C ∩ W, F ), (C, F ∩ W ).\\nМорфизмы в F ∩ W , C ∩ W называются тривиальными или ациклическими (ко)фибрациями.\\nИз этого определения модельной категории вытекают все свойства замкнутости, которые требуют\\nот классов морфизмов в традиционных изложениях6 . В качестве разного характера стандартных\\nтекстов по модельным категориям (включающие, в частности, используемые далее без определения фибрантные и кофибрантные объекты, объект путей и цилиндрический объект, сопряжения\\nКвиллена, кофибрантно порожденные модельные категории, локализацию, модельную структуру\\nКвиллена на sSet), мы отсылаем к [Hov99], [Hir03], [Dwy04] [MP11], [Rie14], [Bal21].\\nОпределение 4.3. Моноидальной модельной категорией называется категория V , снабженная\\nмодельной и замкнутой симметричной моноидальной структурами так что\\n1. для кофибраций f, f ′ их пушаут-произведение f □ f ′ является кофибрацией и она ациклическая, если хотя бы одна из f, f ′ такова\\n6\\n\\nЭто было отмечено Джоялом в его лекциях о квазикатегориях 2008 года и выделено Риел в заметке Emily Riehl,\\n\\nA concise definition of a model category\\n\\n12\\n\\n\\x0c2. для любого кофибрантного объекта X и любой кофибрантной замены единицы QI → I\\nморфизм QI → X является слабой эквивалентностью\\nЗамечание 4.1. Легко видеть, что при этих условиях функторы X ⊗ − и [X, −] образуют сопряжение Квиллена для каждого кофибрантного объекта X. Если единица I кофибрантна, то второе\\nусловие вытекает из первого.\\nОпределение 4.4. Пусть V моноидальная модельная категория. Экспоненцируемая и тензорируемая V -категория C, чья подлежащая категория C0 снабжена модельной структурой, называется\\nV -обогащенной модельной категорией, если тензорирование V × C0 → C0 является бифунктором\\nКвиллена. C0 называется подлежащей модельной категорией C. Симплициальной модельной категорией называется sSetQuillen -обогащенная модельная категория, где sSetQuillen снабжена декартовой\\nмоноидальной структурой.\\nОсобенно хорошо абстрактная теория гомотопий работает в контексте локально представимых\\nмодельных категорий.\\nОпределение 4.5. Комбинаторной модельной категорией называется кофибратно порожденная\\nлокально-представимая модельная категория.\\nКомбинаторные модельные категории, по-видимому, составляют большинство встречающихся\\nв жизни модельных категорий и при этом обладают большим числом технических преимуществ.\\nПоэтому полезно заменять модельную комбинаторную на Квиллен-эквивалентную её комбинаторную, когда это возможно. Так, например, хотя категория топологических пространств Top не\\nлокально-представима, но Квиллен-эквивалентные ей категории дельта-порожденных пространств\\nDeltaSp и, конечно, симплициальных множеств sSet локально-представимы. На самом деле, каждая кофибрантно порожденная модельная категория Квиллен-эквивалентна модельной категории\\nзаданной образующими и соотношениями (то есть локализации категории симплициальных предпучков см. [Dug01]). Эти модельные категории являются также симплициальными модельными\\nкатегориями и особенно хороши.\\nСформулируем теперь понятие трансфера модельной структуры вдоль сопряжения.\\nОпределение 4.6. Пусть F : A ⇆ M : U сопряжение (где F левый, U правый), M модельная\\nкатегория. Модельная структура на A называется правой трансферной, если fib(A) = F −1 (fib(M )),\\nweak(A) = F −1 (weak(M )).\\nЭти условия однозначно определяют модельную структуру на A, потому что кофибрации являются в точности левым ортогональным классом к ациклическим фибрациями, также как ациклические кофибрация являются левым ортогоналоьным классом к фибрациям. Трансферная струкутра не всегда существует (то есть ациклическии кофибрации, как они определены в предыдущем\\n13\\n\\n\\x0cпредложении, не всегда являются пересечением кофибраций и слабых эквивалентностей). Далее\\nмы используем термины для классов морфизмов в A именно так, как они определены здесь.\\nТеорема 4.1 ([Bal21], 4.4.3). A имеет правую трансферную модельную структуру ⇐⇒ следующие\\nдва условия в A выполнены:\\nФакторизация. Каждый морфизм в раскладывается в композицию ациклической кофибрации, за которой следует фибрация, и кофибрации, за которой следует ациклическая фибрация.\\nАцикличность. Каждая ациклическая кофибрация является слабой эквивалентностью.\\nСами по себе оба эти условия трудно проверяемые, но, при некоторых предположениях, для\\nних есть более простые достаточные условия.\\nУтверждение 4.1. Пусть M кофибрантно порождена и U сохраняет малые объекты (что верно,\\nнапример, если F сохраняет фильтрованные копределы), тогда условие факторизации выполнено. Более того, если в этом случае существует модельная структура на A, то она кофибрантно\\nпорожденная: порождающие (ациклические) кофибрации получаются действием U на порождающие (ациклические) кофибрации в M\\nТак, в виду того, что забывающий функтор категории алгебр финитарной монаду сохраняет фильтрованные\\nкопределы, в этом случае условие факторизации для кофибрантно порожденной модельной категории выполняется\\nавтоматически.\\n\\nУтверждение 4.2 ([Qui67], II.4). Пусть (по-прежнему в смысле классов морфизмов, как они были\\nопределены на A выше)\\n1. A имеет функтор фибрантной замены,\\n2. A имеет объект путей для фибрантных объектов, то есть факторизацию диагонального отобweak\\n\\nfib\\n\\nражения X → P (X) → X × X.\\nтогда условие ацикличности выполнено.\\nДля случая комбинаторной симплициальной модельной категории достаточно только предположения о сущестовании функтора фибрантной замены ([Sch99]).\\n\\n5\\n\\nОбогащенная модельная структура на категории алгебр\\n\\nИтак, пусть V моноидальная модельная категория, M модельная V -обогащенная категория. Сейчас мы покажем, что когда C достаточно хорошая модельная категория, категория алгебр финитарной V -монады на C, при некоторых предположениях, имеет правую трансферную модельную\\nструктуру. Это стандартный результат.\\n14\\n\\n\\x0cТеорема 5.1. Пусть M обогащенная кофибрантно порожденная модельная категория, A категория алгебр финитарной V -монады на A, имеющая обогащенный функтор фибрантной замены\\nи имеющая объект путей в подлежащей модельной категории (в смысле 4.2). Тогда A с правой\\nтрансферной модельной структурой является V -обогащенной кофибрантно порожденной модельной категорией.\\nДоказательство. Для подлежащих категорий, в виду того, что забывающий функтор финитарной\\nмонады сохраняет фильтрованные копределы условие факторизации выполняется и из предположения следует, что также выполняется условие ацикличности. Так M0 имеет правую трансферную\\nмодельную структуру. Пусть теперь f : u → v кофибрация в V и g : X → Y фибрация в A. Мы\\nхотим показать, что индуцированный морфизм X v → X u ×Y u Y v фибрация, которая является\\nслабой эквивалентностью, если хотя бы один из f, g является. По определению правой трансферной структуры морфизм является слабой эквивалентностью или фибрацию тогда и только\\nтогда, когда его образ под действием забывающего функтора таков. Так как забывающий функтор U : A → M (будучи правым сопряженным) является обогащено непрерывным, то он сохраняет\\nэкспоненцирование ([Kel05], 3.73), следовательно под его действием наш морфизм переходит в\\nU (X)v → U (X)u ×U (Y )u Y v . Этот морфизм индуцируется морфизмом U (X) → U (Y ) и таким образом из того, что M является обогащенной модельной категорией следует утверждение теоремы.\\n\\nСписок литературы\\n[Qui67]\\n\\nDaniel G. Quillen. Homotopical algebra. 1967.\\n\\n[Koc71]\\n\\nAnders Kock. «Closed categories generated by commutative monads». В: Journal of the\\nAustralian Mathematical Society 12.4 (1971), с. 405—424.\\n\\n[Kel82]\\n\\nG. M. Kelly. «Structures defined by finite limits in the enriched context, I». eng. В: Cahiers\\nde Topologie et Géométrie Différentielle Catégoriques 23.1 (1982), с. 3—42.\\n\\n[AR94]\\n\\nJ. Adamek и J. Rosicky. Locally Presentable and Accessible Categories. 1994.\\n\\n[Bor94]\\n\\nFrancis Borceux. Handbook of Categorical Algebra. Т. 2. 1994.\\n\\n[Hov99]\\n\\nM. Hovey. Model Categories. 1999.\\n\\n[Sch99]\\n\\nStefan Schwede. «Stable homotopical algebra and Γ-spaces». В: Mathematical Proceedings of\\nthe Cambridge Philosophical Society 126 (1999), с. 329—356.\\n\\n[Dug01]\\n\\nDaniel Dugger. «Combinatorial Model Categories Have Presentations». В: Advances in Mathematics\\n164.1 (2001), с. 177—201. arXiv: math/0007068.\\n\\n[Bad02]\\n\\nBernard Badzioch. «Algebraic Theories in Homotopy Theory». В: Annals of Mathematics\\n155.3 (2002), с. 895—913. arXiv: math/0110101.\\n15\\n\\n\\x0c[Hir03]\\n\\nPhilip S. Hirschhorn. Model categories and their localizations. 2003.\\n\\n[Dwy04] W.G. Dwyer. Homotopy Limit Functors on Model Categories and Homotopical Categories.\\n2004.\\n[Kel05]\\n\\nG. Kelly. «The Basic Concepts of Enriched Category Theory». В: Reprints in Theory and\\nApplications of Categories (янв. 2005).\\n\\n[PN09]\\n\\nJohn Power и Koki Nishizawa. «Lawvere theories enriched over a general base». В: Journal\\nof Pure and Applied Algebra 213.3 (2009), с. 377—386.\\n\\n[MP11]\\n\\nJ P May и K Ponto. More Concise Algebraic Topology: Localization, Completion, and Model\\nCategories. 2011.\\n\\n[Sea13]\\n\\nGavin J. Seal. Tensors, monads and actions. 2013. arXiv: 1205.0101 [math.CT].\\n\\n[Bra14]\\n\\nMartin Brandenburg. Tensor categorical foundations of algebraic geometry. 2014. arXiv: 1410.\\n1716 [math.AG].\\n\\n[Rie14]\\n\\nE. Riehl. Categorical Homotopy Theory. Categorical Homotopy Theory. 2014.\\n\\n[BW20]\\n\\nJohn C. Baez и Christian Williams. «Enriched Lawvere Theories for Operational Semantic».\\nВ: Electronic Proceedings in Theoretical Computer Science 323 (сент. 2020), с. 106—135.\\narXiv: 1905.05636v3.\\n\\n[Bal21]\\n\\nS. Balchin. A Handbook of Model Categories. 2021.\\n\\n16\\n\\n\\x0c\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prefix_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ff47d-4134-439c-9777-7c97e3b313e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference model on asessors questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb389dc-bc40-45a6-96c3-4af0b2dfb231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T22:14:22.698255Z",
     "iopub.status.busy": "2024-05-10T22:14:22.697855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b17a57023104e8d9ef32ed4b143e67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af02347f1a2d49be8f2bfde26cc79c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc468c40ea284ea49e9db756e287e40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/long_context_LLMs/src/notebooks/metrics_calculation/../../utils/llm_helpers_fixed_tokenizer.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_id = torch.tensor(res, dtype=torch.int)\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 13700, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0337f1b0619f4f53a2f531c57fbcecce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6dae84c7c64c1a954af3cf09ab5545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/long_context_LLMs/src/notebooks/metrics_calculation/../../utils/llm_helpers_fixed_tokenizer.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_id = torch.tensor(res, dtype=torch.int)\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 11249, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcd3944f2af40fa89bf6c1f5e3c3399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e86ed6139849938bea82d88d957b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 15939, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae326976f03f447b9a3654147a72b5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346d60e2969c479c98e5e462dd0099b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/long_context_LLMs/src/notebooks/metrics_calculation/../../utils/llm_helpers_fixed_tokenizer.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_id = torch.tensor(res, dtype=torch.int)\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 10684, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532926df70e249b4bd32ddcef21df9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7745e336983d4d24abbc6a20ebe7f199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/long_context_LLMs/src/notebooks/metrics_calculation/../../utils/llm_helpers_fixed_tokenizer.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_id = torch.tensor(res, dtype=torch.int)\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 15467, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acc42947e944bd69b62b8daeb260e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788199a493884e6188fe1d7871cf0506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/long_context_LLMs/src/notebooks/metrics_calculation/../../utils/llm_helpers_fixed_tokenizer.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_id = torch.tensor(res, dtype=torch.int)\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 14142, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf94c122944429d9e0a2ca09cf111f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c173d054bdd4e94bc3c545f5c3c920a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 16308, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc39e08163a4cb8bbcd0128d9e7014c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e125d349e34ee7a4ae6bdb723d1208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 16139, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f548f4e9f9aa46c592354c1756678048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca268b5c45342f09973ae780be6b9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/long_context_LLMs/src/notebooks/metrics_calculation/../../utils/llm_helpers_fixed_tokenizer.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_id = torch.tensor(res, dtype=torch.int)\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 12501, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cc494d2d3f46328367eca41218e4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e86057a08cc4dc58d5abfd4df99af3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 16270, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8db958a19a48caad9a8c777f0f17ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e6f4d8e68d449e865f2ce3b91f7fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Texts...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/long_context_LLMs/src/notebooks/metrics_calculation/../../utils/llm_helpers_fixed_tokenizer.py:121: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_id = torch.tensor(res, dtype=torch.int)\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 7024, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llm_helpers_fixed_tokenizer import get_some_model_result\n",
    "\n",
    "test_df = pd.read_csv(ARTIFACTS_DIR_PATH.joinpath(\"diplomas_abstracts/mcs_raw_learnt_abstract_learnt8k.csv\"))\n",
    "\n",
    "new_rows = []\n",
    "for _, row in tqdm(test_df[:first_k].iterrows(), total=len(test_df[:first_k]), desc=\"Rows...\"):\n",
    "    new_row = copy.deepcopy(row)\n",
    "    new_row[MODEL_EXPERIMENT_NAME] = get_some_model_result(peft_model, tokenizer, row, device, diploma_prefix_len=INF)\n",
    "    new_rows.append(new_row)\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ba9b7-6c92-46ab-8732-7a7b4f8bd1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.to_csv(ARTIFACTS_DIR_PATH.joinpath(f\"diplomas_abstracts/baselines_with_{MODEL_EXPERIMENT_NAME}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8dcf64-b373-4f91-bab2-fadaaf7c920e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference model on MERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707c6f4-eacf-45d4-9785-bc5e45457aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mera_helpers import construct_prompt\n",
    "from path_helpers import get_dataset_path, get_metric_dir_path\n",
    "from llm_helpers import calculate_token_interest_probs, get_answer\n",
    "\n",
    "for name, dataset_meta in tqdm(HUGGINGFACE_NAME_TO_DATASET.items(), desc=\"Datasets...\"):\n",
    "    for subset, split in tqdm(zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]), total=len(dataset_meta[\"splits\"]), desc=\"Splits...\"):\n",
    "        path = get_dataset_path(subset, name, split)\n",
    "        dataset = load_from_disk(path)\n",
    "        probs_list = []\n",
    "        a_list = []\n",
    "        for row in tqdm(list(dataset)[:first_k], desc=\"Rows...\"):\n",
    "            q = construct_prompt(row)\n",
    "            probs = calculate_token_interest_probs(q, tokenizer, peft_model)\n",
    "            probs_list.append({\n",
    "                \"probs\": probs,\n",
    "                \"meta\": row[\"meta\"],\n",
    "            })\n",
    "            a = get_answer(probs)\n",
    "            a_list.append({\n",
    "                \"answer\": a,\n",
    "                \"meta\": row[\"meta\"],\n",
    "            })\n",
    "        print(a_list)\n",
    "        metric_dir_path = get_metric_dir_path(MODEL_EXPERIMENT_NAME, subset, name, split)\n",
    "        print(metric_dir_path)\n",
    "        metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "        with open(metric_dir_path.joinpath(\"probs.jsonl\"), \"w\") as f:\n",
    "            json.dump(probs_list, f, ensure_ascii=False, indent=2)\n",
    "        with open(metric_dir_path.joinpath(\"answers.jsonl\"), \"w\") as f:\n",
    "            json.dump(a_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fea04f-f3f0-4ff0-bc16-12b2467fe1b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference & eval model on landmark passkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946d56f-f859-42bd-bb74-676a1510e183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from landmark_run_test import test_passkey_full, load_pipes\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "models = [MODEL_EXPERIMENT_NAME]\n",
    "pipe = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer, device='cuda')\n",
    "pipes = {MODEL_EXPERIMENT_NAME: pipe}\n",
    "test_passkey_full(pipes, models, n_values=[14000, 18000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8dae2-915e-42f9-a952-608e38bdae33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:48:59.387539Z",
     "iopub.status.busy": "2024-05-09T17:48:59.387143Z",
     "iopub.status.idle": "2024-05-09T17:48:59.411426Z",
     "shell.execute_reply": "2024-05-09T17:48:59.410666Z",
     "shell.execute_reply.started": "2024-05-09T17:48:59.387519Z"
    },
    "tags": []
   },
   "source": [
    "## Evaluate model results on tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e723d-b876-425b-90ef-999061b59adf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate asessors questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e89eb-2ef7-418a-9055-8be6b7990b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = new_df.set_index(\"Unnamed: 0\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f8152d-99bd-48d2-9b76-73caab8e209c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T01:10:37.766357Z",
     "iopub.status.busy": "2024-05-11T01:10:37.766037Z",
     "iopub.status.idle": "2024-05-11T01:10:37.778324Z",
     "shell.execute_reply": "2024-05-11T01:10:37.777638Z",
     "shell.execute_reply.started": "2024-05-11T01:10:37.766337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_analization\n",
      "data_collection\n",
      "experiments\n",
      "junk\n",
      "metrics_calculation\n",
      "pattern.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3ee3ce9-9ef0-4688-b4e1-ce204c209028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T22:24:33.044212Z",
     "iopub.status.busy": "2024-05-11T22:24:33.043232Z",
     "iopub.status.idle": "2024-05-11T22:24:33.267741Z",
     "shell.execute_reply": "2024-05-11T22:24:33.266886Z",
     "shell.execute_reply.started": "2024-05-11T22:24:33.044169Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>diploma</th>\n",
       "      <th>abstract</th>\n",
       "      <th>study_field</th>\n",
       "      <th>degree</th>\n",
       "      <th>original_diploma_extension</th>\n",
       "      <th>raw_model</th>\n",
       "      <th>learnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>45042</td>\n",
       "      <td>2023</td>\n",
       "      <td>АЙВАЗЬЯН Аршак Владимирович\\nВыпускная квалифи...</td>\n",
       "      <td>В этой работе мы строим правую трансферную мод...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "      <td>В данной работе рассматривается модельная стру...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>45043</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Пусть 𝐾 выпуклое тело в ℝ^𝑛. Определим 𝑑𝑛,𝑛−1(...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Плотность решетки трансляций - это минимальная...</td>\n",
       "      <td>В работе рассматриваются плотности решеток тра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>45044</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Работа посвящена повышению производительности ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе представлены результаты исслед...</td>\n",
       "      <td>В работе рассматривается задача булевой выполн...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>45046</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются классы случайных проц...</td>\n",
       "      <td>В данной работе рассматривается энергетически-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>45047</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы рассматривается подход ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе рассматривается задача настраи...</td>\n",
       "      <td>В данной работе рассматривается задача добавле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>45131</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В настоящей работе были рассмотрены две задачи...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>MASTER'S STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются морфизмы, порождающие...</td>\n",
       "      <td>В работе рассматриваются некоторые гипотезы о ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>45132</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В данной работе исследовались MS/MS спектры, с...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Данная работа посвящена разработке алгоритмов ...</td>\n",
       "      <td>Данная работа посвящена поиску и классификации...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>45133</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В выпускной работе описываются триангуляции ве...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются асимптотики числа три...</td>\n",
       "      <td>Автор производит подсчёт асимптотики числа три...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>45135</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы было разработано програ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной выпускной квалификационной работе пре...</td>\n",
       "      <td>Работа посвящена разработке программного средс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>45137</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В данной работе представлены реализация фронте...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Данная выпускная квалификационная работа посвя...</td>\n",
       "      <td>Работа посвящена разработке архитектуры и созд...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  ...                                             learnt\n",
       "Unnamed: 0         ...                                                   \n",
       "12          45042  ...  В данной работе рассматривается модельная стру...\n",
       "25          45043  ...  В работе рассматриваются плотности решеток тра...\n",
       "37          45044  ...  В работе рассматривается задача булевой выполн...\n",
       "101         45046  ...  В данной работе рассматривается энергетически-...\n",
       "152         45047  ...  В данной работе рассматривается задача добавле...\n",
       "...           ...  ...                                                ...\n",
       "1062        45131  ...  В работе рассматриваются некоторые гипотезы о ...\n",
       "1064        45132  ...  Данная работа посвящена поиску и классификации...\n",
       "1079        45133  ...  Автор производит подсчёт асимптотики числа три...\n",
       "1209        45135  ...  Работа посвящена разработке программного средс...\n",
       "1238        45137  ...  Работа посвящена разработке архитектуры и созд...\n",
       "\n",
       "[70 rows x 9 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(ARTIFACTS_DIR_PATH.joinpath(\"diplomas_abstracts/mcs_raw_learnt_abstract.csv\"))\n",
    "new_df = df.set_index(\"Unnamed: 0\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c35126a6-3236-4378-849b-b946206f7859",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T12:35:50.396429Z",
     "iopub.status.busy": "2024-05-11T12:35:50.395783Z",
     "iopub.status.idle": "2024-05-11T12:35:50.405950Z",
     "shell.execute_reply": "2024-05-11T12:35:50.405379Z",
     "shell.execute_reply.started": "2024-05-11T12:35:50.396397Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35565"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_df[\"diploma\"][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be3b906-b41a-44c5-9092-6916f4e80e45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T22:01:24.244682Z",
     "iopub.status.busy": "2024-05-11T22:01:24.243840Z",
     "iopub.status.idle": "2024-05-11T22:01:24.268166Z",
     "shell.execute_reply": "2024-05-11T22:01:24.266952Z",
     "shell.execute_reply.started": "2024-05-11T22:01:24.244641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kek\n"
     ]
    }
   ],
   "source": [
    "print(\"kek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86544075-017b-45ca-9ab9-8ab19e56b7c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T22:02:00.032821Z",
     "iopub.status.busy": "2024-05-11T22:02:00.031969Z",
     "iopub.status.idle": "2024-05-11T22:02:00.302166Z",
     "shell.execute_reply": "2024-05-11T22:02:00.300126Z",
     "shell.execute_reply.started": "2024-05-11T22:02:00.032783Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asessors_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b040c69a9e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0masessors_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'asessors_df' is not defined"
     ]
    }
   ],
   "source": [
    "asessors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8985e26b-96ab-420c-9740-6d5ccdfe344b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T22:15:44.739421Z",
     "iopub.status.busy": "2024-05-11T22:15:44.738318Z",
     "iopub.status.idle": "2024-05-11T22:15:44.756128Z",
     "shell.execute_reply": "2024-05-11T22:15:44.754997Z",
     "shell.execute_reply.started": "2024-05-11T22:15:44.739370Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_dir_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/diploma_test/{LLAMA_2_7B}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "probs_entry = {\n",
    "    \"probs\": {\"A\": 0, \"B\": 0.1, \"C\": 0.1, \"D\": 0.2},\n",
    "    \"meta\": {\n",
    "      \"abstract\": \"В этой работе мы строим правую трансферную модельную структуру на категории алгебр обогащенной алгебраической теории в обогащенной модельной категории, удовлетворяющей некоторым свойствам. Также мы даем несколько общих конструкций генерирующих замкнутые симметричные моноидальные локально конечно представимые категори, которые служат базой обогащения.\",\n",
    "      \"id\": \"12\"\n",
    "    },\n",
    "}\n",
    "with open(metric_dir_path.joinpath(\"probs_appended.jsons\"), \"a\") as f:\n",
    "    json.dump(probs_entry, f, ensure_ascii=False)\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bdbfb52-2aeb-4b1a-9566-170b7c474643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T12:01:54.894482Z",
     "iopub.status.busy": "2024-05-12T12:01:54.893856Z",
     "iopub.status.idle": "2024-05-12T12:01:54.920809Z",
     "shell.execute_reply": "2024-05-12T12:01:54.920219Z",
     "shell.execute_reply.started": "2024-05-12T12:01:54.894454Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_dir_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/diploma_appended/{LLAMA_2_7B}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "with open(metric_dir_path.joinpath(\"probs_appended.jsons\"), \"r\") as f:\n",
    "    probs = [json.loads(x) for x in f.readlines()]\n",
    "with open(metric_dir_path.joinpath(\"answers_appended.jsons\"), \"r\") as f:\n",
    "    answers = [json.loads(x) for x in f.readlines()]\n",
    "len(probs), len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d352a711-e6c0-4cc6-bd23-356c74de4455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T12:06:24.041841Z",
     "iopub.status.busy": "2024-05-12T12:06:24.041427Z",
     "iopub.status.idle": "2024-05-12T12:06:24.057984Z",
     "shell.execute_reply": "2024-05-12T12:06:24.057384Z",
     "shell.execute_reply.started": "2024-05-12T12:06:24.041816Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(ARTIFACTS_DIR_PATH.joinpath(\"datasets/diplomas_asessors_questions/mcs_df_human_filled_processed.json\"), \"r\") as f:\n",
    "    asessors_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9e1f1-2fc2-4fc3-870a-e3468a72a3b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T22:45:49.364130Z",
     "iopub.status.busy": "2024-05-11T22:45:49.362736Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b35be3e02b04691ab98f05022fea72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/69 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "\u001b[0;31mKernelOutOfMemory\u001b[0m",
     "evalue": "Kernel ran out of memory and has been restarted. If the restart fails, restart the kernel from the Kernel menu.\nIf the error persists, try choosing a different configuration or optimizing your code.",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "from mera_helpers import construct_prompt\n",
    "from llm_helpers import calculate_token_interest_probs, get_answer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for split in [\"diploma\"]:\n",
    "    # a_list = []\n",
    "    # probs_list = []\n",
    "    metric_dir_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{split}_appended/{LLAMA_2_7B}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "    metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    for row in tqdm(asessors_dataset[1:], desc=\"Rows...\"):\n",
    "        x = copy.deepcopy(row)\n",
    "        if split == \"empty\":\n",
    "            x['inputs']['context'] = \"\"\n",
    "        else:\n",
    "            x['inputs']['context'] = new_df[split].loc[int(x['meta']['id'])] # df[\"diploma\"].iloc[int(x['meta']['id'])]\n",
    "        q = construct_prompt(x)\n",
    "        probs = calculate_token_interest_probs(q, tokenizer, model)\n",
    "        probs_entry = {\n",
    "            \"probs\": probs,\n",
    "            \"meta\": row[\"meta\"],\n",
    "        }\n",
    "        a = get_answer(probs)\n",
    "        a_entry = {\n",
    "            \"answer\": a,\n",
    "            \"meta\": row[\"meta\"],\n",
    "        }\n",
    "        with open(metric_dir_path.joinpath(\"probs_appended.jsons\"), \"a\") as f:\n",
    "            json.dump(probs_entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "        with open(metric_dir_path.joinpath(\"answers_appended.jsons\"), \"a\") as f:\n",
    "            json.dump(a_entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81f9f8a3-92a1-41b3-95c9-57e543891836",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T12:05:30.616436Z",
     "iopub.status.busy": "2024-05-12T12:05:30.615846Z",
     "iopub.status.idle": "2024-05-12T12:05:30.676937Z",
     "shell.execute_reply": "2024-05-12T12:05:30.676286Z",
     "shell.execute_reply.started": "2024-05-12T12:05:30.616410Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/work/resources/long_context_LLMs/artifacts/metrics/diplomas_asessors_questions/diploma_appended/llama-2-7b/answers_appended.jsons\n"
     ]
    }
   ],
   "source": [
    "! ls /home/jupyter/work/resources/long_context_LLMs/artifacts/metrics/diplomas_asessors_questions/diploma_appended/llama-2-7b/answers_appended.jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ad797c7-5095-47dc-869e-3707631f77b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-12T12:09:27.853480Z",
     "iopub.status.busy": "2024-05-12T12:09:27.852852Z",
     "iopub.status.idle": "2024-05-12T12:09:27.892057Z",
     "shell.execute_reply": "2024-05-12T12:09:27.891504Z",
     "shell.execute_reply.started": "2024-05-12T12:09:27.853452Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>subset</th>\n",
       "      <th>split</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>diploma</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>empty</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0.636364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model              subset     split  accuracy_score\n",
       "0  llama-2-7b  asessors_questions   diploma        0.090909\n",
       "1  llama-2-7b  asessors_questions     empty        0.363636\n",
       "2  llama-2-7b  asessors_questions  abstract        0.636364"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "pred_by_model = dict()\n",
    "# for baseline_model in HUGGINFACE_BASELINE_MODELS:\n",
    "splits = [\"diploma\", \"empty\", \"abstract\"]\n",
    "# if baseline_model == LLAMA_2_7B:\n",
    "#     splits.extend([\"learnt\", \"learnt_8k\", MODEL_EXPERIMENT_NAME])\n",
    "for split in splits:\n",
    "    if split == \"diploma\":\n",
    "        metric_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{split}_appended/{LLAMA_2_7B}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "    else:\n",
    "        metric_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{split}/{LLAMA_2_7B}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "    if split == \"diploma\":\n",
    "        some_path = metric_path.joinpath(\"answers_appended.jsons\")\n",
    "        with open(some_path, \"r\") as f:\n",
    "            answers = [json.loads(x) for x in f.readlines()]\n",
    "    else:\n",
    "        some_path = metric_path.joinpath(\"answers.jsonl\")     \n",
    "        with open(some_path, \"r\") as f:\n",
    "            answers = json.load(f) #[json.loads(x) for x in f.readlines()]\n",
    "    # with open(metric_path.joinpath(\"answers.jsonl\"), \"r\") as f:\n",
    "        # answers = json.load(f)\n",
    "    pred = [x[\"answer\"] for x in answers[:22]]\n",
    "    pred_by_model[f\"{split}_{LLAMA_2_7B}\"] = pred\n",
    "    true = [x[\"outputs\"] for x in asessors_dataset[:22]]\n",
    "    rows.append({\n",
    "        \"model\": LLAMA_2_7B,\n",
    "        \"subset\": \"asessors_questions\",\n",
    "        \"split\": split,\n",
    "        \"accuracy_score\": accuracy_score(true, pred),\n",
    "    })\n",
    "asessors_df = pd.DataFrame(rows)\n",
    "asessors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209da0f-75d5-45af-b837-06433293d59d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asessors_df.to_csv(METRICS_DIR_PATH.joinpath(f\"asessors_baseline.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff94c80-17ec-4220-a3cc-826719a31832",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate MERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493a575-dc86-463b-b981-96afcda8a722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for baseline_model in HUGGINFACE_BASELINE_MODELS.union([VIKHR_7B, SAIGA_MISTRAL_7B_LORA]).union([MODEL_EXPERIMENT_NAME]):\n",
    "    for name, dataset_meta in HUGGINGFACE_NAME_TO_DATASET.items():\n",
    "        for subset, split in zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]):\n",
    "            dataset_path = get_dataset_path(subset, name, split)\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "            metric_path = get_metric_dir_path(baseline_model, subset, name, split)\n",
    "            with open(metric_path.joinpath(\"answers.jsonl\"), \"r\") as f:\n",
    "                answers = json.load(f)\n",
    "            pred = [x[\"answer\"] for x in answers[:first_k]]\n",
    "            true = [x[\"outputs\"] for x in list(dataset)[:first_k]]\n",
    "            rows.append({\n",
    "                \"model\": baseline_model,\n",
    "                \"subset\": subset,\n",
    "                \"split\": split,\n",
    "                \"accuracy_score\": accuracy_score(true, pred),\n",
    "            })\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a165a-a392-4811-b00c-f564391fa6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(METRICS_DIR_PATH.joinpath(f\"ru_metrics_with_{MODEL_EXPERIMENT_NAME}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79a564-eeb3-436b-8469-e22603874bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
