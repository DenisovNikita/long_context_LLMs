{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fd1ea6-4c85-457f-8dcc-f3e3a696ee96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdea8f-48ea-49ee-a487-5bf5de98deb9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5998e0e-0188-4d18-88b6-a634a5d69ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:03:01.878783Z",
     "iopub.status.busy": "2024-05-09T18:03:01.878495Z",
     "iopub.status.idle": "2024-05-09T18:03:01.913599Z",
     "shell.execute_reply": "2024-05-09T18:03:01.912982Z",
     "shell.execute_reply.started": "2024-05-09T18:03:01.878764Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0360d445-d3ad-4007-8972-5816c93e9b73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:30:57.235583Z",
     "iopub.status.busy": "2024-05-09T17:30:57.235212Z",
     "iopub.status.idle": "2024-05-09T17:30:57.387763Z",
     "shell.execute_reply": "2024-05-09T17:30:57.387113Z",
     "shell.execute_reply.started": "2024-05-09T17:30:57.235558Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8827ceb3a9e14b8494b219cf56aaeacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb725155-72a6-403d-9c18-e1da14092bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:31:05.644640Z",
     "iopub.status.busy": "2024-05-09T17:31:05.644262Z",
     "iopub.status.idle": "2024-05-09T17:31:11.411709Z",
     "shell.execute_reply": "2024-05-09T17:31:11.410955Z",
     "shell.execute_reply.started": "2024-05-09T17:31:05.644614Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../utils\")\n",
    "from definitions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c446f708-d8ae-415e-bc88-cc77e234a2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:31:11.413251Z",
     "iopub.status.busy": "2024-05-09T17:31:11.412728Z",
     "iopub.status.idle": "2024-05-09T17:31:11.425108Z",
     "shell.execute_reply": "2024-05-09T17:31:11.424546Z",
     "shell.execute_reply.started": "2024-05-09T17:31:11.413223Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(\"../../../../cache/\")\n",
    "DATASET_DIR = Path(\"/home/jupyter/mnt/datasets/diplomas/russian_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db3f5d5d-9db1-4862-8308-3b80e2dfdd56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:31:11.426516Z",
     "iopub.status.busy": "2024-05-09T17:31:11.426093Z",
     "iopub.status.idle": "2024-05-09T17:31:11.442372Z",
     "shell.execute_reply": "2024-05-09T17:31:11.441841Z",
     "shell.execute_reply.started": "2024-05-09T17:31:11.426491Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_EXPERIMENT_NAME = \"test_pipeline_working\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f34b3f-f3e1-4375-a566-dcec395755d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42e56e8e-d59f-4c38-b0b1-489d83518a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:31:11.443302Z",
     "iopub.status.busy": "2024-05-09T17:31:11.442983Z",
     "iopub.status.idle": "2024-05-09T17:31:18.220960Z",
     "shell.execute_reply": "2024-05-09T17:31:18.220211Z",
     "shell.execute_reply.started": "2024-05-09T17:31:11.443279Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.58s/it]\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model_name = LLAMA_2_7B\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    device_map='auto'\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "MODEL_MAX_LENGTH = 16384\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    model_max_length=MODEL_MAX_LENGTH,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=special_tokens_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9001686d-9a4b-4eb5-8db6-1bd51622d329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:32:35.465060Z",
     "iopub.status.busy": "2024-05-09T17:32:35.464656Z",
     "iopub.status.idle": "2024-05-09T17:33:01.760569Z",
     "shell.execute_reply": "2024-05-09T17:33:01.759341Z",
     "shell.execute_reply.started": "2024-05-09T17:32:35.465040Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading adapter_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 674/674 [00:00<00:00, 5.04MB/s]\n",
      "Downloading adapter_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.08G/1.08G [00:17<00:00, 60.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"nvdenisov2002/llama-longLoRA-v1\"\n",
    "peft_model = PeftModel.from_pretrained(model, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47199b-36fd-4a22-b2c2-ace036f3abff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Inference model on tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468c29dd-047e-4fbf-9090-ac9f10b50cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2b5e4-04d8-456e-a2d1-21421982f2e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference model on asessors questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffb389dc-bc40-45a6-96c3-4af0b2dfb231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:40:40.675897Z",
     "iopub.status.busy": "2024-05-09T17:40:40.675383Z",
     "iopub.status.idle": "2024-05-09T17:42:38.965926Z",
     "shell.execute_reply": "2024-05-09T17:42:38.965252Z",
     "shell.execute_reply.started": "2024-05-09T17:40:40.675879Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61630fc2ea5c4bb0bcf9b4b184912d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 4316, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>diploma</th>\n",
       "      <th>abstract</th>\n",
       "      <th>study_field</th>\n",
       "      <th>degree</th>\n",
       "      <th>original_diploma_extension</th>\n",
       "      <th>raw_model</th>\n",
       "      <th>learnt</th>\n",
       "      <th>learnt_8k</th>\n",
       "      <th>test_pipeline_working</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>45042</td>\n",
       "      <td>2023</td>\n",
       "      <td>–ê–ô–í–ê–ó–¨–Ø–ù –ê—Ä—à–∞–∫ –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á\\n–í—ã–ø—É—Å–∫–Ω–∞—è –∫–≤–∞–ª–∏—Ñ–∏...</td>\n",
       "      <td>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –º—ã —Å—Ç—Ä–æ–∏–º –ø—Ä–∞–≤—É—é —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω—É—é –º–æ–¥...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω...</td>\n",
       "      <td>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä—É...</td>\n",
       "      <td>–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∞–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–∏–µ —Ç–µ–æ—Ä–∏–∏...</td>\n",
       "      <td>–ê–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–∞—è —Ç–µ–æ—Ä–∏—è ‚Äì —ç—Ç–æ –ø–æ–Ω—è—Ç–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>45043</td>\n",
       "      <td>2023</td>\n",
       "      <td>–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ...</td>\n",
       "      <td>–ü—É—Å—Ç—å ùêæ –≤—ã–ø—É–∫–ª–æ–µ —Ç–µ–ª–æ –≤ ‚Ñù^ùëõ. –û–ø—Ä–µ–¥–µ–ª–∏–º ùëëùëõ,ùëõ‚àí1(...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–µ—à–µ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π - —ç—Ç–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è...</td>\n",
       "      <td>–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ä–µ—à–µ—Ç–æ–∫ —Ç—Ä–∞...</td>\n",
       "      <td>–í</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>45044</td>\n",
       "      <td>2023</td>\n",
       "      <td>–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ...</td>\n",
       "      <td>–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥...</td>\n",
       "      <td>–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –±—É–ª–µ–≤–æ–π –≤—ã–ø–æ–ª–Ω...</td>\n",
       "      <td>In this work we propose a method for improving...</td>\n",
       "      <td>The Boolean satisfiability problem (often abbr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  ...                              test_pipeline_working\n",
       "0             0  ...  –ê–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–∞—è —Ç–µ–æ—Ä–∏—è ‚Äì —ç—Ç–æ –ø–æ–Ω—è—Ç–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ...\n",
       "1             1  ...                                                 \\n\n",
       "2             2  ...  The Boolean satisfiability problem (often abbr...\n",
       "\n",
       "[3 rows x 13 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_helpers import get_some_model_result\n",
    "\n",
    "test_df = pd.read_csv(ARTIFACTS_DIR_PATH.joinpath(\"diplomas_abstracts/mcs_raw_learnt_abstract_learnt8k.csv\"))\n",
    "\n",
    "new_rows = []\n",
    "for _, row in tqdm(test_df[:first_k].iterrows(), total=len(test_df[:first_k]), desc=\"Rows...\"):\n",
    "    new_row = copy.deepcopy(row)\n",
    "    new_row[MODEL_EXPERIMENT_NAME] = get_some_model_result(peft_model, tokenizer, row, device, diploma_prefix_len=8000)\n",
    "    new_rows.append(new_row)\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "# new_df.to_csv(ARTIFACTS_DIR_PATH.joinpath(f\"diplomas_abstracts/mcs_raw_learnt_abstract_learnt8k_{MODEL_EXPERIMENT_NAME}.csv\"))\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "036997e3-5d02-4db2-97f4-abdacfd54940",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:07:17.839580Z",
     "iopub.status.busy": "2024-05-09T18:07:17.839114Z",
     "iopub.status.idle": "2024-05-09T18:07:17.851813Z",
     "shell.execute_reply": "2024-05-09T18:07:17.851149Z",
     "shell.execute_reply.started": "2024-05-09T18:07:17.839560Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8dcf64-b373-4f91-bab2-fadaaf7c920e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference model on MERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5707c6f4-eacf-45d4-9785-bc5e45457aa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:17:58.360415Z",
     "iopub.status.busy": "2024-05-09T18:17:58.360004Z",
     "iopub.status.idle": "2024-05-09T18:18:00.397201Z",
     "shell.execute_reply": "2024-05-09T18:18:00.396472Z",
     "shell.execute_reply.started": "2024-05-09T18:17:58.360396Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17781374692543b7be0857a05724024e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Datasets...:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3685d113644441bb30407bd5aa9efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Splits...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd038a7e5b242f0a2f75f37843dba7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer': 'B', 'meta': {'domain': 'moral_scenarios', 'id': 0}}, {'answer': 'B', 'meta': {'domain': 'moral_scenarios', 'id': 1}}, {'answer': 'B', 'meta': {'domain': 'moral_scenarios', 'id': 2}}]\n",
      "/home/jupyter/work/resources/long_context_LLMs/artifacts/metrics/rummlu/mera/public_test/test_pipeline_working\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3222c44a89d04547b1f073d973e6ff3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer': 'D', 'meta': {'id': 0}}, {'answer': 'D', 'meta': {'id': 1}}, {'answer': 'D', 'meta': {'id': 2}}]\n",
      "/home/jupyter/work/resources/long_context_LLMs/artifacts/metrics/ruopenbookqa/mera/train/test_pipeline_working\n"
     ]
    }
   ],
   "source": [
    "from mera_helpers import construct_prompt\n",
    "from path_helpers import get_dataset_path, get_metric_dir_path\n",
    "from llm_helpers import calculate_token_interest_probs, get_answer\n",
    "\n",
    "for name, dataset_meta in tqdm(HUGGINGFACE_NAME_TO_DATASET.items(), desc=\"Datasets...\"):\n",
    "    for subset, split in tqdm(zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]), total=len(dataset_meta[\"splits\"]), desc=\"Splits...\"):\n",
    "        path = get_dataset_path(subset, name, split)\n",
    "        dataset = load_from_disk(path)\n",
    "        probs_list = []\n",
    "        a_list = []\n",
    "        for row in tqdm(list(dataset)[:first_k], desc=\"Rows...\"):\n",
    "            q = construct_prompt(row)\n",
    "            probs = calculate_token_interest_probs(q, tokenizer, peft_model)\n",
    "            probs_list.append({\n",
    "                \"probs\": probs,\n",
    "                \"meta\": row[\"meta\"],\n",
    "            })\n",
    "            a = get_answer(probs)\n",
    "            a_list.append({\n",
    "                \"answer\": a,\n",
    "                \"meta\": row[\"meta\"],\n",
    "            })\n",
    "        print(a_list)\n",
    "        metric_dir_path = get_metric_dir_path(MODEL_EXPERIMENT_NAME, subset, name, split)\n",
    "        print(metric_dir_path)\n",
    "        metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "        with open(metric_dir_path.joinpath(\"probs.jsonl\"), \"w\") as f:\n",
    "            json.dump(probs_list, f, ensure_ascii=False, indent=2)\n",
    "        with open(metric_dir_path.joinpath(\"answers.jsonl\"), \"w\") as f:\n",
    "            json.dump(a_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fea04f-f3f0-4ff0-bc16-12b2467fe1b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference & eval model on landmark passkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7133aa6c-8bb3-4094-9f36-5241f2e07708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:54:09.935036Z",
     "iopub.status.busy": "2024-05-09T17:54:09.934596Z",
     "iopub.status.idle": "2024-05-09T17:54:09.969692Z",
     "shell.execute_reply": "2024-05-09T17:54:09.968895Z",
     "shell.execute_reply.started": "2024-05-09T17:54:09.935013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f946d56f-f859-42bd-bb74-676a1510e183",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:55:28.667256Z",
     "iopub.status.busy": "2024-05-09T17:55:28.666762Z",
     "iopub.status.idle": "2024-05-09T18:03:01.877535Z",
     "shell.execute_reply": "2024-05-09T18:03:01.876833Z",
     "shell.execute_reply.started": "2024-05-09T17:55:28.667235Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models = ['test_pipeline_working']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6c9ca25a10417195880e469a714e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "n_values...:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b59a80a64ea4c1b84938fd37a5b7b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tests...:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test_pipeline_working for n = 14000: 0.98%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b64a5eb5c44431845feebde68f9b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tests...:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test_pipeline_working for n = 18000: 0.0%\n"
     ]
    }
   ],
   "source": [
    "from landmark_run_test import test_passkey_full, load_pipes\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "models = [MODEL_EXPERIMENT_NAME]\n",
    "pipe = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer)\n",
    "pipes = {MODEL_EXPERIMENT_NAME: pipe}\n",
    "test_passkey_full(pipes, models, n_values=[14000, 18000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8dae2-915e-42f9-a952-608e38bdae33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:48:59.387539Z",
     "iopub.status.busy": "2024-05-09T17:48:59.387143Z",
     "iopub.status.idle": "2024-05-09T17:48:59.411426Z",
     "shell.execute_reply": "2024-05-09T17:48:59.410666Z",
     "shell.execute_reply.started": "2024-05-09T17:48:59.387519Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluate model results on tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e723d-b876-425b-90ef-999061b59adf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Evaluate asessors questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a2e89eb-2ef7-418a-9055-8be6b7990b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:08:42.244071Z",
     "iopub.status.busy": "2024-05-09T18:08:42.243698Z",
     "iopub.status.idle": "2024-05-09T18:08:42.283452Z",
     "shell.execute_reply": "2024-05-09T18:08:42.282907Z",
     "shell.execute_reply.started": "2024-05-09T18:08:42.244052Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>diploma</th>\n",
       "      <th>abstract</th>\n",
       "      <th>study_field</th>\n",
       "      <th>degree</th>\n",
       "      <th>original_diploma_extension</th>\n",
       "      <th>raw_model</th>\n",
       "      <th>learnt</th>\n",
       "      <th>learnt_8k</th>\n",
       "      <th>test_pipeline_working</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>45042</td>\n",
       "      <td>2023</td>\n",
       "      <td>–ê–ô–í–ê–ó–¨–Ø–ù –ê—Ä—à–∞–∫ –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á\\n–í—ã–ø—É—Å–∫–Ω–∞—è –∫–≤–∞–ª–∏—Ñ–∏...</td>\n",
       "      <td>–í —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ –º—ã —Å—Ç—Ä–æ–∏–º –ø—Ä–∞–≤—É—é —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–Ω—É—é –º–æ–¥...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω...</td>\n",
       "      <td>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å–Ω–∞—è —Å—Ç—Ä—É...</td>\n",
       "      <td>–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –∞–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–∏–µ —Ç–µ–æ—Ä–∏–∏...</td>\n",
       "      <td>–ê–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–∞—è —Ç–µ–æ—Ä–∏—è ‚Äì —ç—Ç–æ –ø–æ–Ω—è—Ç–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>45043</td>\n",
       "      <td>2023</td>\n",
       "      <td>–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ...</td>\n",
       "      <td>–ü—É—Å—Ç—å ùêæ –≤—ã–ø—É–∫–ª–æ–µ —Ç–µ–ª–æ –≤ ‚Ñù^ùëõ. –û–ø—Ä–µ–¥–µ–ª–∏–º ùëëùëõ,ùëõ‚àí1(...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>–ü–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–µ—à–µ—Ç–∫–∏ —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏–π - —ç—Ç–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è...</td>\n",
       "      <td>–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ä–µ—à–µ—Ç–æ–∫ —Ç—Ä–∞...</td>\n",
       "      <td>–í</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>45044</td>\n",
       "      <td>2023</td>\n",
       "      <td>–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∏–π –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ...</td>\n",
       "      <td>–†–∞–±–æ—Ç–∞ –ø–æ—Å–≤—è—â–µ–Ω–∞ –ø–æ–≤—ã—à–µ–Ω–∏—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏—Å—Å–ª–µ–¥...</td>\n",
       "      <td>–í —Ä–∞–±–æ—Ç–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ –±—É–ª–µ–≤–æ–π –≤—ã–ø–æ–ª–Ω...</td>\n",
       "      <td>In this work we propose a method for improving...</td>\n",
       "      <td>The Boolean satisfiability problem (often abbr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0.1  ...                              test_pipeline_working\n",
       "Unnamed: 0                ...                                                   \n",
       "12                     0  ...  –ê–ª–≥–µ–±—Ä–∞–∏—á–µ—Å–∫–∞—è —Ç–µ–æ—Ä–∏—è ‚Äì —ç—Ç–æ –ø–æ–Ω—è—Ç–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –æ...\n",
       "25                     1  ...                                                 \\n\n",
       "37                     2  ...  The Boolean satisfiability problem (often abbr...\n",
       "\n",
       "[3 rows x 12 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df.set_index(\"Unnamed: 0\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32e9e1f1-2fc2-4fc3-870a-e3468a72a3b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:12:15.132804Z",
     "iopub.status.busy": "2024-05-09T18:12:15.132192Z",
     "iopub.status.idle": "2024-05-09T18:12:17.946204Z",
     "shell.execute_reply": "2024-05-09T18:12:17.945351Z",
     "shell.execute_reply.started": "2024-05-09T18:12:15.132784Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b543d2c8ff5464a907fd913ff4be6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(ARTIFACTS_DIR_PATH.joinpath(\"datasets/diplomas_asessors_questions/mcs_df_human_filled_processed.json\"), \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "a_list = []\n",
    "probs_list = []\n",
    "for row in tqdm(dataset[:first_k], desc=\"Rows...\"):\n",
    "    x = copy.deepcopy(row)\n",
    "    x['inputs']['context'] = new_df[MODEL_EXPERIMENT_NAME].loc[int(x['meta']['id'])] # df[\"diploma\"].iloc[int(x['meta']['id'])]\n",
    "    q = construct_prompt(x)\n",
    "    probs = calculate_token_interest_probs(q, tokenizer, peft_model)\n",
    "    probs_list.append({\n",
    "        \"probs\": probs,\n",
    "        \"meta\": row[\"meta\"],\n",
    "    })\n",
    "    a = get_answer(probs)\n",
    "    a_list.append({\n",
    "        \"answer\": a,\n",
    "        \"meta\": row[\"meta\"],\n",
    "    })\n",
    "metric_dir_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{MODEL_EXPERIMENT_NAME}/{LLAMA_2_7B}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "with open(metric_dir_path.joinpath(\"probs.jsonl\"), \"w\") as f:\n",
    "    json.dump(probs_list, f, ensure_ascii=False, indent=2)\n",
    "with open(metric_dir_path.joinpath(\"answers.jsonl\"), \"w\") as f:\n",
    "    json.dump(a_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ad797c7-5095-47dc-869e-3707631f77b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:12:45.683816Z",
     "iopub.status.busy": "2024-05-09T18:12:45.683333Z",
     "iopub.status.idle": "2024-05-09T18:12:45.736830Z",
     "shell.execute_reply": "2024-05-09T18:12:45.736129Z",
     "shell.execute_reply.started": "2024-05-09T18:12:45.683795Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>subset</th>\n",
       "      <th>split</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>empty</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>diploma_8000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>empty</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>diploma_8000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>learnt</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>learnt_8k</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>test_pipeline_working</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model              subset                  split  accuracy_score\n",
       "0  mistral-7b  asessors_questions               abstract        0.000000\n",
       "1  mistral-7b  asessors_questions                  empty        0.333333\n",
       "2  mistral-7b  asessors_questions           diploma_8000        0.333333\n",
       "3  llama-2-7b  asessors_questions               abstract        0.333333\n",
       "4  llama-2-7b  asessors_questions                  empty        0.333333\n",
       "5  llama-2-7b  asessors_questions           diploma_8000        0.000000\n",
       "6  llama-2-7b  asessors_questions                 learnt        0.000000\n",
       "7  llama-2-7b  asessors_questions              learnt_8k        0.000000\n",
       "8  llama-2-7b  asessors_questions  test_pipeline_working        0.000000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "pred_by_model = dict()\n",
    "for baseline_model in HUGGINFACE_BASELINE_MODELS:\n",
    "    splits = [\"abstract\", \"empty\", \"diploma\"]\n",
    "    if baseline_model == LLAMA_2_7B:\n",
    "        splits.extend([\"learnt\", \"learnt_8k\", MODEL_EXPERIMENT_NAME])\n",
    "    for split in splits:\n",
    "        metric_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{split}/{baseline_model}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "        with open(metric_path.joinpath(\"answers.jsonl\"), \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "        pred = [x[\"answer\"] for x in answers[:first_k]]\n",
    "        pred_by_model[f\"{split}_{baseline_model}\"] = pred\n",
    "        true = [x[\"outputs\"] for x in dataset[:first_k]]\n",
    "        rows.append({\n",
    "            \"model\": baseline_model,\n",
    "            \"subset\": \"asessors_questions\",\n",
    "            \"split\": f\"{split}_8000\" if split == \"diploma\" else split,\n",
    "            \"accuracy_score\": accuracy_score(true, pred),\n",
    "        })\n",
    "df = pd.DataFrame(rows)\n",
    "# df.to_csv(METRICS_DIR_PATH.joinpath(\"asessors.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff94c80-17ec-4220-a3cc-826719a31832",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Evaluate MERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3493a575-dc86-463b-b981-96afcda8a722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:18:01.225779Z",
     "iopub.status.busy": "2024-05-09T18:18:01.225327Z",
     "iopub.status.idle": "2024-05-09T18:18:05.316589Z",
     "shell.execute_reply": "2024-05-09T18:18:05.315875Z",
     "shell.execute_reply.started": "2024-05-09T18:18:01.225760Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>subset</th>\n",
       "      <th>split</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>saiga_mistral_7b_lora</td>\n",
       "      <td>rummlu</td>\n",
       "      <td>public_test</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>saiga_mistral_7b_lora</td>\n",
       "      <td>ruopenbookqa</td>\n",
       "      <td>train</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>rummlu</td>\n",
       "      <td>public_test</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>ruopenbookqa</td>\n",
       "      <td>train</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_pipeline_working</td>\n",
       "      <td>rummlu</td>\n",
       "      <td>public_test</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_pipeline_working</td>\n",
       "      <td>ruopenbookqa</td>\n",
       "      <td>train</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vikhr-7b</td>\n",
       "      <td>rummlu</td>\n",
       "      <td>public_test</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vikhr-7b</td>\n",
       "      <td>ruopenbookqa</td>\n",
       "      <td>train</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>rummlu</td>\n",
       "      <td>public_test</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>ruopenbookqa</td>\n",
       "      <td>train</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model        subset        split  accuracy_score\n",
       "0  saiga_mistral_7b_lora        rummlu  public_test        0.333333\n",
       "1  saiga_mistral_7b_lora  ruopenbookqa        train        1.000000\n",
       "2             mistral-7b        rummlu  public_test        0.666667\n",
       "3             mistral-7b  ruopenbookqa        train        1.000000\n",
       "4  test_pipeline_working        rummlu  public_test        0.333333\n",
       "5  test_pipeline_working  ruopenbookqa        train        0.000000\n",
       "6               vikhr-7b        rummlu  public_test        0.333333\n",
       "7               vikhr-7b  ruopenbookqa        train        1.000000\n",
       "8             llama-2-7b        rummlu  public_test        0.333333\n",
       "9             llama-2-7b  ruopenbookqa        train        0.000000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for baseline_model in HUGGINFACE_BASELINE_MODELS.union([VIKHR_7B, SAIGA_MISTRAL_7B_LORA]).union([MODEL_EXPERIMENT_NAME]):\n",
    "    for name, dataset_meta in HUGGINGFACE_NAME_TO_DATASET.items():\n",
    "        for subset, split in zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]):\n",
    "            dataset_path = get_dataset_path(subset, name, split)\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "            metric_path = get_metric_dir_path(baseline_model, subset, name, split)\n",
    "            with open(metric_path.joinpath(\"answers.jsonl\"), \"r\") as f:\n",
    "                answers = json.load(f)\n",
    "            pred = [x[\"answer\"] for x in answers[:first_k]]\n",
    "            true = [x[\"outputs\"] for x in list(dataset)[:first_k]]\n",
    "            rows.append({\n",
    "                \"model\": baseline_model,\n",
    "                \"subset\": subset,\n",
    "                \"split\": split,\n",
    "                \"accuracy_score\": accuracy_score(true, pred),\n",
    "            })\n",
    "df = pd.DataFrame(rows)\n",
    "# df.to_csv(METRICS_DIR_PATH/\"ru_metrics.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79a564-eeb3-436b-8469-e22603874bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
