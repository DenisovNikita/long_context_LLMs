{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fd1ea6-4c85-457f-8dcc-f3e3a696ee96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdea8f-48ea-49ee-a487-5bf5de98deb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5998e0e-0188-4d18-88b6-a634a5d69ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:24:30.963750Z",
     "iopub.status.busy": "2024-05-09T18:24:30.963440Z",
     "iopub.status.idle": "2024-05-09T18:24:31.044340Z",
     "shell.execute_reply": "2024-05-09T18:24:31.043761Z",
     "shell.execute_reply.started": "2024-05-09T18:24:30.963730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0360d445-d3ad-4007-8972-5816c93e9b73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:24:34.750740Z",
     "iopub.status.busy": "2024-05-09T18:24:34.750376Z",
     "iopub.status.idle": "2024-05-09T18:24:34.770946Z",
     "shell.execute_reply": "2024-05-09T18:24:34.770435Z",
     "shell.execute_reply.started": "2024-05-09T18:24:34.750720Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67388f45076348989df7c526d183c95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fb725155-72a6-403d-9c18-e1da14092bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:24:44.046788Z",
     "iopub.status.busy": "2024-05-09T18:24:44.046372Z",
     "iopub.status.idle": "2024-05-09T18:24:44.061236Z",
     "shell.execute_reply": "2024-05-09T18:24:44.060794Z",
     "shell.execute_reply.started": "2024-05-09T18:24:44.046765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../utils\")\n",
    "from definitions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c446f708-d8ae-415e-bc88-cc77e234a2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:24:48.470539Z",
     "iopub.status.busy": "2024-05-09T18:24:48.470099Z",
     "iopub.status.idle": "2024-05-09T18:24:48.482211Z",
     "shell.execute_reply": "2024-05-09T18:24:48.481810Z",
     "shell.execute_reply.started": "2024-05-09T18:24:48.470516Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(\"../../../../cache/\")\n",
    "DATASET_DIR = Path(\"/home/jupyter/mnt/datasets/diplomas/russian_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db3f5d5d-9db1-4862-8308-3b80e2dfdd56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T19:52:19.801377Z",
     "iopub.status.busy": "2024-05-09T19:52:19.800991Z",
     "iopub.status.idle": "2024-05-09T19:52:19.830651Z",
     "shell.execute_reply": "2024-05-09T19:52:19.830129Z",
     "shell.execute_reply.started": "2024-05-09T19:52:19.801358Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_EXPERIMENT_NAME = \"learnt_8k_retest\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f34b3f-f3e1-4375-a566-dcec395755d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "42e56e8e-d59f-4c38-b0b1-489d83518a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:03:04.157261Z",
     "iopub.status.busy": "2024-05-09T20:03:04.156854Z",
     "iopub.status.idle": "2024-05-09T20:03:10.287046Z",
     "shell.execute_reply": "2024-05-09T20:03:10.286470Z",
     "shell.execute_reply.started": "2024-05-09T20:03:04.157242Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.58s/it]\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model_name = LLAMA_2_7B\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    device_map='auto'\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "MODEL_MAX_LENGTH = 16384\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    model_max_length=MODEL_MAX_LENGTH,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=special_tokens_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9001686d-9a4b-4eb5-8db6-1bd51622d329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:03:10.288327Z",
     "iopub.status.busy": "2024-05-09T20:03:10.288063Z",
     "iopub.status.idle": "2024-05-09T20:03:11.997028Z",
     "shell.execute_reply": "2024-05-09T20:03:11.996398Z",
     "shell.execute_reply.started": "2024-05-09T20:03:10.288309Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"nvdenisov2002/llama-longLoRA-v2\"\n",
    "peft_model = PeftModel.from_pretrained(model, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47199b-36fd-4a22-b2c2-ace036f3abff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference model on tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "468c29dd-047e-4fbf-9090-ac9f10b50cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:25:10.014102Z",
     "iopub.status.busy": "2024-05-09T18:25:10.013731Z",
     "iopub.status.idle": "2024-05-09T18:25:10.038264Z",
     "shell.execute_reply": "2024-05-09T18:25:10.037816Z",
     "shell.execute_reply.started": "2024-05-09T18:25:10.014076Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_k = int(1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2b5e4-04d8-456e-a2d1-21421982f2e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference model on asessors questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ffb389dc-bc40-45a6-96c3-4af0b2dfb231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T20:03:28.985986Z",
     "iopub.status.busy": "2024-05-09T20:03:28.985632Z",
     "iopub.status.idle": "2024-05-09T20:08:08.226773Z",
     "shell.execute_reply": "2024-05-09T20:08:08.225273Z",
     "shell.execute_reply.started": "2024-05-09T20:03:28.985967Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951b49bd3a9044d487658670f630e1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 4316, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 4170, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 5251, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.29 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-7737fe3961fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfirst_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfirst_k\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Rows...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnew_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnew_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMODEL_EXPERIMENT_NAME\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_some_model_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiploma_prefix_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnew_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/resources/long_context_LLMs/src/notebooks/metrics_calculation/../../utils/llm_helpers.py\u001b[0m in \u001b[0;36mget_some_model_result\u001b[0;34m(some_model, tokenizer, row, device, diploma_prefix_len)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mprefix_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prefix_len_and_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiploma_prefix_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0msome_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msome_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mgenerated_continue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefix_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated_continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2735\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 )\n\u001b[1;32m    924\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    926\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    636\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.29 GiB. GPU "
     ]
    }
   ],
   "source": [
    "from llm_helpers import get_some_model_result\n",
    "\n",
    "test_df = pd.read_csv(ARTIFACTS_DIR_PATH.joinpath(\"diplomas_abstracts/mcs_raw_learnt_abstract_learnt8k.csv\"))\n",
    "\n",
    "new_rows = []\n",
    "for _, row in tqdm(test_df[:first_k].iterrows(), total=len(test_df[:first_k]), desc=\"Rows...\"):\n",
    "    new_row = copy.deepcopy(row)\n",
    "    new_row[MODEL_EXPERIMENT_NAME] = get_some_model_result(peft_model, tokenizer, row, device, diploma_prefix_len=8000)\n",
    "    new_rows.append(new_row)\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ba9b7-6c92-46ab-8732-7a7b4f8bd1ad",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.227464Z",
     "iopub.status.idle": "2024-05-09T20:08:08.227822Z",
     "shell.execute_reply": "2024-05-09T20:08:08.227695Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.to_csv(ARTIFACTS_DIR_PATH.joinpath(f\"diplomas_abstracts/baselines_with_{MODEL_EXPERIMENT_NAME}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036997e3-5d02-4db2-97f4-abdacfd54940",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.228377Z",
     "iopub.status.idle": "2024-05-09T20:08:08.228708Z",
     "shell.execute_reply": "2024-05-09T20:08:08.228589Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8dcf64-b373-4f91-bab2-fadaaf7c920e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference model on MERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707c6f4-eacf-45d4-9785-bc5e45457aa6",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.229380Z",
     "iopub.status.idle": "2024-05-09T20:08:08.229683Z",
     "shell.execute_reply": "2024-05-09T20:08:08.229577Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mera_helpers import construct_prompt\n",
    "from path_helpers import get_dataset_path, get_metric_dir_path\n",
    "from llm_helpers import calculate_token_interest_probs, get_answer\n",
    "\n",
    "for name, dataset_meta in tqdm(HUGGINGFACE_NAME_TO_DATASET.items(), desc=\"Datasets...\"):\n",
    "    for subset, split in tqdm(zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]), total=len(dataset_meta[\"splits\"]), desc=\"Splits...\"):\n",
    "        path = get_dataset_path(subset, name, split)\n",
    "        dataset = load_from_disk(path)\n",
    "        probs_list = []\n",
    "        a_list = []\n",
    "        for row in tqdm(list(dataset)[:first_k], desc=\"Rows...\"):\n",
    "            q = construct_prompt(row)\n",
    "            probs = calculate_token_interest_probs(q, tokenizer, peft_model)\n",
    "            probs_list.append({\n",
    "                \"probs\": probs,\n",
    "                \"meta\": row[\"meta\"],\n",
    "            })\n",
    "            a = get_answer(probs)\n",
    "            a_list.append({\n",
    "                \"answer\": a,\n",
    "                \"meta\": row[\"meta\"],\n",
    "            })\n",
    "        print(a_list)\n",
    "        metric_dir_path = get_metric_dir_path(MODEL_EXPERIMENT_NAME, subset, name, split)\n",
    "        print(metric_dir_path)\n",
    "        metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "        with open(metric_dir_path.joinpath(\"probs.jsonl\"), \"w\") as f:\n",
    "            json.dump(probs_list, f, ensure_ascii=False, indent=2)\n",
    "        with open(metric_dir_path.joinpath(\"answers.jsonl\"), \"w\") as f:\n",
    "            json.dump(a_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fea04f-f3f0-4ff0-bc16-12b2467fe1b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference & eval model on landmark passkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946d56f-f859-42bd-bb74-676a1510e183",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.230260Z",
     "iopub.status.idle": "2024-05-09T20:08:08.230584Z",
     "shell.execute_reply": "2024-05-09T20:08:08.230465Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from landmark_run_test import test_passkey_full, load_pipes\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "models = [MODEL_EXPERIMENT_NAME]\n",
    "pipe = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer)\n",
    "pipes = {MODEL_EXPERIMENT_NAME: pipe}\n",
    "test_passkey_full(pipes, models, n_values=[14000, 18000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8dae2-915e-42f9-a952-608e38bdae33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:48:59.387539Z",
     "iopub.status.busy": "2024-05-09T17:48:59.387143Z",
     "iopub.status.idle": "2024-05-09T17:48:59.411426Z",
     "shell.execute_reply": "2024-05-09T17:48:59.410666Z",
     "shell.execute_reply.started": "2024-05-09T17:48:59.387519Z"
    },
    "tags": []
   },
   "source": [
    "## Evaluate model results on tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e723d-b876-425b-90ef-999061b59adf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate asessors questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e89eb-2ef7-418a-9055-8be6b7990b29",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.231094Z",
     "iopub.status.idle": "2024-05-09T20:08:08.231429Z",
     "shell.execute_reply": "2024-05-09T20:08:08.231304Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = new_df.set_index(\"Unnamed: 0\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9e1f1-2fc2-4fc3-870a-e3468a72a3b0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.232149Z",
     "iopub.status.idle": "2024-05-09T20:08:08.232514Z",
     "shell.execute_reply": "2024-05-09T20:08:08.232396Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(ARTIFACTS_DIR_PATH.joinpath(\"datasets/diplomas_asessors_questions/mcs_df_human_filled_processed.json\"), \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "a_list = []\n",
    "probs_list = []\n",
    "for row in tqdm(dataset[:first_k], desc=\"Rows...\"):\n",
    "    x = copy.deepcopy(row)\n",
    "    x['inputs']['context'] = new_df[MODEL_EXPERIMENT_NAME].loc[int(x['meta']['id'])] # df[\"diploma\"].iloc[int(x['meta']['id'])]\n",
    "    q = construct_prompt(x)\n",
    "    probs = calculate_token_interest_probs(q, tokenizer, peft_model)\n",
    "    probs_list.append({\n",
    "        \"probs\": probs,\n",
    "        \"meta\": row[\"meta\"],\n",
    "    })\n",
    "    a = get_answer(probs)\n",
    "    a_list.append({\n",
    "        \"answer\": a,\n",
    "        \"meta\": row[\"meta\"],\n",
    "    })\n",
    "metric_dir_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{MODEL_EXPERIMENT_NAME}/{LLAMA_2_7B}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "with open(metric_dir_path.joinpath(\"probs.jsonl\"), \"w\") as f:\n",
    "    json.dump(probs_list, f, ensure_ascii=False, indent=2)\n",
    "with open(metric_dir_path.joinpath(\"answers.jsonl\"), \"w\") as f:\n",
    "    json.dump(a_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad797c7-5095-47dc-869e-3707631f77b8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.233041Z",
     "iopub.status.idle": "2024-05-09T20:08:08.233370Z",
     "shell.execute_reply": "2024-05-09T20:08:08.233248Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "pred_by_model = dict()\n",
    "for baseline_model in HUGGINFACE_BASELINE_MODELS:\n",
    "    splits = [\"abstract\", \"empty\", \"diploma\"]\n",
    "    if baseline_model == LLAMA_2_7B:\n",
    "        splits.extend([\"learnt\", \"learnt_8k\", MODEL_EXPERIMENT_NAME])\n",
    "    for split in splits:\n",
    "        metric_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{split}/{baseline_model}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "        with open(metric_path.joinpath(\"answers.jsonl\"), \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "        pred = [x[\"answer\"] for x in answers[:first_k]]\n",
    "        pred_by_model[f\"{split}_{baseline_model}\"] = pred\n",
    "        true = [x[\"outputs\"] for x in dataset[:first_k]]\n",
    "        rows.append({\n",
    "            \"model\": baseline_model,\n",
    "            \"subset\": \"asessors_questions\",\n",
    "            \"split\": f\"{split}_8000\" if split == \"diploma\" else split,\n",
    "            \"accuracy_score\": accuracy_score(true, pred),\n",
    "        })\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209da0f-75d5-45af-b837-06433293d59d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.233926Z",
     "iopub.status.idle": "2024-05-09T20:08:08.234297Z",
     "shell.execute_reply": "2024-05-09T20:08:08.234177Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(METRICS_DIR_PATH.joinpath(f\"asessors_with_{MODEL_EXPERIMENT_NAME}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff94c80-17ec-4220-a3cc-826719a31832",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate MERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493a575-dc86-463b-b981-96afcda8a722",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.234936Z",
     "iopub.status.idle": "2024-05-09T20:08:08.235407Z",
     "shell.execute_reply": "2024-05-09T20:08:08.235276Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for baseline_model in HUGGINFACE_BASELINE_MODELS.union([VIKHR_7B, SAIGA_MISTRAL_7B_LORA]).union([MODEL_EXPERIMENT_NAME]):\n",
    "    for name, dataset_meta in HUGGINGFACE_NAME_TO_DATASET.items():\n",
    "        for subset, split in zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]):\n",
    "            dataset_path = get_dataset_path(subset, name, split)\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "            metric_path = get_metric_dir_path(baseline_model, subset, name, split)\n",
    "            with open(metric_path.joinpath(\"answers.jsonl\"), \"r\") as f:\n",
    "                answers = json.load(f)\n",
    "            pred = [x[\"answer\"] for x in answers[:first_k]]\n",
    "            true = [x[\"outputs\"] for x in list(dataset)[:first_k]]\n",
    "            rows.append({\n",
    "                \"model\": baseline_model,\n",
    "                \"subset\": subset,\n",
    "                \"split\": split,\n",
    "                \"accuracy_score\": accuracy_score(true, pred),\n",
    "            })\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a165a-a392-4811-b00c-f564391fa6c0",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-09T20:08:08.235953Z",
     "iopub.status.idle": "2024-05-09T20:08:08.236246Z",
     "shell.execute_reply": "2024-05-09T20:08:08.236138Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(METRICS_DIR_PATH.joinpath(f\"ru_metrics_with_{MODEL_EXPERIMENT_NAME}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79a564-eeb3-436b-8469-e22603874bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
