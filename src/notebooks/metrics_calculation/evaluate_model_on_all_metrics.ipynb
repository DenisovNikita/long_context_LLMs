{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94fd1ea6-4c85-457f-8dcc-f3e3a696ee96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prepare model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdea8f-48ea-49ee-a487-5bf5de98deb9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5998e0e-0188-4d18-88b6-a634a5d69ee5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:26:43.018631Z",
     "iopub.status.busy": "2024-05-10T00:26:43.018192Z",
     "iopub.status.idle": "2024-05-10T00:26:43.034210Z",
     "shell.execute_reply": "2024-05-10T00:26:43.033758Z",
     "shell.execute_reply.started": "2024-05-10T00:26:43.018609Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"`do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0360d445-d3ad-4007-8972-5816c93e9b73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:26:43.419808Z",
     "iopub.status.busy": "2024-05-10T00:26:43.419478Z",
     "iopub.status.idle": "2024-05-10T00:26:44.225406Z",
     "shell.execute_reply": "2024-05-10T00:26:44.224894Z",
     "shell.execute_reply.started": "2024-05-10T00:26:43.419789Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d52ba23ee9240eca21e22926bcf655d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb725155-72a6-403d-9c18-e1da14092bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:27:26.607627Z",
     "iopub.status.busy": "2024-05-10T00:27:26.607221Z",
     "iopub.status.idle": "2024-05-10T00:27:26.620629Z",
     "shell.execute_reply": "2024-05-10T00:27:26.619979Z",
     "shell.execute_reply.started": "2024-05-10T00:27:26.607606Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../utils\")\n",
    "from definitions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c446f708-d8ae-415e-bc88-cc77e234a2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:27:27.146840Z",
     "iopub.status.busy": "2024-05-10T00:27:27.146449Z",
     "iopub.status.idle": "2024-05-10T00:27:27.160439Z",
     "shell.execute_reply": "2024-05-10T00:27:27.159781Z",
     "shell.execute_reply.started": "2024-05-10T00:27:27.146822Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(\"../../../../cache/\")\n",
    "DATASET_DIR = Path(\"/home/jupyter/mnt/datasets/diplomas/russian_dataset/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db3f5d5d-9db1-4862-8308-3b80e2dfdd56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:27:27.625626Z",
     "iopub.status.busy": "2024-05-10T00:27:27.625145Z",
     "iopub.status.idle": "2024-05-10T00:27:27.641827Z",
     "shell.execute_reply": "2024-05-10T00:27:27.641188Z",
     "shell.execute_reply.started": "2024-05-10T00:27:27.625593Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_EXPERIMENT_NAME = \"learnt_8k_retest_greedy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f34b3f-f3e1-4375-a566-dcec395755d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42e56e8e-d59f-4c38-b0b1-489d83518a94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:27:52.017360Z",
     "iopub.status.busy": "2024-05-10T00:27:52.016928Z",
     "iopub.status.idle": "2024-05-10T00:27:58.032820Z",
     "shell.execute_reply": "2024-05-10T00:27:58.032080Z",
     "shell.execute_reply.started": "2024-05-10T00:27:52.017340Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.44s/it]\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model_name = LLAMA_2_7B\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    device_map='auto'\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "MODEL_MAX_LENGTH = 16384\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    model_max_length=MODEL_MAX_LENGTH,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=special_tokens_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9001686d-9a4b-4eb5-8db6-1bd51622d329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:27:58.034060Z",
     "iopub.status.busy": "2024-05-10T00:27:58.033784Z",
     "iopub.status.idle": "2024-05-10T00:28:39.596428Z",
     "shell.execute_reply": "2024-05-10T00:28:39.594944Z",
     "shell.execute_reply.started": "2024-05-10T00:27:58.034041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading adapter_config.json: 100%|██████████| 674/674 [00:00<00:00, 4.95MB/s]\n",
      "Downloading adapter_model.bin: 100%|██████████| 1.08G/1.08G [00:30<00:00, 35.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"nvdenisov2002/llama-longLoRA-v2\"\n",
    "peft_model = PeftModel.from_pretrained(model, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47199b-36fd-4a22-b2c2-ace036f3abff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Inference model on tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "468c29dd-047e-4fbf-9090-ac9f10b50cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:28:39.598090Z",
     "iopub.status.busy": "2024-05-10T00:28:39.597762Z",
     "iopub.status.idle": "2024-05-10T00:28:39.614071Z",
     "shell.execute_reply": "2024-05-10T00:28:39.613246Z",
     "shell.execute_reply.started": "2024-05-10T00:28:39.598064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_k = int(1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d1a7157-5a13-4b1a-bfcd-d44ca832c094",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:28:39.615303Z",
     "iopub.status.busy": "2024-05-10T00:28:39.614992Z",
     "iopub.status.idle": "2024-05-10T00:28:39.626028Z",
     "shell.execute_reply": "2024-05-10T00:28:39.625396Z",
     "shell.execute_reply.started": "2024-05-10T00:28:39.615285Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa2b5e4-04d8-456e-a2d1-21421982f2e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference model on asessors questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb389dc-bc40-45a6-96c3-4af0b2dfb231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:28:39.627159Z",
     "iopub.status.busy": "2024-05-10T00:28:39.626653Z",
     "iopub.status.idle": "2024-05-10T00:41:07.164081Z",
     "shell.execute_reply": "2024-05-10T00:41:07.163371Z",
     "shell.execute_reply.started": "2024-05-10T00:28:39.627139Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ee251056f040e1ac0c033bd255ce1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 4316, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 4170, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 5251, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 5349, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 4808, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 5726, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>diploma</th>\n",
       "      <th>abstract</th>\n",
       "      <th>study_field</th>\n",
       "      <th>degree</th>\n",
       "      <th>original_diploma_extension</th>\n",
       "      <th>raw_model</th>\n",
       "      <th>learnt</th>\n",
       "      <th>learnt_8k</th>\n",
       "      <th>learnt_8k_retest_greedy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>45042</td>\n",
       "      <td>2023</td>\n",
       "      <td>АЙВАЗЬЯН Аршак Владимирович\\nВыпускная квалифи...</td>\n",
       "      <td>В этой работе мы строим правую трансферную мод...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "      <td>В данной работе рассматривается модельная стру...</td>\n",
       "      <td>В работе рассматриваются алгебраические теории...</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>45043</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Пусть 𝐾 выпуклое тело в ℝ^𝑛. Определим 𝑑𝑛,𝑛−1(...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Плотность решетки трансляций - это минимальная...</td>\n",
       "      <td>В работе рассматриваются плотности решеток тра...</td>\n",
       "      <td>В</td>\n",
       "      <td>В</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>45044</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Работа посвящена повышению производительности ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе представлены результаты исслед...</td>\n",
       "      <td>В работе рассматривается задача булевой выполн...</td>\n",
       "      <td>In this work we propose a method for improving...</td>\n",
       "      <td>В данной работе рассматривается решение задачи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>45046</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются классы случайных проц...</td>\n",
       "      <td>В данной работе рассматривается энергетически-...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>45047</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы рассматривается подход ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе рассматривается задача настраи...</td>\n",
       "      <td>В данной работе рассматривается задача добавле...</td>\n",
       "      <td>В работе рассматривается применение добавления...</td>\n",
       "      <td>В данной работе рассматривается применение доб...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  ...                            learnt_8k_retest_greedy\n",
       "0             0  ...  В работе рассматривается модельная структура н...\n",
       "1             1  ...                                                  В\n",
       "2             2  ...  В данной работе рассматривается решение задачи...\n",
       "3             3  ...  В работе мы обобщаем результаты об энергии нат...\n",
       "4             4  ...  В данной работе рассматривается применение доб...\n",
       "\n",
       "[5 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_helpers import get_some_model_result\n",
    "\n",
    "test_df = pd.read_csv(ARTIFACTS_DIR_PATH.joinpath(\"diplomas_abstracts/mcs_raw_learnt_abstract_learnt8k.csv\"))\n",
    "\n",
    "new_rows = []\n",
    "for _, row in tqdm(test_df[:first_k].iterrows(), total=len(test_df[:first_k]), desc=\"Rows...\"):\n",
    "    new_row = copy.deepcopy(row)\n",
    "    new_row[MODEL_EXPERIMENT_NAME] = get_some_model_result(peft_model, tokenizer, row, device, diploma_prefix_len=8000)\n",
    "    new_rows.append(new_row)\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "384c943a-3ae6-46f5-b4a3-ffc1bd748204",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:41:07.165231Z",
     "iopub.status.busy": "2024-05-10T00:41:07.164921Z",
     "iopub.status.idle": "2024-05-10T00:41:07.193749Z",
     "shell.execute_reply": "2024-05-10T00:41:07.193072Z",
     "shell.execute_reply.started": "2024-05-10T00:41:07.165205Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>diploma</th>\n",
       "      <th>abstract</th>\n",
       "      <th>study_field</th>\n",
       "      <th>degree</th>\n",
       "      <th>original_diploma_extension</th>\n",
       "      <th>raw_model</th>\n",
       "      <th>learnt</th>\n",
       "      <th>learnt_8k</th>\n",
       "      <th>learnt_8k_retest_greedy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>45042</td>\n",
       "      <td>2023</td>\n",
       "      <td>АЙВАЗЬЯН Аршак Владимирович\\nВыпускная квалифи...</td>\n",
       "      <td>В этой работе мы строим правую трансферную мод...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "      <td>В данной работе рассматривается модельная стру...</td>\n",
       "      <td>В работе рассматриваются алгебраические теории...</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>45043</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Пусть 𝐾 выпуклое тело в ℝ^𝑛. Определим 𝑑𝑛,𝑛−1(...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Плотность решетки трансляций - это минимальная...</td>\n",
       "      <td>В работе рассматриваются плотности решеток тра...</td>\n",
       "      <td>В</td>\n",
       "      <td>В</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>45044</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Работа посвящена повышению производительности ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе представлены результаты исслед...</td>\n",
       "      <td>В работе рассматривается задача булевой выполн...</td>\n",
       "      <td>In this work we propose a method for improving...</td>\n",
       "      <td>В данной работе рассматривается решение задачи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>45046</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются классы случайных проц...</td>\n",
       "      <td>В данной работе рассматривается энергетически-...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>45047</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы рассматривается подход ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе рассматривается задача настраи...</td>\n",
       "      <td>В данной работе рассматривается задача добавле...</td>\n",
       "      <td>В работе рассматривается применение добавления...</td>\n",
       "      <td>В данной работе рассматривается применение доб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>1062</td>\n",
       "      <td>45131</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В настоящей работе были рассмотрены две задачи...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>MASTER'S STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются морфизмы, порождающие...</td>\n",
       "      <td>В работе рассматриваются некоторые гипотезы о ...</td>\n",
       "      <td>В данной работе изучаются некоторые гипотезы о...</td>\n",
       "      <td>В данной работе рассматриваются некоторые гипо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>1064</td>\n",
       "      <td>45132</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В данной работе исследовались MS/MS спектры, с...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Данная работа посвящена разработке алгоритмов ...</td>\n",
       "      <td>Данная работа посвящена поиску и классификации...</td>\n",
       "      <td>Данная работа посвящена исследованию поведения...</td>\n",
       "      <td>В данной работе рассматриваются масс-спектры б...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>67</td>\n",
       "      <td>1079</td>\n",
       "      <td>45133</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В выпускной работе описываются триангуляции ве...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются асимптотики числа три...</td>\n",
       "      <td>Автор производит подсчёт асимптотики числа три...</td>\n",
       "      <td>Рассматриваются выпуклые триангуляции, то есть...</td>\n",
       "      <td>В данной работе изучаются выпуклые триангуляци...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>1209</td>\n",
       "      <td>45135</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы было разработано програ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной выпускной квалификационной работе пре...</td>\n",
       "      <td>Работа посвящена разработке программного средс...</td>\n",
       "      <td>В работе рассмотрена проблема автоматической и...</td>\n",
       "      <td>В данной работе реализованы алгоритмы автомати...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>69</td>\n",
       "      <td>1238</td>\n",
       "      <td>45137</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В данной работе представлены реализация фронте...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Данная выпускная квалификационная работа посвя...</td>\n",
       "      <td>Работа посвящена разработке архитектуры и созд...</td>\n",
       "      <td>Работа состоит из 4 глав:\\n• Первая глава – вв...</td>\n",
       "      <td>В данной работе рассматривается архитектура и ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0.1  ...                            learnt_8k_retest_greedy\n",
       "0              0  ...  В работе рассматривается модельная структура н...\n",
       "1              1  ...                                                  В\n",
       "2              2  ...  В данной работе рассматривается решение задачи...\n",
       "3              3  ...  В работе мы обобщаем результаты об энергии нат...\n",
       "4              4  ...  В данной работе рассматривается применение доб...\n",
       "..           ...  ...                                                ...\n",
       "65            65  ...  В данной работе рассматриваются некоторые гипо...\n",
       "66            66  ...  В данной работе рассматриваются масс-спектры б...\n",
       "67            67  ...  В данной работе изучаются выпуклые триангуляци...\n",
       "68            68  ...  В данной работе реализованы алгоритмы автомати...\n",
       "69            69  ...  В данной работе рассматривается архитектура и ...\n",
       "\n",
       "[70 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "779ba9b7-6c92-46ab-8732-7a7b4f8bd1ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:41:07.195705Z",
     "iopub.status.busy": "2024-05-10T00:41:07.195448Z",
     "iopub.status.idle": "2024-05-10T00:41:07.325177Z",
     "shell.execute_reply": "2024-05-10T00:41:07.324416Z",
     "shell.execute_reply.started": "2024-05-10T00:41:07.195686Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.to_csv(ARTIFACTS_DIR_PATH.joinpath(f\"diplomas_abstracts/baselines_with_{MODEL_EXPERIMENT_NAME}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036997e3-5d02-4db2-97f4-abdacfd54940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8dcf64-b373-4f91-bab2-fadaaf7c920e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference model on MERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707c6f4-eacf-45d4-9785-bc5e45457aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mera_helpers import construct_prompt\n",
    "from path_helpers import get_dataset_path, get_metric_dir_path\n",
    "from llm_helpers import calculate_token_interest_probs, get_answer\n",
    "\n",
    "for name, dataset_meta in tqdm(HUGGINGFACE_NAME_TO_DATASET.items(), desc=\"Datasets...\"):\n",
    "    for subset, split in tqdm(zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]), total=len(dataset_meta[\"splits\"]), desc=\"Splits...\"):\n",
    "        path = get_dataset_path(subset, name, split)\n",
    "        dataset = load_from_disk(path)\n",
    "        probs_list = []\n",
    "        a_list = []\n",
    "        for row in tqdm(list(dataset)[:first_k], desc=\"Rows...\"):\n",
    "            q = construct_prompt(row)\n",
    "            probs = calculate_token_interest_probs(q, tokenizer, peft_model)\n",
    "            probs_list.append({\n",
    "                \"probs\": probs,\n",
    "                \"meta\": row[\"meta\"],\n",
    "            })\n",
    "            a = get_answer(probs)\n",
    "            a_list.append({\n",
    "                \"answer\": a,\n",
    "                \"meta\": row[\"meta\"],\n",
    "            })\n",
    "        print(a_list)\n",
    "        metric_dir_path = get_metric_dir_path(MODEL_EXPERIMENT_NAME, subset, name, split)\n",
    "        print(metric_dir_path)\n",
    "        metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "        with open(metric_dir_path.joinpath(\"probs.jsonl\"), \"w\") as f:\n",
    "            json.dump(probs_list, f, ensure_ascii=False, indent=2)\n",
    "        with open(metric_dir_path.joinpath(\"answers.jsonl\"), \"w\") as f:\n",
    "            json.dump(a_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fea04f-f3f0-4ff0-bc16-12b2467fe1b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Inference & eval model on landmark passkey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946d56f-f859-42bd-bb74-676a1510e183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from landmark_run_test import test_passkey_full, load_pipes\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "models = [MODEL_EXPERIMENT_NAME]\n",
    "pipe = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer)\n",
    "pipes = {MODEL_EXPERIMENT_NAME: pipe}\n",
    "test_passkey_full(pipes, models, n_values=[14000, 18000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8dae2-915e-42f9-a952-608e38bdae33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T17:48:59.387539Z",
     "iopub.status.busy": "2024-05-09T17:48:59.387143Z",
     "iopub.status.idle": "2024-05-09T17:48:59.411426Z",
     "shell.execute_reply": "2024-05-09T17:48:59.410666Z",
     "shell.execute_reply.started": "2024-05-09T17:48:59.387519Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluate model results on tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e723d-b876-425b-90ef-999061b59adf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate asessors questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a2e89eb-2ef7-418a-9055-8be6b7990b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:41:07.326484Z",
     "iopub.status.busy": "2024-05-10T00:41:07.326215Z",
     "iopub.status.idle": "2024-05-10T00:41:07.370686Z",
     "shell.execute_reply": "2024-05-10T00:41:07.370075Z",
     "shell.execute_reply.started": "2024-05-10T00:41:07.326465Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>diploma</th>\n",
       "      <th>abstract</th>\n",
       "      <th>study_field</th>\n",
       "      <th>degree</th>\n",
       "      <th>original_diploma_extension</th>\n",
       "      <th>raw_model</th>\n",
       "      <th>learnt</th>\n",
       "      <th>learnt_8k</th>\n",
       "      <th>learnt_8k_retest_greedy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>45042</td>\n",
       "      <td>2023</td>\n",
       "      <td>АЙВАЗЬЯН Аршак Владимирович\\nВыпускная квалифи...</td>\n",
       "      <td>В этой работе мы строим правую трансферную мод...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "      <td>В данной работе рассматривается модельная стру...</td>\n",
       "      <td>В работе рассматриваются алгебраические теории...</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>45043</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Пусть 𝐾 выпуклое тело в ℝ^𝑛. Определим 𝑑𝑛,𝑛−1(...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Плотность решетки трансляций - это минимальная...</td>\n",
       "      <td>В работе рассматриваются плотности решеток тра...</td>\n",
       "      <td>В</td>\n",
       "      <td>В</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2</td>\n",
       "      <td>45044</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Работа посвящена повышению производительности ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе представлены результаты исслед...</td>\n",
       "      <td>В работе рассматривается задача булевой выполн...</td>\n",
       "      <td>In this work we propose a method for improving...</td>\n",
       "      <td>В данной работе рассматривается решение задачи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>3</td>\n",
       "      <td>45046</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются классы случайных проц...</td>\n",
       "      <td>В данной работе рассматривается энергетически-...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>4</td>\n",
       "      <td>45047</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы рассматривается подход ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе рассматривается задача настраи...</td>\n",
       "      <td>В данной работе рассматривается задача добавле...</td>\n",
       "      <td>В работе рассматривается применение добавления...</td>\n",
       "      <td>В данной работе рассматривается применение доб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>65</td>\n",
       "      <td>45131</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В настоящей работе были рассмотрены две задачи...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>MASTER'S STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются морфизмы, порождающие...</td>\n",
       "      <td>В работе рассматриваются некоторые гипотезы о ...</td>\n",
       "      <td>В данной работе изучаются некоторые гипотезы о...</td>\n",
       "      <td>В данной работе рассматриваются некоторые гипо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>66</td>\n",
       "      <td>45132</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В данной работе исследовались MS/MS спектры, с...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Данная работа посвящена разработке алгоритмов ...</td>\n",
       "      <td>Данная работа посвящена поиску и классификации...</td>\n",
       "      <td>Данная работа посвящена исследованию поведения...</td>\n",
       "      <td>В данной работе рассматриваются масс-спектры б...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>67</td>\n",
       "      <td>45133</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В выпускной работе описываются триангуляции ве...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются асимптотики числа три...</td>\n",
       "      <td>Автор производит подсчёт асимптотики числа три...</td>\n",
       "      <td>Рассматриваются выпуклые триангуляции, то есть...</td>\n",
       "      <td>В данной работе изучаются выпуклые триангуляци...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>68</td>\n",
       "      <td>45135</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы было разработано програ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной выпускной квалификационной работе пре...</td>\n",
       "      <td>Работа посвящена разработке программного средс...</td>\n",
       "      <td>В работе рассмотрена проблема автоматической и...</td>\n",
       "      <td>В данной работе реализованы алгоритмы автомати...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1238</th>\n",
       "      <td>69</td>\n",
       "      <td>45137</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В данной работе представлены реализация фронте...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Данная выпускная квалификационная работа посвя...</td>\n",
       "      <td>Работа посвящена разработке архитектуры и созд...</td>\n",
       "      <td>Работа состоит из 4 глав:\\n• Первая глава – вв...</td>\n",
       "      <td>В данной работе рассматривается архитектура и ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Unnamed: 0.1  ...                            learnt_8k_retest_greedy\n",
       "Unnamed: 0                ...                                                   \n",
       "12                     0  ...  В работе рассматривается модельная структура н...\n",
       "25                     1  ...                                                  В\n",
       "37                     2  ...  В данной работе рассматривается решение задачи...\n",
       "101                    3  ...  В работе мы обобщаем результаты об энергии нат...\n",
       "152                    4  ...  В данной работе рассматривается применение доб...\n",
       "...                  ...  ...                                                ...\n",
       "1062                  65  ...  В данной работе рассматриваются некоторые гипо...\n",
       "1064                  66  ...  В данной работе рассматриваются масс-спектры б...\n",
       "1079                  67  ...  В данной работе изучаются выпуклые триангуляци...\n",
       "1209                  68  ...  В данной работе реализованы алгоритмы автомати...\n",
       "1238                  69  ...  В данной работе рассматривается архитектура и ...\n",
       "\n",
       "[70 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = new_df.set_index(\"Unnamed: 0\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32e9e1f1-2fc2-4fc3-870a-e3468a72a3b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:48:20.643940Z",
     "iopub.status.busy": "2024-05-10T00:48:20.643521Z",
     "iopub.status.idle": "2024-05-10T00:48:43.984390Z",
     "shell.execute_reply": "2024-05-10T00:48:43.983690Z",
     "shell.execute_reply.started": "2024-05-10T00:48:20.643919Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d04c029361bf4c95b508cd02c384589a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mera_helpers import construct_prompt\n",
    "from llm_helpers import calculate_token_interest_probs, get_answer\n",
    "\n",
    "\n",
    "with open(ARTIFACTS_DIR_PATH.joinpath(\"datasets/diplomas_asessors_questions/mcs_df_human_filled_processed.json\"), \"r\") as f:\n",
    "    asessors_dataset = json.load(f)\n",
    "\n",
    "a_list = []\n",
    "probs_list = []\n",
    "for row in tqdm(asessors_dataset[:first_k], desc=\"Rows...\"):\n",
    "    x = copy.deepcopy(row)\n",
    "    x['inputs']['context'] = new_df[MODEL_EXPERIMENT_NAME].loc[int(x['meta']['id'])] # df[\"diploma\"].iloc[int(x['meta']['id'])]\n",
    "    q = construct_prompt(x)\n",
    "    probs = calculate_token_interest_probs(q, tokenizer, peft_model)\n",
    "    probs_list.append({\n",
    "        \"probs\": probs,\n",
    "        \"meta\": row[\"meta\"],\n",
    "    })\n",
    "    a = get_answer(probs)\n",
    "    a_list.append({\n",
    "        \"answer\": a,\n",
    "        \"meta\": row[\"meta\"],\n",
    "    })\n",
    "metric_dir_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{MODEL_EXPERIMENT_NAME}/{LLAMA_2_7B}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "with open(metric_dir_path.joinpath(\"probs.jsonl\"), \"w\") as f:\n",
    "    json.dump(probs_list, f, ensure_ascii=False, indent=2)\n",
    "with open(metric_dir_path.joinpath(\"answers.jsonl\"), \"w\") as f:\n",
    "    json.dump(a_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ad797c7-5095-47dc-869e-3707631f77b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:48:43.985747Z",
     "iopub.status.busy": "2024-05-10T00:48:43.985402Z",
     "iopub.status.idle": "2024-05-10T00:48:44.017992Z",
     "shell.execute_reply": "2024-05-10T00:48:44.017361Z",
     "shell.execute_reply.started": "2024-05-10T00:48:43.985723Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>subset</th>\n",
       "      <th>split</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>empty</td>\n",
       "      <td>0.185714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>diploma_8000</td>\n",
       "      <td>0.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>abstract</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>empty</td>\n",
       "      <td>0.228571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>diploma_8000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>learnt</td>\n",
       "      <td>0.228571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>learnt_8k</td>\n",
       "      <td>0.271429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama-2-7b</td>\n",
       "      <td>asessors_questions</td>\n",
       "      <td>learnt_8k_retest_greedy</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model              subset                    split  accuracy_score\n",
       "0  mistral-7b  asessors_questions                 abstract        0.285714\n",
       "1  mistral-7b  asessors_questions                    empty        0.185714\n",
       "2  mistral-7b  asessors_questions             diploma_8000        0.257143\n",
       "3  llama-2-7b  asessors_questions                 abstract        0.285714\n",
       "4  llama-2-7b  asessors_questions                    empty        0.228571\n",
       "5  llama-2-7b  asessors_questions             diploma_8000        0.200000\n",
       "6  llama-2-7b  asessors_questions                   learnt        0.228571\n",
       "7  llama-2-7b  asessors_questions                learnt_8k        0.271429\n",
       "8  llama-2-7b  asessors_questions  learnt_8k_retest_greedy        0.200000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "pred_by_model = dict()\n",
    "for baseline_model in HUGGINFACE_BASELINE_MODELS:\n",
    "    splits = [\"abstract\", \"empty\", \"diploma\"]\n",
    "    if baseline_model == LLAMA_2_7B:\n",
    "        splits.extend([\"learnt\", \"learnt_8k\", MODEL_EXPERIMENT_NAME])\n",
    "    for split in splits:\n",
    "        metric_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{split}/{baseline_model}/\") # get_metric_dir_path(baseline_model, subset, name, split)\n",
    "        with open(metric_path.joinpath(\"answers.jsonl\"), \"r\") as f:\n",
    "            answers = json.load(f)\n",
    "        pred = [x[\"answer\"] for x in answers[:first_k]]\n",
    "        pred_by_model[f\"{split}_{baseline_model}\"] = pred\n",
    "        true = [x[\"outputs\"] for x in asessors_dataset[:first_k]]\n",
    "        rows.append({\n",
    "            \"model\": baseline_model,\n",
    "            \"subset\": \"asessors_questions\",\n",
    "            \"split\": f\"{split}_8000\" if split == \"diploma\" else split,\n",
    "            \"accuracy_score\": accuracy_score(true, pred),\n",
    "        })\n",
    "asessors_df = pd.DataFrame(rows)\n",
    "asessors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9209da0f-75d5-45af-b837-06433293d59d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T00:49:06.319545Z",
     "iopub.status.busy": "2024-05-10T00:49:06.319112Z",
     "iopub.status.idle": "2024-05-10T00:49:06.341068Z",
     "shell.execute_reply": "2024-05-10T00:49:06.340380Z",
     "shell.execute_reply.started": "2024-05-10T00:49:06.319520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "asessors_df.to_csv(METRICS_DIR_PATH.joinpath(f\"asessors_with_{MODEL_EXPERIMENT_NAME}.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff94c80-17ec-4220-a3cc-826719a31832",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Evaluate MERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493a575-dc86-463b-b981-96afcda8a722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for baseline_model in HUGGINFACE_BASELINE_MODELS.union([VIKHR_7B, SAIGA_MISTRAL_7B_LORA]).union([MODEL_EXPERIMENT_NAME]):\n",
    "    for name, dataset_meta in HUGGINGFACE_NAME_TO_DATASET.items():\n",
    "        for subset, split in zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]):\n",
    "            dataset_path = get_dataset_path(subset, name, split)\n",
    "            dataset = load_from_disk(dataset_path)\n",
    "            metric_path = get_metric_dir_path(baseline_model, subset, name, split)\n",
    "            with open(metric_path.joinpath(\"answers.jsonl\"), \"r\") as f:\n",
    "                answers = json.load(f)\n",
    "            pred = [x[\"answer\"] for x in answers[:first_k]]\n",
    "            true = [x[\"outputs\"] for x in list(dataset)[:first_k]]\n",
    "            rows.append({\n",
    "                \"model\": baseline_model,\n",
    "                \"subset\": subset,\n",
    "                \"split\": split,\n",
    "                \"accuracy_score\": accuracy_score(true, pred),\n",
    "            })\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57a165a-a392-4811-b00c-f564391fa6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(METRICS_DIR_PATH.joinpath(f\"ru_metrics_with_{MODEL_EXPERIMENT_NAME}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a79a564-eeb3-436b-8469-e22603874bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
