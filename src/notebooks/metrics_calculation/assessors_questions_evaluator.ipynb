{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"`do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\",\n",
    ")\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../utils\")\n",
    "from definitions import *\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../../../LongLoRA-diploma-research\")\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "login(os.environ['hf-read-token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_EXPERIMENT_NAME = \"lora_v1\"\n",
    "MODEL_MAX_LENGTH = 16384\n",
    "INPUT_COLUMNS = [\"test_row\", \"id\", \"abstract\"] # ['id', 'abstract', 'diploma', 'begin', 'raw_model_v2', 'learnt']\n",
    "CACHE_DIR = Path(\"../../../../cache/\")\n",
    "DATASET_DIR = Path(\"/home/jupyter/mnt/datasets/spbu_diplomas/russian_dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load inputs & assessors dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = pd.read_csv(ARTIFACTS_DIR_PATH.joinpath(\"metrics/diplomas_asessors_questions/inputs.csv\"), usecols=INPUT_COLUMNS)\n",
    "inputs = inputs.set_index(\"test_row\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(ARTIFACTS_DIR_PATH.joinpath(\"datasets/diplomas_asessors_questions/mcs_df_human_filled_processed.json\"), \"r\") as f:\n",
    "    asessors_dataset = json.load(f)\n",
    "asessors_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[LLAMA_2_7B], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[LLAMA_2_7B],\n",
    "    cache_dir=CACHE_DIR,\n",
    "    model_max_length=MODEL_MAX_LENGTH,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "print(\"Loaded tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "from llama_attn_replace_sft import replace_llama_attn\n",
    "from gptneox_attn_replace import replace_gpt_neox_attn\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch.distributed import barrier\n",
    "\n",
    "\n",
    "model_name = LLAMA_2_7B\n",
    "\n",
    "replace_llama_attn(True, False, inference=True)\n",
    "\n",
    "# Set RoPE scaling factor\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name],\n",
    "    cache_dir=CACHE_DIR,\n",
    ")\n",
    "\n",
    "orig_rope_scaling = getattr(config, \"rope_scaling\", None)\n",
    "if orig_rope_scaling is None:\n",
    "    orig_rope_scaling = {\"factor\": 1}\n",
    "orig_rope_scaling_factor = orig_rope_scaling[\"factor\"] if \"factor\" in orig_rope_scaling.keys() else 1\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len:\n",
    "    orig_ctx_len *= orig_rope_scaling_factor\n",
    "    if MODEL_MAX_LENGTH > orig_ctx_len:\n",
    "        scaling_factor = float(math.ceil(MODEL_MAX_LENGTH / orig_ctx_len))\n",
    "        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "\n",
    "print(\"Created config\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(\"Loaded model\")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name],\n",
    "    cache_dir=CACHE_DIR,\n",
    "    model_max_length=MODEL_MAX_LENGTH,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    ")\n",
    "\n",
    "print(\"Loaded tokenizer\")\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "\n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "        \n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=special_tokens_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"nvdenisov2002/llama-longLoRA-v1\"\n",
    "model = PeftModel.from_pretrained(model, model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model on inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mera_helpers import construct_prompt\n",
    "from llm_helpers import calculate_token_interest_probs, get_answer\n",
    "\n",
    "for cur_input in inputs.columns:\n",
    "    if cur_input == \"id\":\n",
    "        continue\n",
    "    metric_dir_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{MODEL_EXPERIMENT_NAME}_{cur_input}_appended/{LLAMA_2_7B}/\")\n",
    "    metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "    for row in tqdm(asessors_dataset, desc=\"Rows...\"):\n",
    "        x = copy.deepcopy(row)\n",
    "        x['inputs']['context'] = inputs[cur_input].loc[int(x['meta']['id'])]\n",
    "        q = construct_prompt(x)\n",
    "        probs = calculate_token_interest_probs(q, tokenizer, model)\n",
    "        probs_entry = {\n",
    "            \"probs\": probs,\n",
    "            \"meta\": row[\"meta\"],\n",
    "        }\n",
    "        a = get_answer(probs)\n",
    "        a_entry = {\n",
    "            \"answer\": a,\n",
    "            \"meta\": row[\"meta\"],\n",
    "        }\n",
    "        with open(metric_dir_path.joinpath(\"probs_appended.jsons\"), \"a\") as f:\n",
    "            json.dump(probs_entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "        with open(metric_dir_path.joinpath(\"answers_appended.jsons\"), \"a\") as f:\n",
    "            json.dump(a_entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for cur_input in inputs.columns:\n",
    "    if cur_input == \"id\":\n",
    "        continue\n",
    "    metric_path = ARTIFACTS_DIR_PATH.joinpath(f\"metrics/diplomas_asessors_questions/{MODEL_EXPERIMENT_NAME}_{cur_input}_appended/{LLAMA_2_7B}/\")\n",
    "    some_path = metric_path.joinpath(\"answers_appended.jsons\")\n",
    "    with open(some_path, \"r\") as f:\n",
    "        answers = [json.loads(x) for x in f.readlines()]\n",
    "    pred = [x[\"answer\"] for x in answers]\n",
    "    true = [x[\"outputs\"] for x in asessors_dataset]\n",
    "    rows.append({\n",
    "        \"model\": MODEL_EXPERIMENT_NAME,\n",
    "        \"subset\": \"asessors_questions\",\n",
    "        \"split\": cur_input,\n",
    "        \"accuracy_score\": accuracy_score(true, pred),\n",
    "    })\n",
    "asessors_df = pd.DataFrame(rows)\n",
    "asessors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T14:36:28.459493Z",
     "iopub.status.busy": "2024-05-19T14:36:28.459075Z",
     "iopub.status.idle": "2024-05-19T14:36:28.476099Z",
     "shell.execute_reply": "2024-05-19T14:36:28.475480Z",
     "shell.execute_reply.started": "2024-05-19T14:36:28.459469Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "asessors_df.to_csv(METRICS_DIR_PATH.joinpath(f\"assessors_{MODEL_EXPERIMENT_NAME}_{'-'.join(INPUT_COLUMNS)}.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
