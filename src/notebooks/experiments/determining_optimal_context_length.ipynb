{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T13:56:19.193612Z",
     "iopub.status.busy": "2024-05-09T13:56:19.193193Z",
     "iopub.status.idle": "2024-05-09T13:56:19.206532Z",
     "shell.execute_reply": "2024-05-09T13:56:19.205953Z",
     "shell.execute_reply.started": "2024-05-09T13:56:19.193588Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\",\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T13:56:19.617143Z",
     "iopub.status.busy": "2024-05-09T13:56:19.616868Z",
     "iopub.status.idle": "2024-05-09T13:56:19.942651Z",
     "shell.execute_reply": "2024-05-09T13:56:19.942007Z",
     "shell.execute_reply.started": "2024-05-09T13:56:19.617124Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b623affb4f4431a8e8c0d1eed21622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T13:56:26.090071Z",
     "iopub.status.busy": "2024-05-09T13:56:26.089364Z",
     "iopub.status.idle": "2024-05-09T13:59:57.263227Z",
     "shell.execute_reply": "2024-05-09T13:59:57.262454Z",
     "shell.execute_reply.started": "2024-05-09T13:56:26.090050Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../utils\")\n",
    "from definitions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T17:30:52.085441Z",
     "iopub.status.busy": "2024-05-08T17:30:52.085160Z",
     "iopub.status.idle": "2024-05-08T17:30:52.099110Z",
     "shell.execute_reply": "2024-05-08T17:30:52.098625Z",
     "shell.execute_reply.started": "2024-05-08T17:30:52.085424Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LongLoRA-diploma-research\n",
      "OpenBookQA\n",
      "YandexGPT-api-call_ru.ipynb\n",
      "cache\n",
      "dataflow_en.ipynb\n",
      "gpt-week\n",
      "long_context_LLMs\n",
      "modelcache\n",
      "nlp_course\n",
      "venv\n",
      "view_machine.ipynb\n",
      "wandb\n",
      "wandb_try.ipynb\n"
     ]
    }
   ],
   "source": [
    "! ls ../../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T13:59:57.264664Z",
     "iopub.status.busy": "2024-05-09T13:59:57.264235Z",
     "iopub.status.idle": "2024-05-09T13:59:57.278055Z",
     "shell.execute_reply": "2024-05-09T13:59:57.277436Z",
     "shell.execute_reply.started": "2024-05-09T13:59:57.264645Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CACHE_DIR = Path(\"../../../../cache/\")\n",
    "DATASET_DIR = Path(\"/home/jupyter/mnt/datasets/diplomas/russian_dataset/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example of Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper imports & definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T13:59:57.279332Z",
     "iopub.status.busy": "2024-05-09T13:59:57.279049Z",
     "iopub.status.idle": "2024-05-09T14:00:12.206760Z",
     "shell.execute_reply": "2024-05-09T14:00:12.205975Z",
     "shell.execute_reply.started": "2024-05-09T13:59:57.279315Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-09 14:00:05.373089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:00:12.208609Z",
     "iopub.status.busy": "2024-05-09T14:00:12.207850Z",
     "iopub.status.idle": "2024-05-09T14:00:12.229877Z",
     "shell.execute_reply": "2024-05-09T14:00:12.229240Z",
     "shell.execute_reply.started": "2024-05-09T14:00:12.208589Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input_llama2\":(\n",
    "        \"[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\"\n",
    "        \"<</SYS>> \\n\\n {instruction} [/INST]\"\n",
    "    ),\n",
    "    \"prompt_input_llama2\": (\n",
    "        \"[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\"\n",
    "        \"<</SYS>> \\n\\n {instruction} \\n{input} [/INST]\"\n",
    "    ),\n",
    "    \"prompt_llama2\": \"[INST]{instruction}[/INST]\",\n",
    "    \"prompt_input_diploma_special\":(\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\nBelow is a diploma text. Your task is to generate abstract of this diploma.\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:00:12.230797Z",
     "iopub.status.busy": "2024-05-09T14:00:12.230528Z",
     "iopub.status.idle": "2024-05-09T14:00:12.244416Z",
     "shell.execute_reply": "2024-05-09T14:00:12.243805Z",
     "shell.execute_reply.started": "2024-05-09T14:00:12.230781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:00:12.245344Z",
     "iopub.status.busy": "2024-05-09T14:00:12.245039Z",
     "iopub.status.idle": "2024-05-09T14:00:12.271731Z",
     "shell.execute_reply": "2024-05-09T14:00:12.271228Z",
     "shell.execute_reply.started": "2024-05-09T14:00:12.245327Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer, nrows: int, diploma_prefix_len: int):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        data_table = pd.read_csv(data_path)\n",
    "        data_table = data_table.sample(min(len(data_table), nrows))\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "\n",
    "        prompt_input_diploma = PROMPT_DICT[\"prompt_input_diploma_special\"]\n",
    "        sources = [\n",
    "            prompt_input_diploma.format(input=diploma[:diploma_prefix_len])\n",
    "            for diploma in data_table[\"diploma\"]\n",
    "        ]\n",
    "\n",
    "        targets = [f\"{abstract}{tokenizer.eos_token}\" for abstract in data_table[\"abstract\"]]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Downloading model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:00:12.272598Z",
     "iopub.status.busy": "2024-05-09T14:00:12.272293Z",
     "iopub.status.idle": "2024-05-09T14:00:12.293634Z",
     "shell.execute_reply": "2024-05-09T14:00:12.293121Z",
     "shell.execute_reply.started": "2024-05-09T14:00:12.272581Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = LLAMA_2_7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:00:12.295536Z",
     "iopub.status.busy": "2024-05-09T14:00:12.295139Z",
     "iopub.status.idle": "2024-05-09T14:03:53.362318Z",
     "shell.execute_reply": "2024-05-09T14:03:53.360322Z",
     "shell.execute_reply.started": "2024-05-09T14:00:12.295513Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [03:39<00:00, 109.78s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(HUGGINGFACE_MODEL_TO_REPO[model_name], cache_dir=CACHE_DIR, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:03:53.364902Z",
     "iopub.status.busy": "2024-05-09T14:03:53.364622Z",
     "iopub.status.idle": "2024-05-09T14:03:53.379209Z",
     "shell.execute_reply": "2024-05-09T14:03:53.378226Z",
     "shell.execute_reply.started": "2024-05-09T14:03:53.364883Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:03:53.380476Z",
     "iopub.status.busy": "2024-05-09T14:03:53.380121Z",
     "iopub.status.idle": "2024-05-09T14:03:53.390782Z",
     "shell.execute_reply": "2024-05-09T14:03:53.389893Z",
     "shell.execute_reply.started": "2024-05-09T14:03:53.380454Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_MAX_LENGTH = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:03:53.392006Z",
     "iopub.status.busy": "2024-05-09T14:03:53.391720Z",
     "iopub.status.idle": "2024-05-09T14:03:54.015970Z",
     "shell.execute_reply": "2024-05-09T14:03:54.014921Z",
     "shell.execute_reply.started": "2024-05-09T14:03:53.391989Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_TO_REPO[model_name], \n",
    "    cache_dir=CACHE_DIR, \n",
    "    model_max_length=MODEL_MAX_LENGTH,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:03:54.017134Z",
     "iopub.status.busy": "2024-05-09T14:03:54.016822Z",
     "iopub.status.idle": "2024-05-09T14:03:54.028777Z",
     "shell.execute_reply": "2024-05-09T14:03:54.027555Z",
     "shell.execute_reply.started": "2024-05-09T14:03:54.017116Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Add new tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:03:54.030312Z",
     "iopub.status.busy": "2024-05-09T14:03:54.029933Z",
     "iopub.status.idle": "2024-05-09T14:03:54.191278Z",
     "shell.execute_reply": "2024-05-09T14:03:54.189282Z",
     "shell.execute_reply.started": "2024-05-09T14:03:54.030295Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "special_tokens_dict = dict()\n",
    "if tokenizer.pad_token is None:\n",
    "    special_tokens_dict[\"pad_token\"] = DEFAULT_PAD_TOKEN\n",
    "if tokenizer.eos_token is None:\n",
    "    special_tokens_dict[\"eos_token\"] = DEFAULT_EOS_TOKEN\n",
    "if tokenizer.bos_token is None:\n",
    "    special_tokens_dict[\"bos_token\"] = DEFAULT_BOS_TOKEN\n",
    "if tokenizer.unk_token is None:\n",
    "    special_tokens_dict[\"unk_token\"] = DEFAULT_UNK_TOKEN\n",
    "\n",
    "smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict=special_tokens_dict,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Load train/val/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:03:54.193060Z",
     "iopub.status.busy": "2024-05-09T14:03:54.192575Z",
     "iopub.status.idle": "2024-05-09T14:06:24.302257Z",
     "shell.execute_reply": "2024-05-09T14:06:24.272757Z",
     "shell.execute_reply.started": "2024-05-09T14:03:54.193037Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SupervisedDataset(DATASET_DIR.joinpath(\"russian_dataset_train.csv\"), tokenizer, nrows=720 * 3, diploma_prefix_len=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:06:24.303258Z",
     "iopub.status.busy": "2024-05-09T14:06:24.302998Z",
     "iopub.status.idle": "2024-05-09T14:06:24.322979Z",
     "shell.execute_reply": "2024-05-09T14:06:24.322401Z",
     "shell.execute_reply.started": "2024-05-09T14:06:24.303240Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2160"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:06:24.323884Z",
     "iopub.status.busy": "2024-05-09T14:06:24.323621Z",
     "iopub.status.idle": "2024-05-09T14:07:08.065534Z",
     "shell.execute_reply": "2024-05-09T14:07:08.064916Z",
     "shell.execute_reply.started": "2024-05-09T14:06:24.323867Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = SupervisedDataset(DATASET_DIR.joinpath(\"russian_dataset_val.csv\"), tokenizer,  nrows=720 * 3, diploma_prefix_len=8000)\n",
    "val_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:08.066649Z",
     "iopub.status.busy": "2024-05-09T14:07:08.066378Z",
     "iopub.status.idle": "2024-05-09T14:07:28.615766Z",
     "shell.execute_reply": "2024-05-09T14:07:28.614732Z",
     "shell.execute_reply.started": "2024-05-09T14:07:08.066631Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SupervisedDataset(DATASET_DIR.joinpath(\"russian_dataset_test.csv\"), tokenizer,  nrows=10, diploma_prefix_len=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Ensure that diploma_prefix_len is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:28.616811Z",
     "iopub.status.busy": "2024-05-09T14:07:28.616564Z",
     "iopub.status.idle": "2024-05-09T14:07:28.630303Z",
     "shell.execute_reply": "2024-05-09T14:07:28.629593Z",
     "shell.execute_reply.started": "2024-05-09T14:07:28.616793Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(test_dataset[9][\"labels\"].tolist())).__len__() > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### View how dataset built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T20:25:58.394843Z",
     "iopub.status.busy": "2024-05-08T20:25:58.394446Z",
     "iopub.status.idle": "2024-05-08T20:25:58.407522Z",
     "shell.execute_reply": "2024-05-08T20:25:58.406358Z",
     "shell.execute_reply.started": "2024-05-08T20:25:58.394822Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Below is a diploma text. Your task is to generate abstract of this diploma.\n",
      "\n",
      "### Input:\n",
      "Санкт-Петербургский государственный университет\n",
      "\n",
      "\n",
      "АВРАМЕНКО Полина Андреевна\n",
      "Выпускная квалификационная работа\n",
      "Веб-туны как часть южнокорейской культуры в XXI веке (на примере романтических историй) \n",
      "Уровень образования: магистратура\n",
      "Направление 58.04.01 «Востоковедение и африканистика»\n",
      "Основная образовательная программа BM.5808 «Культура народов Азии и Африки (с изучением языков Азии и Африки)»\n",
      "\n",
      "\n",
      "Научный руководитель:\n",
      "доцент, Кафедра корееведения, Санкт-Петербургский государственный университет Гурьева Анастасия Александровна\n",
      "\n",
      "Рецензент:\n",
      "приглашенный преподаватель, Кафедра корееведения, Санкт-Петербургская школа социальных наук и востоковедения,\n",
      "доцент, Санкт-Петербургский филиал федерального государственного автономного образовательного учреждения высшего образования «Национальный исследовательский университет «Высшая школа экономики»\n",
      "Георгиева Рус Нели Петрова\n",
      "\n",
      "Санкт-Петербург\n",
      "2023\n",
      "СОДЕРЖАНИЕ\n",
      "Введение\t3\n",
      "Глава 1 Феномен веб-туна: история и характеристики\t10\n",
      "§ 1.1 Появление корейских \n",
      "\n",
      "### Response:Данная выпускная квалификационная работа посвящена одному из основных элементов, формирующих массовый культурный контент Республики Корея - веб-тунам (webtoon) – цифровым комиксам, появившимся в начале XXI века. Целью работы является выявление места веб-тунов в южнокорейской культуре, а также их культурной специфики. Актуальность исследования обусловлена тем, что в наши дни в Южной Корее к веб-тунам наблюдается повышенный интерес общества. В ходе исследования был собран, изучен и систематизирован материал об истории манхва как предшественника веб-тунов. Были рассмотрены этапы развития веб-тунов, причины популярности и основные характеристики. В качестве материала для исследования были выбраны и проанализированы три популярных южнокорейских веб-туна. Посредством анализа была выявлена специфика подачи материала и связь с культурой. Благодаря анализу удалось выявить взаимосвязь веб-тунов с традиционной литературой и ролью текста в корейской культуре и традиционным распределением ролей.</s>\n"
     ]
    }
   ],
   "source": [
    "text_labels = tokenizer.decode(test_dataset[9][\"input_ids\"])\n",
    "print(text_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T20:30:10.488885Z",
     "iopub.status.busy": "2024-05-08T20:30:10.488486Z",
     "iopub.status.idle": "2024-05-08T20:30:10.508045Z",
     "shell.execute_reply": "2024-05-08T20:30:10.507466Z",
     "shell.execute_reply.started": "2024-05-08T20:30:10.488835Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данная выпускная квалификационная работа посвящена одному из основных элементов, формирующих массовый культурный контент Республики Корея - веб-тунам (webtoon) – цифровым комиксам, появившимся в начале XXI века. Целью работы является выявление места веб-тунов в южнокорейской культуре, а также их культурной специфики. Актуальность исследования обусловлена тем, что в наши дни в Южной Корее к веб-тунам наблюдается повышенный интерес общества. В ходе исследования был собран, изучен и систематизирован материал об истории манхва как предшественника веб-тунов. Были рассмотрены этапы развития веб-тунов, причины популярности и основные характеристики. В качестве материала для исследования были выбраны и проанализированы три популярных южнокорейских веб-туна. Посредством анализа была выявлена специфика подачи материала и связь с культурой. Благодаря анализу удалось выявить взаимосвязь веб-тунов с традиционной литературой и ролью текста в корейской культуре и традиционным распределением ролей.</s>\n"
     ]
    }
   ],
   "source": [
    "unignored_tokens = []\n",
    "for token in test_dataset[9][\"labels\"]:\n",
    "    if token != IGNORE_INDEX:\n",
    "        unignored_tokens.append(token)\n",
    "text_labels = tokenizer.decode(unignored_tokens)\n",
    "print(text_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generating with raw model before learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T23:09:42.534524Z",
     "iopub.status.busy": "2024-05-08T23:09:42.534267Z",
     "iopub.status.idle": "2024-05-08T23:09:51.615948Z",
     "shell.execute_reply": "2024-05-08T23:09:51.614780Z",
     "shell.execute_reply.started": "2024-05-08T23:09:42.534508Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 7349, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-2bcec6e21934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprefix_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mIGNORE_INDEX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprefix_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mprefix_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0;31m# 13. run sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             return self.sample(\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2735\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 )\n\u001b[1;32m    924\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    926\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!"
     ]
    }
   ],
   "source": [
    "# inference train sample\n",
    "model.eval()\n",
    "prefix_len = np.sum(np.array(train_dataset[9][\"labels\"]) == IGNORE_INDEX)\n",
    "prefix_tokens = train_dataset[9][\"input_ids\"][:prefix_len]\n",
    "generated = model.generate(prefix_tokens.reshape((1, -1)).to(device))\n",
    "generated_text = tokenizer.decode(generated.to('cpu').flatten())\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T20:33:01.001452Z",
     "iopub.status.busy": "2024-05-08T20:33:01.001061Z",
     "iopub.status.idle": "2024-05-08T20:33:01.050868Z",
     "shell.execute_reply": "2024-05-08T20:33:01.050167Z",
     "shell.execute_reply.started": "2024-05-08T20:33:01.001431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_len = np.sum(np.array(test_dataset[9][\"labels\"]) == IGNORE_INDEX)\n",
    "prefix_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T20:33:56.907270Z",
     "iopub.status.busy": "2024-05-08T20:33:56.906823Z",
     "iopub.status.idle": "2024-05-08T20:34:01.339080Z",
     "shell.execute_reply": "2024-05-08T20:34:01.338418Z",
     "shell.execute_reply.started": "2024-05-08T20:33:56.907249Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Below is a diploma text. Your task is to generate abstract of this diploma.\n",
      "\n",
      "### Input:\n",
      "Санкт-Петербургский государственный университет\n",
      "\n",
      "\n",
      "АВРАМЕНКО Полина Андреевна\n",
      "Выпускная квалификационная работа\n",
      "Веб-туны как часть южнокорейской культуры в XXI веке (на примере романтических историй) \n",
      "Уровень образования: магистратура\n",
      "Направление 58.04.01 «Востоковедение и африканистика»\n",
      "Основная образовательная программа BM.5808 «Культура народов Азии и Африки (с изучением языков Азии и Африки)»\n",
      "\n",
      "\n",
      "Научный руководитель:\n",
      "доцент, Кафедра корееведения, Санкт-Петербургский государственный университет Гурьева Анастасия Александровна\n",
      "\n",
      "Рецензент:\n",
      "приглашенный преподаватель, Кафедра корееведения, Санкт-Петербургская школа социальных наук и востоковедения,\n",
      "доцент, Санкт-Петербургский филиал федерального государственного автономного образовательного учреждения высшего образования «Национальный исследовательский университет «Высшая школа экономики»\n",
      "Георгиева Рус Нели Петрова\n",
      "\n",
      "Санкт-Петербург\n",
      "2023\n",
      "СОДЕРЖАНИЕ\n",
      "Введение\t3\n",
      "Глава 1 Феномен веб-туна: история и характеристики\t10\n",
      "§ 1.1 Появление корейских \n",
      "\n",
      "### Response:\n",
      "\n",
      "<s> Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Below is a diploma text. Your task is to generate abstract of this diploma.\n",
      "\n",
      "### Input:\n",
      "Санкт-Петербургский государственный университет\n",
      "\n",
      "\n",
      "АВРАМЕНКО Полина Андреевна\n",
      "Выпускная квалификационная работа\n",
      "Веб-туны как часть южнокорейской культуры в XXI веке (на примере романтических историй) \n",
      "Уровень образования: магистратура\n",
      "Направление 58.04.01 «Востоковедение и африканистика»\n",
      "Основная образовательная программа BM.5808 «Культура народов Азии и Африки (с изучением языков Азии и Африки)»\n",
      "\n",
      "\n",
      "Научный руководитель:\n",
      "доцент, Кафедра корееведения, Санкт-Петербургский государственный университет Гурьева Анастасия Александровна\n",
      "\n",
      "Рецензент:\n",
      "приглашенный преподаватель, Кафедра корееведения, Санкт-Петербургская школа социальных наук и востоковедения,\n",
      "доцент, Санкт-Петербургский филиал федерального государственного автономного образовательного учреждения высшего образования «Национальный исследовательский университет «Высшая школа экономики»\n",
      "Георгиева Рус Нели Петрова\n",
      "\n",
      "Санкт-Петербург\n",
      "2023\n",
      "СОДЕРЖАНИЕ\n",
      "Введение\t3\n",
      "Глава 1 Феномен веб-туна: история и характеристики\t10\n",
      "§ 1.1 Появление корейских \n",
      "\n",
      "### Response:В работе рассмотрены феномен веб-туна и его место в культуре современной Южной Кореи. Описаны различные формы веб-тунов, их функции, характер и содержание. Анализируются особенности формирования и распространения веб-тунов. В работе также представлены результаты исследования, проведенного автором в рамках дипломной работы.</s>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prefix_tokens = test_dataset[9][\"input_ids\"][:prefix_len]\n",
    "text = tokenizer.decode(prefix_tokens)\n",
    "print(text)\n",
    "print()\n",
    "generated = model.generate(prefix_tokens.reshape((1, -1)).to(device))\n",
    "generated_text = tokenizer.decode(generated.to('cpu').flatten())\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:28.631445Z",
     "iopub.status.busy": "2024-05-09T14:07:28.631006Z",
     "iopub.status.idle": "2024-05-09T14:07:28.639979Z",
     "shell.execute_reply": "2024-05-09T14:07:28.639355Z",
     "shell.execute_reply.started": "2024-05-09T14:07:28.631425Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = \"llama\" # default\n",
    "if model_type == \"gpt-neox\":\n",
    "    # added `dense` to match with llama as the basic LoRA would only target 'query_key_value'\n",
    "    targets = [\"query_key_value\", \"dense\"]\n",
    "else:\n",
    "    targets=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=targets,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:28.641120Z",
     "iopub.status.busy": "2024-05-09T14:07:28.640724Z",
     "iopub.status.idle": "2024-05-09T14:07:28.849168Z",
     "shell.execute_reply": "2024-05-09T14:07:28.848404Z",
     "shell.execute_reply.started": "2024-05-09T14:07:28.641102Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:28.850311Z",
     "iopub.status.busy": "2024-05-09T14:07:28.850033Z",
     "iopub.status.idle": "2024-05-09T14:07:28.859759Z",
     "shell.execute_reply": "2024-05-09T14:07:28.859143Z",
     "shell.execute_reply.started": "2024-05-09T14:07:28.850292Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainable_params = \"embed,norm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:28.860720Z",
     "iopub.status.busy": "2024-05-09T14:07:28.860478Z",
     "iopub.status.idle": "2024-05-09T14:07:28.872417Z",
     "shell.execute_reply": "2024-05-09T14:07:28.871864Z",
     "shell.execute_reply.started": "2024-05-09T14:07:28.860704Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "[p.requires_grad_() for n, p in peft_model.named_parameters() if any([k in n for k in trainable_params.split(\",\")])]\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:28.874815Z",
     "iopub.status.busy": "2024-05-09T14:07:28.874566Z",
     "iopub.status.idle": "2024-05-09T14:07:28.884442Z",
     "shell.execute_reply": "2024-05-09T14:07:28.883910Z",
     "shell.execute_reply.started": "2024-05-09T14:07:28.874798Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_model.config.use_cache = False         # required for gradient checkpointing\n",
    "peft_model.enable_input_require_grads()     # required for gradient checkpointing\n",
    "peft_model.gradient_checkpointing_enable()  # enable gradient checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:28.885482Z",
     "iopub.status.busy": "2024-05-09T14:07:28.885235Z",
     "iopub.status.idle": "2024-05-09T14:07:28.893450Z",
     "shell.execute_reply": "2024-05-09T14:07:28.892903Z",
     "shell.execute_reply.started": "2024-05-09T14:07:28.885466Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"output_dir_8k_big_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:28.894395Z",
     "iopub.status.busy": "2024-05-09T14:07:28.894104Z",
     "iopub.status.idle": "2024-05-09T14:07:28.903886Z",
     "shell.execute_reply": "2024-05-09T14:07:28.903350Z",
     "shell.execute_reply.started": "2024-05-09T14:07:28.894378Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:07:28.904799Z",
     "iopub.status.busy": "2024-05-09T14:07:28.904519Z",
     "iopub.status.idle": "2024-05-09T14:07:28.948506Z",
     "shell.execute_reply": "2024-05-09T14:07:28.947810Z",
     "shell.execute_reply.started": "2024-05-09T14:07:28.904783Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir='output_dir_8k_big_data', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5, max_steps=-1, lr_scheduler_type=<SchedulerType.CONSTANT_WITH_WARMUP: 'constant_with_warmup'>, warmup_ratio=0.0, warmup_steps=20, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='output_dir_8k_big_data/runs/May09_14-07-28_g21-fd038e8e-8af5-485b-a83d-2ec263cebf0e', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=1, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=2, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=True, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=0, past_index=-1, run_name='output_dir_8k_big_data', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed='ds_configs/stage2.json', label_smoothing_factor=0.0, optim=<OptimizerNames.ADAMW_TORCH: 'adamw_torch'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, hub_always_push=False, gradient_checkpointing=False, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, dispatch_batches=None, include_tokens_per_second=False, cache_dir=None, model_max_length=32768, use_flash_attn=True, use_full_attn=False, low_rank_training=True, trainable_params='embed,norm')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from accelerate.utils import DistributedType\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=8192 * 4,\n",
    "        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
    "    )\n",
    "    use_flash_attn: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether use flash attention for training.\"},\n",
    "    )\n",
    "    use_full_attn: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether to use plain, full-attention for training.\"},\n",
    "    )\n",
    "    low_rank_training: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether use low rank adaptation for training.\"},\n",
    "    )\n",
    "    trainable_params: str = field(\n",
    "        default=\"embed,norm\",\n",
    "        metadata={\"help\": \"Additional trainable parameters except LoRA weights, if low rank training.\"},\n",
    "    )\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    bf16=True,\n",
    "    use_flash_attn=True,\n",
    "    low_rank_training=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    evaluation_strategy=\"no\",\n",
    "    # save_strategy=\"steps\",\n",
    "    # save_steps=1,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.0,\n",
    "    warmup_steps=20,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    logging_steps=1,\n",
    "    deepspeed=\"ds_configs/stage2.json\",\n",
    "    tf32=True,\n",
    "    report_to=['tensorboard'],\n",
    ")\n",
    "# training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:10:43.048234Z",
     "iopub.status.busy": "2024-05-09T14:10:43.047813Z",
     "iopub.status.idle": "2024-05-09T14:10:43.175122Z",
     "shell.execute_reply": "2024-05-09T14:10:43.174416Z",
     "shell.execute_reply.started": "2024-05-09T14:10:43.048214Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=peft_model, \n",
    "    tokenizer=tokenizer, \n",
    "    args=training_args, \n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset, \n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T23:54:54.757338Z",
     "iopub.status.busy": "2024-05-08T23:54:54.757057Z",
     "iopub.status.idle": "2024-05-09T02:38:11.532705Z",
     "shell.execute_reply": "2024-05-09T02:38:11.531339Z",
     "shell.execute_reply.started": "2024-05-08T23:54:54.757320Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|          | 0/1350 [00:00<?, ?it/s]\u001b[A\n",
      "                                                  \u001b[A\n",
      "  0%|          | 1/1350 [01:11<8:49:28, 23.55s/it]\n",
      "  0%|          | 1/1350 [00:23<8:49:23, 23.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6495, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  \u001b[A\n",
      "  0%|          | 1/1350 [01:32<8:49:28, 23.55s/it]\n",
      "  0%|          | 2/1350 [00:44<8:20:34, 22.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0973, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  \u001b[A\n",
      "  0%|          | 1/1350 [01:52<8:49:28, 23.55s/it]\n",
      "  0%|          | 3/1350 [01:05<8:00:47, 21.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1142, 'learning_rate': 3e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  \u001b[A\n",
      "  0%|          | 1/1350 [02:13<8:49:28, 23.55s/it]\n",
      "  0%|          | 4/1350 [01:26<7:57:38, 21.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5671, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  \u001b[A\n",
      "  0%|          | 1/1350 [02:32<8:49:28, 23.55s/it]\n",
      "  0%|          | 5/1350 [01:44<7:32:12, 20.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.242, 'learning_rate': 5e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  \u001b[A\n",
      "  0%|          | 1/1350 [02:52<8:49:28, 23.55s/it]\n",
      "  0%|          | 6/1350 [02:04<7:32:43, 20.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0608, 'learning_rate': 6e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  \u001b[A\n",
      "  0%|          | 1/1350 [03:15<8:49:28, 23.55s/it]\n",
      "  1%|          | 7/1350 [02:27<7:51:26, 21.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.986, 'learning_rate': 7e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  \u001b[A\n",
      "  0%|          | 1/1350 [03:33<8:49:28, 23.55s/it]\n",
      "  1%|          | 8/1350 [02:45<7:29:34, 20.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1857, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  \u001b[A\n",
      "  0%|          | 1/1350 [03:53<8:49:28, 23.55s/it]\n",
      "  1%|          | 9/1350 [03:06<7:33:08, 20.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.564, 'learning_rate': 9e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [04:13<8:49:28, 23.55s/it] \n",
      "  1%|          | 10/1350 [03:25<7:25:57, 19.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1665, 'learning_rate': 1e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [04:33<8:49:28, 23.55s/it] \n",
      "  1%|          | 11/1350 [03:45<7:26:53, 20.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0275, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [04:52<8:49:28, 23.55s/it] \n",
      "  1%|          | 12/1350 [04:04<7:17:25, 19.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2889, 'learning_rate': 1.2e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [05:10<8:49:28, 23.55s/it] \n",
      "  1%|          | 13/1350 [04:22<7:08:41, 19.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.96, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [05:29<8:49:28, 23.55s/it] \n",
      "  1%|          | 14/1350 [04:41<7:05:10, 19.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8579, 'learning_rate': 1.4e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [05:48<8:49:28, 23.55s/it] \n",
      "  1%|          | 15/1350 [05:01<7:07:21, 19.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5223, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [06:05<8:49:28, 23.55s/it] \n",
      "  1%|          | 16/1350 [05:18<6:52:54, 18.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0516, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [06:23<8:49:28, 23.55s/it] \n",
      "  1%|▏         | 17/1350 [05:36<6:48:06, 18.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0442, 'learning_rate': 1.7e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [06:42<8:49:28, 23.55s/it] \n",
      "  1%|▏         | 18/1350 [05:55<6:53:44, 18.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9041, 'learning_rate': 1.8e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [07:00<8:49:28, 23.55s/it] \n",
      "  1%|▏         | 19/1350 [06:13<6:46:36, 18.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.051, 'learning_rate': 1.9e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [07:18<8:49:28, 23.55s/it] \n",
      "  1%|▏         | 20/1350 [06:30<6:43:28, 18.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8983, 'learning_rate': 2e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [07:36<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 21/1350 [06:48<6:39:33, 18.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2203, 'learning_rate': 2e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [07:57<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 22/1350 [07:09<6:58:38, 18.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7296, 'learning_rate': 2e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [08:19<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 23/1350 [07:31<7:21:51, 19.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9189, 'learning_rate': 2e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [08:37<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 24/1350 [07:50<7:09:48, 19.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8973, 'learning_rate': 2e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [08:58<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 25/1350 [08:11<7:21:04, 19.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5491, 'learning_rate': 2e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [09:18<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 26/1350 [08:31<7:21:13, 20.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0886, 'learning_rate': 2e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [09:38<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 27/1350 [08:51<7:20:34, 19.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9792, 'learning_rate': 2e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [09:58<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 28/1350 [09:11<7:18:52, 19.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0754, 'learning_rate': 2e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [10:19<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 29/1350 [09:32<7:24:56, 20.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0363, 'learning_rate': 2e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [10:38<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 30/1350 [09:50<7:12:59, 19.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.833, 'learning_rate': 2e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [10:55<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 31/1350 [10:08<7:01:27, 19.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9397, 'learning_rate': 2e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [11:17<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 32/1350 [10:29<7:13:25, 19.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5333, 'learning_rate': 2e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [11:37<8:49:28, 23.55s/it] \n",
      "  2%|▏         | 33/1350 [10:49<7:16:45, 19.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2699, 'learning_rate': 2e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [11:56<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 34/1350 [11:09<7:14:25, 19.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0356, 'learning_rate': 2e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [12:16<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 35/1350 [11:28<7:12:10, 19.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1792, 'learning_rate': 2e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [12:33<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 36/1350 [11:46<6:54:59, 18.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0532, 'learning_rate': 2e-05, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [12:52<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 37/1350 [12:05<6:56:31, 19.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8306, 'learning_rate': 2e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [13:10<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 38/1350 [12:23<6:49:37, 18.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0542, 'learning_rate': 2e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [13:31<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 39/1350 [12:44<7:05:04, 19.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3102, 'learning_rate': 2e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [13:51<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 40/1350 [13:03<7:02:53, 19.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7745, 'learning_rate': 2e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [14:10<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 41/1350 [13:23<7:02:48, 19.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9433, 'learning_rate': 2e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [14:30<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 42/1350 [13:43<7:08:51, 19.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0653, 'learning_rate': 2e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [14:49<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 43/1350 [14:02<7:02:59, 19.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0165, 'learning_rate': 2e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [15:08<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 44/1350 [14:21<7:01:09, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9938, 'learning_rate': 2e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [15:26<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 45/1350 [14:38<6:46:11, 18.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9777, 'learning_rate': 2e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [15:43<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 46/1350 [14:56<6:41:08, 18.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8733, 'learning_rate': 2e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [16:04<8:49:28, 23.55s/it] \n",
      "  3%|▎         | 47/1350 [15:16<6:53:01, 19.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7873, 'learning_rate': 2e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [16:22<8:49:28, 23.55s/it] \n",
      "  4%|▎         | 48/1350 [15:35<6:50:10, 18.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7786, 'learning_rate': 2e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [16:42<8:49:28, 23.55s/it] \n",
      "  4%|▎         | 49/1350 [15:55<6:55:08, 19.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.071, 'learning_rate': 2e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [17:02<8:49:28, 23.55s/it] \n",
      "  4%|▎         | 50/1350 [16:15<7:01:25, 19.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0156, 'learning_rate': 2e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [17:22<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 51/1350 [16:34<7:00:05, 19.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9436, 'learning_rate': 2e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [17:42<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 52/1350 [16:55<7:08:39, 19.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4652, 'learning_rate': 2e-05, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [18:01<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 53/1350 [17:14<7:01:06, 19.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.741, 'learning_rate': 2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [18:25<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 54/1350 [17:38<7:29:58, 20.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5086, 'learning_rate': 2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [18:45<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 55/1350 [17:58<7:26:04, 20.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1596, 'learning_rate': 2e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [19:06<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 56/1350 [18:18<7:24:28, 20.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8642, 'learning_rate': 2e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [19:26<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 57/1350 [18:39<7:21:58, 20.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8354, 'learning_rate': 2e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [19:48<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 58/1350 [19:00<7:27:50, 20.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3893, 'learning_rate': 2e-05, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [20:06<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 59/1350 [19:18<7:11:28, 20.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7298, 'learning_rate': 2e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [20:23<8:49:28, 23.55s/it] \n",
      "  4%|▍         | 60/1350 [19:36<6:53:18, 19.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0632, 'learning_rate': 2e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [20:41<8:49:28, 23.55s/it] \n",
      "  5%|▍         | 61/1350 [19:53<6:40:52, 18.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9528, 'learning_rate': 2e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [21:02<8:49:28, 23.55s/it] \n",
      "  5%|▍         | 62/1350 [20:14<6:56:34, 19.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2759, 'learning_rate': 2e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [21:20<8:49:28, 23.55s/it] \n",
      "  5%|▍         | 63/1350 [20:33<6:49:34, 19.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0606, 'learning_rate': 2e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [21:40<8:49:28, 23.55s/it] \n",
      "  5%|▍         | 64/1350 [20:52<6:52:16, 19.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.067, 'learning_rate': 2e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [21:59<8:49:28, 23.55s/it] \n",
      "  5%|▍         | 65/1350 [21:11<6:50:59, 19.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1082, 'learning_rate': 2e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [22:18<8:49:28, 23.55s/it] \n",
      "  5%|▍         | 66/1350 [21:30<6:49:16, 19.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.098, 'learning_rate': 2e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [22:37<8:49:28, 23.55s/it] \n",
      "  5%|▍         | 67/1350 [21:49<6:49:22, 19.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1464, 'learning_rate': 2e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [22:56<8:49:28, 23.55s/it] \n",
      "  5%|▌         | 68/1350 [22:09<6:50:08, 19.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9531, 'learning_rate': 2e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [23:14<8:49:28, 23.55s/it] \n",
      "  5%|▌         | 69/1350 [22:27<6:42:11, 18.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0147, 'learning_rate': 2e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [23:32<8:49:28, 23.55s/it] \n",
      "  5%|▌         | 70/1350 [22:44<6:32:53, 18.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0095, 'learning_rate': 2e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [23:53<8:49:28, 23.55s/it] \n",
      "  5%|▌         | 71/1350 [23:06<6:52:55, 19.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1511, 'learning_rate': 2e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [24:14<8:49:28, 23.55s/it] \n",
      "  5%|▌         | 72/1350 [23:26<7:00:00, 19.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.29, 'learning_rate': 2e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [24:42<8:49:28, 23.55s/it] \n",
      "  5%|▌         | 73/1350 [23:54<7:51:43, 22.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3011, 'learning_rate': 2e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [24:59<8:49:28, 23.55s/it] \n",
      "  5%|▌         | 74/1350 [24:12<7:21:19, 20.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9373, 'learning_rate': 2e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [25:19<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 75/1350 [24:31<7:14:28, 20.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1206, 'learning_rate': 2e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [25:36<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 76/1350 [24:49<6:54:34, 19.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1128, 'learning_rate': 2e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [25:55<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 77/1350 [25:08<6:52:51, 19.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1069, 'learning_rate': 2e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [26:17<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 78/1350 [25:30<7:06:28, 20.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0736, 'learning_rate': 2e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [26:37<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 79/1350 [25:49<7:02:32, 19.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9069, 'learning_rate': 2e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [26:55<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 80/1350 [26:07<6:49:27, 19.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8666, 'learning_rate': 2e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [27:14<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 81/1350 [26:27<6:49:40, 19.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7585, 'learning_rate': 2e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [27:33<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 82/1350 [26:45<6:45:15, 19.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7758, 'learning_rate': 2e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [27:53<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 83/1350 [27:06<6:53:11, 19.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9695, 'learning_rate': 2e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [28:11<8:49:28, 23.55s/it] \n",
      "  6%|▌         | 84/1350 [27:24<6:43:17, 19.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.044, 'learning_rate': 2e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [28:30<8:49:28, 23.55s/it] \n",
      "  6%|▋         | 85/1350 [27:43<6:41:48, 19.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0333, 'learning_rate': 2e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [28:51<8:49:28, 23.55s/it] \n",
      "  6%|▋         | 86/1350 [28:03<6:50:17, 19.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0432, 'learning_rate': 2e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [29:11<8:49:28, 23.55s/it] \n",
      "  6%|▋         | 87/1350 [28:23<6:52:28, 19.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7642, 'learning_rate': 2e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [29:33<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 88/1350 [28:45<7:07:48, 20.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0001, 'learning_rate': 2e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [29:50<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 89/1350 [29:03<6:49:19, 19.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0698, 'learning_rate': 2e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [30:09<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 90/1350 [29:22<6:46:25, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1594, 'learning_rate': 2e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [30:29<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 91/1350 [29:42<6:52:15, 19.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.11, 'learning_rate': 2e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [30:49<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 92/1350 [30:02<6:53:36, 19.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7767, 'learning_rate': 2e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [31:08<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 93/1350 [30:20<6:44:10, 19.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7941, 'learning_rate': 2e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [31:28<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 94/1350 [30:41<6:51:34, 19.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5869, 'learning_rate': 2e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [31:48<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 95/1350 [31:00<6:49:38, 19.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6892, 'learning_rate': 2e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [32:06<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 96/1350 [31:18<6:40:10, 19.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0942, 'learning_rate': 2e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [32:25<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 97/1350 [31:38<6:43:04, 19.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9785, 'learning_rate': 2e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [32:46<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 98/1350 [31:58<6:48:47, 19.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7582, 'learning_rate': 2e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  ]\u001b[A\n",
      "  0%|          | 1/1350 [33:06<8:49:28, 23.55s/it] \n",
      "  7%|▋         | 99/1350 [32:18<6:52:20, 19.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9182, 'learning_rate': 2e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [33:23<8:49:28, 23.55s/it]  \n",
      "  7%|▋         | 100/1350 [32:36<6:37:38, 19.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9132, 'learning_rate': 2e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [33:41<8:49:28, 23.55s/it]  \n",
      "  7%|▋         | 101/1350 [32:54<6:30:04, 18.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1594, 'learning_rate': 2e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [33:58<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 102/1350 [33:11<6:19:18, 18.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9674, 'learning_rate': 2e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [34:18<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 103/1350 [33:31<6:29:49, 18.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.155, 'learning_rate': 2e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [34:41<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 104/1350 [33:53<6:52:31, 19.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0151, 'learning_rate': 2e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [35:00<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 105/1350 [34:13<6:50:47, 19.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0118, 'learning_rate': 2e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [35:24<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 106/1350 [34:37<7:14:49, 20.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3116, 'learning_rate': 2e-05, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [35:41<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 107/1350 [34:53<6:47:07, 19.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1338, 'learning_rate': 2e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [35:58<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 108/1350 [35:10<6:30:41, 18.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1176, 'learning_rate': 2e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [36:16<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 109/1350 [35:28<6:25:33, 18.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0553, 'learning_rate': 2e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [36:36<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 110/1350 [35:48<6:32:03, 18.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8129, 'learning_rate': 2e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [36:53<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 111/1350 [36:05<6:21:51, 18.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9779, 'learning_rate': 2e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [37:14<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 112/1350 [36:26<6:34:51, 19.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3079, 'learning_rate': 2e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [37:32<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 113/1350 [36:45<6:30:46, 18.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0467, 'learning_rate': 2e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [37:52<8:49:28, 23.55s/it]  \n",
      "  8%|▊         | 114/1350 [37:05<6:36:40, 19.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9661, 'learning_rate': 2e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [38:11<8:49:28, 23.55s/it]  \n",
      "  9%|▊         | 115/1350 [37:24<6:34:51, 19.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9769, 'learning_rate': 2e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [38:31<8:49:28, 23.55s/it]  \n",
      "  9%|▊         | 116/1350 [37:44<6:40:44, 19.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.681, 'learning_rate': 2e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [38:51<8:49:28, 23.55s/it]  \n",
      "  9%|▊         | 117/1350 [38:03<6:40:06, 19.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0187, 'learning_rate': 2e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [39:09<8:49:28, 23.55s/it]  \n",
      "  9%|▊         | 118/1350 [38:22<6:33:11, 19.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0789, 'learning_rate': 2e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [39:28<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 119/1350 [38:41<6:33:08, 19.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9411, 'learning_rate': 2e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [39:48<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 120/1350 [39:00<6:35:08, 19.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.125, 'learning_rate': 2e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [40:06<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 121/1350 [39:18<6:26:40, 18.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9242, 'learning_rate': 2e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [40:26<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 122/1350 [39:39<6:37:06, 19.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1904, 'learning_rate': 2e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [40:46<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 123/1350 [39:58<6:37:37, 19.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0103, 'learning_rate': 2e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [41:05<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 124/1350 [40:17<6:32:25, 19.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8261, 'learning_rate': 2e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [41:24<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 125/1350 [40:37<6:35:05, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9392, 'learning_rate': 2e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [41:44<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 126/1350 [40:56<6:35:43, 19.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.074, 'learning_rate': 2e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [42:01<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 127/1350 [41:14<6:22:33, 18.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9428, 'learning_rate': 2e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [42:21<8:49:28, 23.55s/it]  \n",
      "  9%|▉         | 128/1350 [41:33<6:28:15, 19.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9557, 'learning_rate': 2e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [42:39<8:49:28, 23.55s/it]  \n",
      " 10%|▉         | 129/1350 [41:51<6:20:39, 18.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0452, 'learning_rate': 2e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [43:00<8:49:28, 23.55s/it]  \n",
      " 10%|▉         | 130/1350 [42:13<6:37:26, 19.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1761, 'learning_rate': 2e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [43:27<8:49:28, 23.55s/it]  \n",
      " 10%|▉         | 131/1350 [42:39<7:19:01, 21.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8256, 'learning_rate': 2e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [43:47<8:49:28, 23.55s/it]  \n",
      " 10%|▉         | 132/1350 [42:59<7:10:14, 21.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9861, 'learning_rate': 2e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [44:08<8:49:28, 23.55s/it]  \n",
      " 10%|▉         | 133/1350 [43:21<7:11:02, 21.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9597, 'learning_rate': 2e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [44:27<8:49:28, 23.55s/it]  \n",
      " 10%|▉         | 134/1350 [43:39<6:54:46, 20.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8508, 'learning_rate': 2e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [44:46<8:49:28, 23.55s/it]  \n",
      " 10%|█         | 135/1350 [43:58<6:43:07, 19.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0814, 'learning_rate': 2e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [45:04<8:49:28, 23.55s/it]  \n",
      " 10%|█         | 136/1350 [44:17<6:36:08, 19.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1386, 'learning_rate': 2e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [45:26<8:49:28, 23.55s/it]  \n",
      " 10%|█         | 137/1350 [44:38<6:46:35, 20.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9824, 'learning_rate': 2e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [45:44<8:49:28, 23.55s/it]  \n",
      " 10%|█         | 138/1350 [44:56<6:34:54, 19.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1065, 'learning_rate': 2e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [46:05<8:49:28, 23.55s/it]  \n",
      " 10%|█         | 139/1350 [45:18<6:45:43, 20.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0455, 'learning_rate': 2e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [46:28<8:49:28, 23.55s/it]  \n",
      " 10%|█         | 140/1350 [45:41<7:01:59, 20.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1284, 'learning_rate': 2e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [46:47<8:49:28, 23.55s/it]  \n",
      " 10%|█         | 141/1350 [46:00<6:51:59, 20.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.949, 'learning_rate': 2e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [47:10<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 142/1350 [46:22<7:03:38, 21.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9569, 'learning_rate': 2e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [47:27<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 143/1350 [46:39<6:36:37, 19.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9156, 'learning_rate': 2e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [47:42<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 144/1350 [46:55<6:13:24, 18.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9817, 'learning_rate': 2e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [48:03<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 145/1350 [47:15<6:22:22, 19.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8395, 'learning_rate': 2e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [48:21<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 146/1350 [47:33<6:17:42, 18.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8359, 'learning_rate': 2e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [48:41<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 147/1350 [47:53<6:23:29, 19.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6674, 'learning_rate': 2e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [49:02<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 148/1350 [48:15<6:38:40, 19.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8276, 'learning_rate': 2e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [49:24<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 149/1350 [48:36<6:46:03, 20.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0184, 'learning_rate': 2e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [49:46<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 150/1350 [48:58<6:57:19, 20.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0829, 'learning_rate': 2e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [50:04<8:49:28, 23.55s/it]  \n",
      " 11%|█         | 151/1350 [49:17<6:41:15, 20.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9885, 'learning_rate': 2e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [50:24<8:49:28, 23.55s/it]  \n",
      " 11%|█▏        | 152/1350 [49:37<6:40:22, 20.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9973, 'learning_rate': 2e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [50:43<8:49:28, 23.55s/it]  \n",
      " 11%|█▏        | 153/1350 [49:55<6:31:40, 19.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8484, 'learning_rate': 2e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [51:01<8:49:28, 23.55s/it]  \n",
      " 11%|█▏        | 154/1350 [50:13<6:20:26, 19.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.674, 'learning_rate': 2e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [51:18<8:49:28, 23.55s/it]  \n",
      " 11%|█▏        | 155/1350 [50:31<6:11:24, 18.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0884, 'learning_rate': 2e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [51:38<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 156/1350 [50:51<6:19:50, 19.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7661, 'learning_rate': 2e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [52:00<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 157/1350 [51:13<6:38:16, 20.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0236, 'learning_rate': 2e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [52:20<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 158/1350 [51:33<6:34:58, 19.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.096, 'learning_rate': 2e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [52:37<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 159/1350 [51:50<6:18:53, 19.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7776, 'learning_rate': 2e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [52:59<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 160/1350 [52:12<6:36:05, 19.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2198, 'learning_rate': 2e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [53:17<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 161/1350 [52:29<6:21:49, 19.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9028, 'learning_rate': 2e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [53:35<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 162/1350 [52:47<6:12:48, 18.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0687, 'learning_rate': 2e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [53:53<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 163/1350 [53:05<6:08:32, 18.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0215, 'learning_rate': 2e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [54:11<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 164/1350 [53:24<6:07:25, 18.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8577, 'learning_rate': 2e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [54:29<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 165/1350 [53:42<6:02:35, 18.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.799, 'learning_rate': 2e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [54:51<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 166/1350 [54:03<6:21:11, 19.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.054, 'learning_rate': 2e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [55:12<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 167/1350 [54:25<6:35:04, 20.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0163, 'learning_rate': 2e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [55:33<8:49:28, 23.55s/it]  \n",
      " 12%|█▏        | 168/1350 [54:46<6:39:21, 20.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.885, 'learning_rate': 2e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [55:56<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 169/1350 [55:08<6:50:34, 20.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8226, 'learning_rate': 2e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [56:15<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 170/1350 [55:27<6:40:54, 20.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0212, 'learning_rate': 2e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [56:32<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 171/1350 [55:45<6:23:41, 19.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.999, 'learning_rate': 2e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [56:52<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 172/1350 [56:04<6:21:37, 19.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7681, 'learning_rate': 2e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [57:11<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 173/1350 [56:24<6:22:56, 19.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8537, 'learning_rate': 2e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [57:31<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 174/1350 [56:43<6:21:14, 19.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9715, 'learning_rate': 2e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [57:48<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 175/1350 [57:01<6:11:04, 18.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1372, 'learning_rate': 2e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [58:05<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 176/1350 [57:17<5:57:00, 18.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9863, 'learning_rate': 2e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [58:22<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 177/1350 [57:35<5:50:54, 17.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.965, 'learning_rate': 2e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [58:42<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 178/1350 [57:55<6:04:20, 18.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9927, 'learning_rate': 2e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [59:03<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 179/1350 [58:15<6:13:17, 19.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5951, 'learning_rate': 2e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [59:24<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 180/1350 [58:37<6:26:36, 19.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9889, 'learning_rate': 2e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [59:44<8:49:28, 23.55s/it]  \n",
      " 13%|█▎        | 181/1350 [58:56<6:24:58, 19.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8075, 'learning_rate': 2e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                  t]\u001b[A\n",
      "  0%|          | 1/1350 [1:00:04<8:49:28, 23.55s/it]\n",
      " 13%|█▎        | 182/1350 [59:17<6:27:44, 19.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2269, 'learning_rate': 2e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    \u001b[A\n",
      "  0%|          | 1/1350 [1:00:24<8:49:28, 23.55s/it]\n",
      " 14%|█▎        | 183/1350 [59:37<6:29:54, 20.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8124, 'learning_rate': 2e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    \u001b[A\n",
      "  0%|          | 1/1350 [1:00:45<8:49:28, 23.55s/it]\n",
      " 14%|█▎        | 184/1350 [59:58<6:32:39, 20.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0636, 'learning_rate': 2e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:01:03<8:49:28, 23.55s/it]  \n",
      " 14%|█▎        | 185/1350 [1:00:15<6:18:42, 19.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8362, 'learning_rate': 2e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:01:20<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 186/1350 [1:00:33<6:07:13, 18.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8828, 'learning_rate': 2e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:01:40<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 187/1350 [1:00:53<6:10:44, 19.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8706, 'learning_rate': 2e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:01:59<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 188/1350 [1:01:11<6:07:28, 18.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8415, 'learning_rate': 2e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:02:17<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 189/1350 [1:01:29<6:01:01, 18.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9263, 'learning_rate': 2e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:02:33<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 190/1350 [1:01:46<5:50:25, 18.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.125, 'learning_rate': 2e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:02:51<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 191/1350 [1:02:04<5:47:40, 18.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7335, 'learning_rate': 2e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:03:15<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 192/1350 [1:02:27<6:20:18, 19.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9849, 'learning_rate': 2e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:03:33<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 193/1350 [1:02:46<6:11:11, 19.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0536, 'learning_rate': 2e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:04:00<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 194/1350 [1:03:13<6:56:41, 21.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.699, 'learning_rate': 2e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:04:20<8:49:28, 23.55s/it]  \n",
      " 14%|█▍        | 195/1350 [1:03:33<6:47:06, 21.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7838, 'learning_rate': 2e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:04:40<8:49:28, 23.55s/it]  \n",
      " 15%|█▍        | 196/1350 [1:03:52<6:37:55, 20.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8451, 'learning_rate': 2e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:05:00<8:49:28, 23.55s/it]  \n",
      " 15%|█▍        | 197/1350 [1:04:13<6:35:52, 20.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1626, 'learning_rate': 2e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:05:19<8:49:28, 23.55s/it]  \n",
      " 15%|█▍        | 198/1350 [1:04:31<6:22:45, 19.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6274, 'learning_rate': 2e-05, 'epoch': 0.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:05:37<8:49:28, 23.55s/it]  \n",
      " 15%|█▍        | 199/1350 [1:04:50<6:14:01, 19.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8469, 'learning_rate': 2e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:05:54<8:49:28, 23.55s/it]  \n",
      " 15%|█▍        | 200/1350 [1:05:07<5:59:43, 18.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2816, 'learning_rate': 2e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:06:16<8:49:28, 23.55s/it]  \n",
      " 15%|█▍        | 201/1350 [1:05:28<6:15:35, 19.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0247, 'learning_rate': 2e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:06:33<8:49:28, 23.55s/it]  \n",
      " 15%|█▍        | 202/1350 [1:05:45<5:59:55, 18.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8989, 'learning_rate': 2e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:06:51<8:49:28, 23.55s/it]  \n",
      " 15%|█▌        | 203/1350 [1:06:04<5:58:00, 18.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9511, 'learning_rate': 2e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:07:11<8:49:28, 23.55s/it]  \n",
      " 15%|█▌        | 204/1350 [1:06:24<6:06:17, 19.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1216, 'learning_rate': 2e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:07:31<8:49:28, 23.55s/it]  \n",
      " 15%|█▌        | 205/1350 [1:06:43<6:06:36, 19.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9972, 'learning_rate': 2e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:07:52<8:49:28, 23.55s/it]  \n",
      " 15%|█▌        | 206/1350 [1:07:04<6:16:25, 19.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8327, 'learning_rate': 2e-05, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:08:14<8:49:28, 23.55s/it]  \n",
      " 15%|█▌        | 207/1350 [1:07:26<6:29:46, 20.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9975, 'learning_rate': 2e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:08:33<8:49:28, 23.55s/it]  \n",
      " 15%|█▌        | 208/1350 [1:07:46<6:23:55, 20.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9656, 'learning_rate': 2e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:08:54<8:49:28, 23.55s/it]  \n",
      " 15%|█▌        | 209/1350 [1:08:06<6:25:42, 20.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9615, 'learning_rate': 2e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:09:14<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 210/1350 [1:08:26<6:21:23, 20.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0888, 'learning_rate': 2e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:09:34<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 211/1350 [1:08:46<6:21:03, 20.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6961, 'learning_rate': 2e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:09:52<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 212/1350 [1:09:04<6:08:37, 19.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0419, 'learning_rate': 2e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:10:09<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 213/1350 [1:09:22<5:58:07, 18.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.115, 'learning_rate': 2e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:10:28<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 214/1350 [1:09:41<5:57:57, 18.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9965, 'learning_rate': 2e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:10:48<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 215/1350 [1:10:00<6:03:04, 19.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8239, 'learning_rate': 2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:11:07<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 216/1350 [1:10:19<6:00:11, 19.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9977, 'learning_rate': 2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:11:31<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 217/1350 [1:10:43<6:27:41, 20.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2513, 'learning_rate': 2e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:11:50<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 218/1350 [1:11:02<6:20:13, 20.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1562, 'learning_rate': 2e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:12:11<8:49:28, 23.55s/it]  \n",
      " 16%|█▌        | 219/1350 [1:11:23<6:24:57, 20.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9157, 'learning_rate': 2e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:12:31<8:49:28, 23.55s/it]  \n",
      " 16%|█▋        | 220/1350 [1:11:44<6:23:11, 20.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7446, 'learning_rate': 2e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:12:51<8:49:28, 23.55s/it]  \n",
      " 16%|█▋        | 221/1350 [1:12:03<6:17:55, 20.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8569, 'learning_rate': 2e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:13:09<8:49:28, 23.55s/it]  \n",
      " 16%|█▋        | 222/1350 [1:12:22<6:09:12, 19.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1378, 'learning_rate': 2e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:13:29<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 223/1350 [1:12:42<6:10:17, 19.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0561, 'learning_rate': 2e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:13:48<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 224/1350 [1:13:00<6:04:22, 19.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8081, 'learning_rate': 2e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:14:05<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 225/1350 [1:13:18<5:54:06, 18.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8238, 'learning_rate': 2e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:14:26<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 226/1350 [1:13:38<6:02:29, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0701, 'learning_rate': 2e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:14:46<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 227/1350 [1:13:59<6:06:37, 19.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.967, 'learning_rate': 2e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:15:09<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 228/1350 [1:14:22<6:25:40, 20.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9843, 'learning_rate': 2e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:15:30<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 229/1350 [1:14:43<6:28:16, 20.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0846, 'learning_rate': 2e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:15:48<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 230/1350 [1:15:00<6:09:42, 19.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9353, 'learning_rate': 2e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:16:05<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 231/1350 [1:15:18<5:55:59, 19.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1145, 'learning_rate': 2e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:16:27<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 232/1350 [1:15:39<6:10:22, 19.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2259, 'learning_rate': 2e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:16:45<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 233/1350 [1:15:57<5:58:28, 19.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0319, 'learning_rate': 2e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:17:05<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 234/1350 [1:16:17<6:03:28, 19.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6845, 'learning_rate': 2e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:17:24<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 235/1350 [1:16:37<6:02:31, 19.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9108, 'learning_rate': 2e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:17:45<8:49:28, 23.55s/it]  \n",
      " 17%|█▋        | 236/1350 [1:16:57<6:08:19, 19.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8882, 'learning_rate': 2e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:18:07<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 237/1350 [1:17:20<6:20:51, 20.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1119, 'learning_rate': 2e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:18:31<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 238/1350 [1:17:44<6:39:36, 21.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0153, 'learning_rate': 2e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:18:51<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 239/1350 [1:18:04<6:30:48, 21.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9323, 'learning_rate': 2e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:19:15<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 240/1350 [1:18:28<6:46:08, 21.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1381, 'learning_rate': 2e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:19:32<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 241/1350 [1:18:45<6:20:15, 20.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8053, 'learning_rate': 2e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:19:53<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 242/1350 [1:19:06<6:21:36, 20.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1677, 'learning_rate': 2e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:20:11<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 243/1350 [1:19:24<6:05:56, 19.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1645, 'learning_rate': 2e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:20:29<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 244/1350 [1:19:41<5:53:59, 19.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0214, 'learning_rate': 2e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:20:48<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 245/1350 [1:20:00<5:53:00, 19.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8579, 'learning_rate': 2e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:21:05<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 246/1350 [1:20:18<5:41:16, 18.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8706, 'learning_rate': 2e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:21:22<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 247/1350 [1:20:35<5:32:32, 18.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.77, 'learning_rate': 2e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:21:42<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 248/1350 [1:20:55<5:42:18, 18.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.093, 'learning_rate': 2e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:22:00<8:49:28, 23.55s/it]  \n",
      " 18%|█▊        | 249/1350 [1:21:12<5:37:17, 18.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.826, 'learning_rate': 2e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:22:21<8:49:28, 23.55s/it]  \n",
      " 19%|█▊        | 250/1350 [1:21:34<5:52:50, 19.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0865, 'learning_rate': 2e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:22:41<8:49:28, 23.55s/it]  \n",
      " 19%|█▊        | 251/1350 [1:21:54<5:57:47, 19.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8563, 'learning_rate': 2e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:23:02<8:49:28, 23.55s/it]  \n",
      " 19%|█▊        | 252/1350 [1:22:15<6:04:45, 19.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9891, 'learning_rate': 2e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:23:29<8:49:28, 23.55s/it]  \n",
      " 19%|█▊        | 253/1350 [1:22:42<6:44:57, 22.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2768, 'learning_rate': 2e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:23:48<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 254/1350 [1:23:00<6:22:55, 20.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.04, 'learning_rate': 2e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:24:06<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 255/1350 [1:23:19<6:08:35, 20.20s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7674, 'learning_rate': 2e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:24:26<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 256/1350 [1:23:39<6:07:10, 20.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8145, 'learning_rate': 2e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:24:46<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 257/1350 [1:23:58<6:04:25, 20.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7823, 'learning_rate': 2e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:25:04<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 258/1350 [1:24:16<5:52:13, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8872, 'learning_rate': 2e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:25:21<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 259/1350 [1:24:34<5:41:49, 18.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.804, 'learning_rate': 2e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:25:41<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 260/1350 [1:24:54<5:49:07, 19.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8013, 'learning_rate': 2e-05, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:26:01<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 261/1350 [1:25:13<5:49:36, 19.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9494, 'learning_rate': 2e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:26:19<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 262/1350 [1:25:32<5:46:48, 19.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9956, 'learning_rate': 2e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:26:38<8:49:28, 23.55s/it]  \n",
      " 19%|█▉        | 263/1350 [1:25:51<5:44:42, 19.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0707, 'learning_rate': 2e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:26:59<8:49:28, 23.55s/it]  \n",
      " 20%|█▉        | 264/1350 [1:26:11<5:53:05, 19.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3308, 'learning_rate': 2e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:27:20<8:49:28, 23.55s/it]  \n",
      " 20%|█▉        | 265/1350 [1:26:32<5:59:25, 19.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1884, 'learning_rate': 2e-05, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:27:38<8:49:28, 23.55s/it]  \n",
      " 20%|█▉        | 266/1350 [1:26:51<5:51:51, 19.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9459, 'learning_rate': 2e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:27:58<8:49:28, 23.55s/it]  \n",
      " 20%|█▉        | 267/1350 [1:27:10<5:51:09, 19.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9376, 'learning_rate': 2e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:28:15<8:49:28, 23.55s/it]  \n",
      " 20%|█▉        | 268/1350 [1:27:27<5:38:08, 18.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7441, 'learning_rate': 2e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:28:36<8:49:28, 23.55s/it]  \n",
      " 20%|█▉        | 269/1350 [1:27:49<5:52:23, 19.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3308, 'learning_rate': 2e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:28:54<8:49:28, 23.55s/it]  \n",
      " 20%|██        | 270/1350 [1:28:07<5:44:03, 19.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7245, 'learning_rate': 2e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:29:15<8:49:28, 23.55s/it]  \n",
      " 20%|██        | 271/1350 [1:28:28<5:54:02, 19.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0476, 'learning_rate': 2e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:29:33<8:49:28, 23.55s/it]  \n",
      " 20%|██        | 272/1350 [1:28:45<5:42:10, 19.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7897, 'learning_rate': 2e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:29:53<8:49:28, 23.55s/it]  \n",
      " 20%|██        | 273/1350 [1:29:06<5:48:51, 19.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8912, 'learning_rate': 2e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:30:13<8:49:28, 23.55s/it]  \n",
      " 20%|██        | 274/1350 [1:29:25<5:49:13, 19.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0383, 'learning_rate': 2e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:30:31<8:49:28, 23.55s/it]  \n",
      " 20%|██        | 275/1350 [1:29:44<5:42:43, 19.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9816, 'learning_rate': 2e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:30:51<8:49:28, 23.55s/it]  \n",
      " 20%|██        | 276/1350 [1:30:04<5:49:09, 19.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8742, 'learning_rate': 2e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:31:10<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 277/1350 [1:30:23<5:44:07, 19.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8141, 'learning_rate': 2e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:31:28<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 278/1350 [1:30:41<5:38:59, 18.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.931, 'learning_rate': 2e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:31:48<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 279/1350 [1:31:00<5:40:00, 19.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9368, 'learning_rate': 2e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:32:10<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 280/1350 [1:31:22<5:55:30, 19.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0989, 'learning_rate': 2e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:32:29<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 281/1350 [1:31:41<5:49:49, 19.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9939, 'learning_rate': 2e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:32:50<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 282/1350 [1:32:03<6:00:50, 20.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2596, 'learning_rate': 2e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:33:08<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 283/1350 [1:32:21<5:47:22, 19.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8136, 'learning_rate': 2e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:33:27<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 284/1350 [1:32:40<5:43:46, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0307, 'learning_rate': 2e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:33:45<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 285/1350 [1:32:58<5:38:27, 19.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8622, 'learning_rate': 2e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:34:06<8:49:28, 23.55s/it]  \n",
      " 21%|██        | 286/1350 [1:33:19<5:46:17, 19.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0971, 'learning_rate': 2e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:34:26<8:49:28, 23.55s/it]  \n",
      " 21%|██▏       | 287/1350 [1:33:39<5:48:49, 19.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7586, 'learning_rate': 2e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:34:48<8:49:28, 23.55s/it]  \n",
      " 21%|██▏       | 288/1350 [1:34:01<6:01:52, 20.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9058, 'learning_rate': 2e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:35:07<8:49:28, 23.55s/it]  \n",
      " 21%|██▏       | 289/1350 [1:34:20<5:53:45, 20.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9993, 'learning_rate': 2e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:35:29<8:49:28, 23.55s/it]  \n",
      " 21%|██▏       | 290/1350 [1:34:42<6:04:31, 20.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1998, 'learning_rate': 2e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:35:49<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 291/1350 [1:35:02<5:59:03, 20.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8979, 'learning_rate': 2e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:36:11<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 292/1350 [1:35:24<6:08:46, 20.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9472, 'learning_rate': 2e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:36:32<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 293/1350 [1:35:45<6:07:19, 20.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0445, 'learning_rate': 2e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:36:49<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 294/1350 [1:36:01<5:43:58, 19.54s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8972, 'learning_rate': 2e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:37:07<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 295/1350 [1:36:19<5:36:13, 19.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.169, 'learning_rate': 2e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:37:26<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 296/1350 [1:36:38<5:36:47, 19.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.869, 'learning_rate': 2e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:37:44<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 297/1350 [1:36:56<5:28:32, 18.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9066, 'learning_rate': 2e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:38:02<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 298/1350 [1:37:15<5:28:25, 18.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.76, 'learning_rate': 2e-05, 'epoch': 1.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:38:24<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 299/1350 [1:37:36<5:42:30, 19.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9721, 'learning_rate': 2e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:38:44<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 300/1350 [1:37:56<5:43:19, 19.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.203, 'learning_rate': 2e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:39:02<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 301/1350 [1:38:15<5:39:09, 19.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.179, 'learning_rate': 2e-05, 'epoch': 1.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:39:22<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 302/1350 [1:38:34<5:36:55, 19.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7698, 'learning_rate': 2e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:39:43<8:49:28, 23.55s/it]  \n",
      " 22%|██▏       | 303/1350 [1:38:55<5:46:34, 19.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9535, 'learning_rate': 2e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:40:03<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 304/1350 [1:39:15<5:46:56, 19.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0994, 'learning_rate': 2e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:40:23<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 305/1350 [1:39:36<5:49:22, 20.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0193, 'learning_rate': 2e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:40:46<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 306/1350 [1:39:58<6:02:32, 20.84s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0694, 'learning_rate': 2e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:41:08<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 307/1350 [1:40:20<6:07:10, 21.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9992, 'learning_rate': 2e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:41:26<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 308/1350 [1:40:38<5:50:37, 20.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7329, 'learning_rate': 2e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:41:44<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 309/1350 [1:40:56<5:39:54, 19.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9309, 'learning_rate': 2e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:42:02<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 310/1350 [1:41:15<5:32:35, 19.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0346, 'learning_rate': 2e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:42:21<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 311/1350 [1:41:34<5:32:45, 19.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9296, 'learning_rate': 2e-05, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:42:42<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 312/1350 [1:41:55<5:40:23, 19.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.998, 'learning_rate': 2e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:43:02<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 313/1350 [1:42:15<5:43:19, 19.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8913, 'learning_rate': 2e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:43:20<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 314/1350 [1:42:32<5:29:50, 19.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8941, 'learning_rate': 2e-05, 'epoch': 1.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:43:46<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 315/1350 [1:42:58<6:04:15, 21.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8265, 'learning_rate': 2e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:44:05<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 316/1350 [1:43:18<5:56:18, 20.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0869, 'learning_rate': 2e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:44:22<8:49:28, 23.55s/it]  \n",
      " 23%|██▎       | 317/1350 [1:43:35<5:38:01, 19.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8422, 'learning_rate': 2e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:44:39<8:49:28, 23.55s/it]  \n",
      " 24%|██▎       | 318/1350 [1:43:52<5:24:51, 18.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8418, 'learning_rate': 2e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:45:00<8:49:28, 23.55s/it]  \n",
      " 24%|██▎       | 319/1350 [1:44:12<5:32:40, 19.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9221, 'learning_rate': 2e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:45:22<8:49:28, 23.55s/it]  \n",
      " 24%|██▎       | 320/1350 [1:44:34<5:45:13, 20.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2065, 'learning_rate': 2e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:45:40<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 321/1350 [1:44:52<5:32:45, 19.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0972, 'learning_rate': 2e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:45:58<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 322/1350 [1:45:11<5:27:31, 19.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0011, 'learning_rate': 2e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:46:16<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 323/1350 [1:45:28<5:21:08, 18.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6553, 'learning_rate': 2e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:46:38<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 324/1350 [1:45:50<5:37:22, 19.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.889, 'learning_rate': 2e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:46:58<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 325/1350 [1:46:11<5:40:16, 19.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9633, 'learning_rate': 2e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:47:16<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 326/1350 [1:46:29<5:28:41, 19.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9174, 'learning_rate': 2e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:47:39<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 327/1350 [1:46:52<5:48:23, 20.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2632, 'learning_rate': 2e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:47:59<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 328/1350 [1:47:11<5:43:47, 20.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8784, 'learning_rate': 2e-05, 'epoch': 1.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:48:19<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 329/1350 [1:47:32<5:45:59, 20.33s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7619, 'learning_rate': 2e-05, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:48:38<8:49:28, 23.55s/it]  \n",
      " 24%|██▍       | 330/1350 [1:47:51<5:38:16, 19.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7578, 'learning_rate': 2e-05, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:48:58<8:49:28, 23.55s/it]  \n",
      " 25%|██▍       | 331/1350 [1:48:10<5:35:20, 19.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8881, 'learning_rate': 2e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:49:21<8:49:28, 23.55s/it]  \n",
      " 25%|██▍       | 332/1350 [1:48:33<5:50:39, 20.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9224, 'learning_rate': 2e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:49:40<8:49:28, 23.55s/it]  \n",
      " 25%|██▍       | 333/1350 [1:48:53<5:44:28, 20.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0281, 'learning_rate': 2e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:49:59<8:49:28, 23.55s/it]  \n",
      " 25%|██▍       | 334/1350 [1:49:11<5:34:34, 19.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1359, 'learning_rate': 2e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:50:17<8:49:28, 23.55s/it]  \n",
      " 25%|██▍       | 335/1350 [1:49:29<5:27:23, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1873, 'learning_rate': 2e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:50:36<8:49:28, 23.55s/it]  \n",
      " 25%|██▍       | 336/1350 [1:49:49<5:27:04, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9966, 'learning_rate': 2e-05, 'epoch': 1.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:50:57<8:49:28, 23.55s/it]  \n",
      " 25%|██▍       | 337/1350 [1:50:09<5:32:21, 19.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9993, 'learning_rate': 2e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:51:18<8:49:28, 23.55s/it]  \n",
      " 25%|██▌       | 338/1350 [1:50:30<5:37:57, 20.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.943, 'learning_rate': 2e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:51:35<8:49:28, 23.55s/it]  \n",
      " 25%|██▌       | 339/1350 [1:50:48<5:26:06, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0904, 'learning_rate': 2e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:51:57<8:49:28, 23.55s/it]  \n",
      " 25%|██▌       | 340/1350 [1:51:09<5:35:26, 19.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.145, 'learning_rate': 2e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:52:17<8:49:28, 23.55s/it]  \n",
      " 25%|██▌       | 341/1350 [1:51:29<5:36:52, 20.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9512, 'learning_rate': 2e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:52:40<8:49:28, 23.55s/it]  \n",
      " 25%|██▌       | 342/1350 [1:51:52<5:49:40, 20.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0715, 'learning_rate': 2e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:52:59<8:49:28, 23.55s/it]  \n",
      " 25%|██▌       | 343/1350 [1:52:12<5:44:00, 20.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1505, 'learning_rate': 2e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:53:17<8:49:28, 23.55s/it]  \n",
      " 25%|██▌       | 344/1350 [1:52:29<5:29:27, 19.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8151, 'learning_rate': 2e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:53:36<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 345/1350 [1:52:49<5:27:41, 19.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9655, 'learning_rate': 2e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:53:56<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 346/1350 [1:53:08<5:26:08, 19.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6346, 'learning_rate': 2e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:54:14<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 347/1350 [1:53:26<5:17:35, 19.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1487, 'learning_rate': 2e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:54:36<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 348/1350 [1:53:48<5:32:22, 19.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.03, 'learning_rate': 2e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:54:55<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 349/1350 [1:54:08<5:31:12, 19.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9564, 'learning_rate': 2e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:55:14<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 350/1350 [1:54:27<5:26:54, 19.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0754, 'learning_rate': 2e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:55:32<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 351/1350 [1:54:45<5:17:29, 19.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8478, 'learning_rate': 2e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:55:52<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 352/1350 [1:55:04<5:20:45, 19.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1766, 'learning_rate': 2e-05, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:56:12<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 353/1350 [1:55:24<5:24:02, 19.50s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6071, 'learning_rate': 2e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:56:30<8:49:28, 23.55s/it]  \n",
      " 26%|██▌       | 354/1350 [1:55:42<5:16:28, 19.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7738, 'learning_rate': 2e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:56:49<8:49:28, 23.55s/it]  \n",
      " 26%|██▋       | 355/1350 [1:56:02<5:16:10, 19.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0837, 'learning_rate': 2e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:57:12<8:49:28, 23.55s/it]  \n",
      " 26%|██▋       | 356/1350 [1:56:24<5:33:45, 20.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1472, 'learning_rate': 2e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:57:28<8:49:28, 23.55s/it]  \n",
      " 26%|██▋       | 357/1350 [1:56:40<5:14:17, 18.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8948, 'learning_rate': 2e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:57:47<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 358/1350 [1:57:00<5:16:07, 19.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6962, 'learning_rate': 2e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:58:04<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 359/1350 [1:57:17<5:03:57, 18.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8765, 'learning_rate': 2e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:58:23<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 360/1350 [1:57:36<5:07:35, 18.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6898, 'learning_rate': 2e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:58:50<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 361/1350 [1:58:02<5:45:10, 20.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2451, 'learning_rate': 2e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:59:08<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 362/1350 [1:58:20<5:30:26, 20.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7218, 'learning_rate': 2e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:59:27<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 363/1350 [1:58:40<5:27:30, 19.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9488, 'learning_rate': 2e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [1:59:47<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 364/1350 [1:59:00<5:27:04, 19.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6982, 'learning_rate': 2e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:00:08<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 365/1350 [1:59:21<5:34:03, 20.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8331, 'learning_rate': 2e-05, 'epoch': 1.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:00:27<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 366/1350 [1:59:40<5:26:02, 19.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.983, 'learning_rate': 2e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:00:46<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 367/1350 [1:59:58<5:18:23, 19.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8535, 'learning_rate': 2e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:01:05<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 368/1350 [2:00:18<5:18:52, 19.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9559, 'learning_rate': 2e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:01:25<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 369/1350 [2:00:37<5:17:38, 19.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.894, 'learning_rate': 2e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:01:43<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 370/1350 [2:00:55<5:11:42, 19.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0305, 'learning_rate': 2e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:02:05<8:49:28, 23.55s/it]  \n",
      " 27%|██▋       | 371/1350 [2:01:18<5:26:44, 20.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0876, 'learning_rate': 2e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:02:25<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 372/1350 [2:01:38<5:26:49, 20.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9982, 'learning_rate': 2e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:02:46<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 373/1350 [2:01:59<5:32:23, 20.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9228, 'learning_rate': 2e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:03:07<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 374/1350 [2:02:19<5:32:27, 20.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9694, 'learning_rate': 2e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:03:26<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 375/1350 [2:02:38<5:23:59, 19.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9067, 'learning_rate': 2e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:03:47<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 376/1350 [2:03:00<5:30:42, 20.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2304, 'learning_rate': 2e-05, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:04:05<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 377/1350 [2:03:18<5:18:57, 19.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8335, 'learning_rate': 2e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:04:26<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 378/1350 [2:03:39<5:25:36, 20.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9534, 'learning_rate': 2e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:04:45<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 379/1350 [2:03:58<5:20:40, 19.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7256, 'learning_rate': 2e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:05:04<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 380/1350 [2:04:17<5:15:33, 19.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.959, 'learning_rate': 2e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:05:23<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 381/1350 [2:04:35<5:11:13, 19.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0742, 'learning_rate': 2e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:05:46<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 382/1350 [2:04:58<5:28:24, 20.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6539, 'learning_rate': 2e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:06:05<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 383/1350 [2:05:17<5:20:55, 19.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8127, 'learning_rate': 2e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:06:25<8:49:28, 23.55s/it]  \n",
      " 28%|██▊       | 384/1350 [2:05:37<5:20:58, 19.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1013, 'learning_rate': 2e-05, 'epoch': 1.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:06:43<8:49:28, 23.55s/it]  \n",
      " 29%|██▊       | 385/1350 [2:05:55<5:11:53, 19.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9003, 'learning_rate': 2e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:07:02<8:49:28, 23.55s/it]  \n",
      " 29%|██▊       | 386/1350 [2:06:15<5:12:10, 19.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8804, 'learning_rate': 2e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:07:21<8:49:28, 23.55s/it]  \n",
      " 29%|██▊       | 387/1350 [2:06:34<5:09:31, 19.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9523, 'learning_rate': 2e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:07:43<8:49:28, 23.55s/it]  \n",
      " 29%|██▊       | 388/1350 [2:06:55<5:18:47, 19.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8938, 'learning_rate': 2e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:08:02<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 389/1350 [2:07:15<5:18:18, 19.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9994, 'learning_rate': 2e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:08:19<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 390/1350 [2:07:32<5:03:54, 18.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8905, 'learning_rate': 2e-05, 'epoch': 1.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:08:39<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 391/1350 [2:07:51<5:05:59, 19.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9337, 'learning_rate': 2e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:09:00<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 392/1350 [2:08:12<5:15:12, 19.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9561, 'learning_rate': 2e-05, 'epoch': 1.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:09:22<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 393/1350 [2:08:34<5:24:09, 20.32s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9915, 'learning_rate': 2e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:09:41<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 394/1350 [2:08:54<5:21:14, 20.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1285, 'learning_rate': 2e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:10:00<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 395/1350 [2:09:12<5:12:01, 19.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8703, 'learning_rate': 2e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:10:19<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 396/1350 [2:09:31<5:09:39, 19.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9535, 'learning_rate': 2e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:10:41<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 397/1350 [2:09:54<5:22:11, 20.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.166, 'learning_rate': 2e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:10:59<8:49:28, 23.55s/it]  \n",
      " 29%|██▉       | 398/1350 [2:10:12<5:12:17, 19.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6918, 'learning_rate': 2e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:11:19<8:49:28, 23.55s/it]  \n",
      " 30%|██▉       | 399/1350 [2:10:32<5:12:54, 19.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9731, 'learning_rate': 2e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:11:39<8:49:28, 23.55s/it]  \n",
      " 30%|██▉       | 400/1350 [2:10:52<5:14:48, 19.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9348, 'learning_rate': 2e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:11:57<8:49:28, 23.55s/it]  \n",
      " 30%|██▉       | 401/1350 [2:11:10<5:05:04, 19.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0799, 'learning_rate': 2e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:12:16<8:49:28, 23.55s/it]  \n",
      " 30%|██▉       | 402/1350 [2:11:28<5:01:08, 19.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8805, 'learning_rate': 2e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:12:36<8:49:28, 23.55s/it]  \n",
      " 30%|██▉       | 403/1350 [2:11:49<5:07:31, 19.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7519, 'learning_rate': 2e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:12:56<8:49:28, 23.55s/it]  \n",
      " 30%|██▉       | 404/1350 [2:12:08<5:06:11, 19.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8837, 'learning_rate': 2e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:13:21<8:49:28, 23.55s/it]  \n",
      " 30%|███       | 405/1350 [2:12:33<5:33:39, 21.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3375, 'learning_rate': 2e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:13:38<8:49:28, 23.55s/it]  \n",
      " 30%|███       | 406/1350 [2:12:51<5:14:20, 19.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0224, 'learning_rate': 2e-05, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:13:57<8:49:28, 23.55s/it]  \n",
      " 30%|███       | 407/1350 [2:13:10<5:11:03, 19.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0625, 'learning_rate': 2e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:14:17<8:49:28, 23.55s/it]  \n",
      " 30%|███       | 408/1350 [2:13:29<5:07:47, 19.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1804, 'learning_rate': 2e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:14:35<8:49:28, 23.55s/it]  \n",
      " 30%|███       | 409/1350 [2:13:48<5:03:33, 19.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8016, 'learning_rate': 2e-05, 'epoch': 1.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:14:53<8:49:28, 23.55s/it]  \n",
      " 30%|███       | 410/1350 [2:14:05<4:55:01, 18.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8629, 'learning_rate': 2e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:15:14<8:49:28, 23.55s/it]  \n",
      " 30%|███       | 411/1350 [2:14:27<5:05:17, 19.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8826, 'learning_rate': 2e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:15:37<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 412/1350 [2:14:49<5:20:36, 20.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9419, 'learning_rate': 2e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:15:54<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 413/1350 [2:15:07<5:05:53, 19.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9025, 'learning_rate': 2e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:16:13<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 414/1350 [2:15:26<5:02:00, 19.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0793, 'learning_rate': 2e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:16:34<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 415/1350 [2:15:47<5:08:53, 19.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0183, 'learning_rate': 2e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:16:54<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 416/1350 [2:16:07<5:09:36, 19.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8291, 'learning_rate': 2e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:17:14<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 417/1350 [2:16:26<5:08:45, 19.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2048, 'learning_rate': 2e-05, 'epoch': 1.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:17:33<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 418/1350 [2:16:46<5:06:16, 19.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.986, 'learning_rate': 2e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:17:54<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 419/1350 [2:17:07<5:11:17, 20.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.845, 'learning_rate': 2e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:18:15<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 420/1350 [2:17:27<5:14:29, 20.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0042, 'learning_rate': 2e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:18:35<8:49:28, 23.55s/it]  \n",
      " 31%|███       | 421/1350 [2:17:47<5:12:39, 20.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8828, 'learning_rate': 2e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:18:55<8:49:28, 23.55s/it]  \n",
      " 31%|███▏      | 422/1350 [2:18:07<5:10:37, 20.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9288, 'learning_rate': 2e-05, 'epoch': 1.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:19:14<8:49:28, 23.55s/it]  \n",
      " 31%|███▏      | 423/1350 [2:18:26<5:05:36, 19.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7054, 'learning_rate': 2e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:19:34<8:49:28, 23.55s/it]  \n",
      " 31%|███▏      | 424/1350 [2:18:47<5:07:41, 19.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7633, 'learning_rate': 2e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:19:51<8:49:28, 23.55s/it]  \n",
      " 31%|███▏      | 425/1350 [2:19:04<4:54:19, 19.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7754, 'learning_rate': 2e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:20:14<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 426/1350 [2:19:26<5:09:23, 20.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8997, 'learning_rate': 2e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:20:34<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 427/1350 [2:19:47<5:11:26, 20.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0507, 'learning_rate': 2e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:20:54<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 428/1350 [2:20:07<5:09:22, 20.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0435, 'learning_rate': 2e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:21:13<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 429/1350 [2:20:26<5:04:18, 19.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6594, 'learning_rate': 2e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:21:32<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 430/1350 [2:20:44<4:56:48, 19.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7741, 'learning_rate': 2e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:21:50<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 431/1350 [2:21:02<4:51:37, 19.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1059, 'learning_rate': 2e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:22:10<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 432/1350 [2:21:23<4:56:54, 19.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7822, 'learning_rate': 2e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:22:28<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 433/1350 [2:21:41<4:50:31, 19.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8304, 'learning_rate': 2e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:22:47<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 434/1350 [2:21:59<4:49:13, 18.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.952, 'learning_rate': 2e-05, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:23:08<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 435/1350 [2:22:20<4:58:19, 19.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9755, 'learning_rate': 2e-05, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:23:29<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 436/1350 [2:22:42<5:05:18, 20.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.013, 'learning_rate': 2e-05, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:23:47<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 437/1350 [2:22:59<4:54:59, 19.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9188, 'learning_rate': 2e-05, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:24:06<8:49:28, 23.55s/it]  \n",
      " 32%|███▏      | 438/1350 [2:23:18<4:51:00, 19.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0722, 'learning_rate': 2e-05, 'epoch': 1.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:24:23<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 439/1350 [2:23:35<4:40:59, 18.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8029, 'learning_rate': 2e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:24:46<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 440/1350 [2:23:58<5:01:37, 19.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8774, 'learning_rate': 2e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:25:03<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 441/1350 [2:24:15<4:47:45, 18.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9157, 'learning_rate': 2e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:25:23<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 442/1350 [2:24:35<4:53:38, 19.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9619, 'learning_rate': 2e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:25:41<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 443/1350 [2:24:54<4:48:35, 19.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8164, 'learning_rate': 2e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:26:01<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 444/1350 [2:25:13<4:50:21, 19.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8753, 'learning_rate': 2e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:26:20<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 445/1350 [2:25:32<4:47:23, 19.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1227, 'learning_rate': 2e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:26:39<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 446/1350 [2:25:52<4:50:55, 19.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8894, 'learning_rate': 2e-05, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:26:57<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 447/1350 [2:26:10<4:44:28, 18.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.973, 'learning_rate': 2e-05, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:27:15<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 448/1350 [2:26:28<4:39:58, 18.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8647, 'learning_rate': 2e-05, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:27:35<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 449/1350 [2:26:47<4:43:00, 18.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0778, 'learning_rate': 2e-05, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:27:55<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 450/1350 [2:27:07<4:46:57, 19.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9414, 'learning_rate': 2e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:28:13<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 451/1350 [2:27:26<4:44:34, 18.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9415, 'learning_rate': 2e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:28:31<8:49:28, 23.55s/it]  \n",
      " 33%|███▎      | 452/1350 [2:27:43<4:37:05, 18.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9842, 'learning_rate': 2e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:28:52<8:49:28, 23.55s/it]  \n",
      " 34%|███▎      | 453/1350 [2:28:04<4:47:44, 19.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9097, 'learning_rate': 2e-05, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:29:11<8:49:28, 23.55s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9119, 'learning_rate': 2e-05, 'epoch': 1.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▎      | 454/1350 [2:28:23<4:46:48, 19.21s/it]\u001b[A\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:29:31<8:49:28, 23.55s/it]  \n",
      " 34%|███▎      | 455/1350 [2:28:43<4:50:18, 19.46s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.905, 'learning_rate': 2e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:29:50<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 456/1350 [2:29:03<4:49:30, 19.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7439, 'learning_rate': 2e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:30:07<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 457/1350 [2:29:20<4:39:25, 18.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9904, 'learning_rate': 2e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:30:27<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 458/1350 [2:29:39<4:42:02, 18.97s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8771, 'learning_rate': 2e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:30:45<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 459/1350 [2:29:58<4:39:36, 18.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9494, 'learning_rate': 2e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:31:04<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 460/1350 [2:30:17<4:41:07, 18.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.976, 'learning_rate': 2e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:31:25<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 461/1350 [2:30:37<4:47:05, 19.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8648, 'learning_rate': 2e-05, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:31:43<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 462/1350 [2:30:55<4:39:35, 18.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6113, 'learning_rate': 2e-05, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:32:00<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 463/1350 [2:31:13<4:34:14, 18.55s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0269, 'learning_rate': 2e-05, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:32:22<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 464/1350 [2:31:34<4:46:03, 19.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9822, 'learning_rate': 2e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:32:41<8:49:28, 23.55s/it]  \n",
      " 34%|███▍      | 465/1350 [2:31:54<4:46:13, 19.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8202, 'learning_rate': 2e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:33:02<8:49:28, 23.55s/it]  \n",
      " 35%|███▍      | 466/1350 [2:32:15<4:53:14, 19.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8614, 'learning_rate': 2e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:33:22<8:49:28, 23.55s/it]  \n",
      " 35%|███▍      | 467/1350 [2:32:34<4:52:12, 19.86s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7149, 'learning_rate': 2e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:33:41<8:49:28, 23.55s/it]  \n",
      " 35%|███▍      | 468/1350 [2:32:54<4:49:43, 19.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.071, 'learning_rate': 2e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:34:01<8:49:28, 23.55s/it]  \n",
      " 35%|███▍      | 469/1350 [2:33:13<4:48:50, 19.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8317, 'learning_rate': 2e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:34:21<8:49:28, 23.55s/it]  \n",
      " 35%|███▍      | 470/1350 [2:33:33<4:49:50, 19.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.089, 'learning_rate': 2e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:34:44<8:49:28, 23.55s/it]  \n",
      " 35%|███▍      | 471/1350 [2:33:56<5:03:22, 20.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6451, 'learning_rate': 2e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:35:01<8:49:28, 23.55s/it]  \n",
      " 35%|███▍      | 472/1350 [2:34:14<4:49:45, 19.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9793, 'learning_rate': 2e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:35:22<8:49:28, 23.55s/it]  \n",
      " 35%|███▌      | 473/1350 [2:34:34<4:52:22, 20.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9791, 'learning_rate': 2e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:35:42<8:49:28, 23.55s/it]  \n",
      " 35%|███▌      | 474/1350 [2:34:54<4:52:03, 20.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8881, 'learning_rate': 2e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:36:00<8:49:28, 23.55s/it]  \n",
      " 35%|███▌      | 475/1350 [2:35:12<4:42:30, 19.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7654, 'learning_rate': 2e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:36:19<8:49:28, 23.55s/it]  \n",
      " 35%|███▌      | 476/1350 [2:35:31<4:40:48, 19.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7818, 'learning_rate': 2e-05, 'epoch': 1.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:36:40<8:49:28, 23.55s/it]  \n",
      " 35%|███▌      | 477/1350 [2:35:52<4:47:26, 19.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8141, 'learning_rate': 2e-05, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:36:58<8:49:28, 23.55s/it]  \n",
      " 35%|███▌      | 478/1350 [2:36:10<4:38:43, 19.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9512, 'learning_rate': 2e-05, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:37:16<8:49:28, 23.55s/it]  \n",
      " 35%|███▌      | 479/1350 [2:36:29<4:35:53, 19.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0581, 'learning_rate': 2e-05, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:37:34<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 480/1350 [2:36:47<4:31:38, 18.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0774, 'learning_rate': 2e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:37:53<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 481/1350 [2:37:05<4:30:44, 18.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.899, 'learning_rate': 2e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:38:13<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 482/1350 [2:37:26<4:36:41, 19.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7886, 'learning_rate': 2e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:38:31<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 483/1350 [2:37:44<4:31:30, 18.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.969, 'learning_rate': 2e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:38:51<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 484/1350 [2:38:04<4:38:18, 19.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6556, 'learning_rate': 2e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:39:09<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 485/1350 [2:38:22<4:32:10, 18.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9568, 'learning_rate': 2e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:39:31<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 486/1350 [2:38:43<4:43:30, 19.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1716, 'learning_rate': 2e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:39:50<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 487/1350 [2:39:03<4:41:50, 19.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0293, 'learning_rate': 2e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:40:12<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 488/1350 [2:39:24<4:49:26, 20.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6453, 'learning_rate': 2e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:40:32<8:49:28, 23.55s/it]  \n",
      " 36%|███▌      | 489/1350 [2:39:45<4:49:29, 20.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8583, 'learning_rate': 2e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:40:50<8:49:28, 23.55s/it]  \n",
      " 36%|███▋      | 490/1350 [2:40:03<4:41:11, 19.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9042, 'learning_rate': 2e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:41:10<8:49:28, 23.55s/it]  \n",
      " 36%|███▋      | 491/1350 [2:40:23<4:42:19, 19.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8932, 'learning_rate': 2e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:41:27<8:49:28, 23.55s/it]  \n",
      " 36%|███▋      | 492/1350 [2:40:39<4:28:19, 18.76s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6144, 'learning_rate': 2e-05, 'epoch': 1.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:41:47<8:49:28, 23.55s/it]  \n",
      " 37%|███▋      | 493/1350 [2:40:59<4:32:22, 19.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8905, 'learning_rate': 2e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:42:07<8:49:28, 23.55s/it]  \n",
      " 37%|███▋      | 494/1350 [2:41:19<4:36:01, 19.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9108, 'learning_rate': 2e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:42:27<8:49:28, 23.55s/it]  \n",
      " 37%|███▋      | 495/1350 [2:41:40<4:41:13, 19.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9893, 'learning_rate': 2e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:42:48<8:49:28, 23.55s/it]  \n",
      " 37%|███▋      | 496/1350 [2:42:00<4:44:40, 20.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6814, 'learning_rate': 2e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:43:07<8:49:28, 23.55s/it]  \n",
      " 37%|███▋      | 497/1350 [2:42:19<4:39:40, 19.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7628, 'learning_rate': 2e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:43:26<8:49:28, 23.55s/it]  \n",
      " 37%|███▋      | 498/1350 [2:42:39<4:37:43, 19.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9163, 'learning_rate': 2e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:43:45<8:49:28, 23.55s/it]  \n",
      " 37%|███▋      | 499/1350 [2:42:58<4:36:44, 19.51s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9997, 'learning_rate': 2e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                    t]\u001b[A\n",
      "  0%|          | 1/1350 [2:44:03<8:49:28, 23.55s/it]  \n",
      " 37%|███▋      | 500/1350 [2:43:16<4:28:04, 18.92s/it]\u001b[A/home/jupyter/.local/lib/python3.10/site-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 2a81632a-5bf8-411c-b9a3-1623fe6397e3)') - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9896, 'learning_rate': 2e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaForCausalLM' object has no attribute 'save_checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# defer to nn.Module's logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PeftModelForCausalLM' object has no attribute 'save_checkpoint'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/model.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# defer to nn.Module's logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LoraModel' object has no attribute 'save_checkpoint'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-a7fca1af9ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1982\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1984\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1985\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1986\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2339\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2340\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2398\u001b[0m             \u001b[0;31m# under zero3 model file itself doesn't get saved since it's bogus! Unless deepspeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m             \u001b[0;31m# config `stage3_gather_16bit_weights_on_model_save` is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_wrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m         \u001b[0;31m# Save optimizer and scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/peft_model.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# defer to nn.Module's logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/peft/tuners/lora/model.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# defer to nn.Module's logic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_peft_config_as_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaForCausalLM' object has no attribute 'save_checkpoint'"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T04:48:45.389854Z",
     "iopub.status.busy": "2024-05-09T04:48:45.389375Z",
     "iopub.status.idle": "2024-05-09T04:49:44.153508Z",
     "shell.execute_reply": "2024-05-09T04:49:44.152411Z",
     "shell.execute_reply.started": "2024-05-09T04:48:45.389822Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "  0%|          | 1/1350 [4:54:38<6624:21:56, 17678.07s/it]\n",
      " 37%|███▋      | 500/1350 [4:53:50<8:19:31, 35.26s/it]\n",
      "  0%|          | 1/1350 [00:23<8:49:10, 23.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2017, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1350 [00:44<8:20:21, 22.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0156, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-a7fca1af9ab8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata_collator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1892\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2011\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir=training_args.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Try to train from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T14:23:57.481056Z",
     "iopub.status.busy": "2024-05-09T14:23:57.480684Z",
     "iopub.status.idle": "2024-05-09T14:23:57.821239Z",
     "shell.execute_reply": "2024-05-09T14:23:57.820350Z",
     "shell.execute_reply.started": "2024-05-09T14:23:57.481036Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find a valid checkpoint at output_dir_8k_big_data/checkpoint-500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-76cea7b55f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_deepspeed_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1747\u001b[0;31m                 \u001b[0mdeepspeed_load_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_wrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/integrations/deepspeed.py\u001b[0m in \u001b[0;36mdeepspeed_load_checkpoint\u001b[0;34m(deepspeed_engine, checkpoint_path)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[deepspeed] failed to resume from checkpoint {checkpoint_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't find a valid checkpoint at {checkpoint_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Can't find a valid checkpoint at output_dir_8k_big_data/checkpoint-500"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Push learn model to hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T04:35:40.217555Z",
     "iopub.status.busy": "2024-05-09T04:35:40.217073Z",
     "iopub.status.idle": "2024-05-09T04:36:26.158946Z",
     "shell.execute_reply": "2024-05-09T04:36:26.158178Z",
     "shell.execute_reply.started": "2024-05-09T04:35:40.217536Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /tmp/xdg_cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "adapter_model.bin:   0%|          | 0.00/1.08G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   0%|          | 8.19k/1.08G [00:00<7:19:38, 41.0kB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   0%|          | 336k/1.08G [00:00<14:54, 1.21MB/s]   \u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   0%|          | 2.06M/1.08G [00:00<02:59, 6.01MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   0%|          | 5.27M/1.08G [00:00<01:31, 11.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   1%|          | 8.15M/1.08G [00:00<01:08, 15.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   1%|          | 12.1M/1.08G [00:00<00:50, 21.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   1%|▏         | 16.0M/1.08G [00:01<01:29, 11.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   2%|▏         | 24.4M/1.08G [00:01<00:46, 22.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   3%|▎         | 32.0M/1.08G [00:02<00:56, 18.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   4%|▎         | 40.0M/1.08G [00:02<00:43, 23.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   4%|▍         | 43.5M/1.08G [00:02<00:45, 22.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   4%|▍         | 46.5M/1.08G [00:02<00:44, 23.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   5%|▍         | 49.4M/1.08G [00:03<01:19, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   5%|▌         | 55.8M/1.08G [00:03<00:58, 17.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   5%|▌         | 59.1M/1.08G [00:03<00:59, 17.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   6%|▌         | 61.4M/1.08G [00:03<00:58, 17.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   6%|▌         | 64.0M/1.08G [00:04<01:40, 10.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   7%|▋         | 70.7M/1.08G [00:04<01:01, 16.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   7%|▋         | 75.1M/1.08G [00:04<01:04, 15.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   7%|▋         | 77.8M/1.08G [00:04<01:00, 16.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   7%|▋         | 80.3M/1.08G [00:05<01:32, 10.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   8%|▊         | 88.4M/1.08G [00:05<00:51, 19.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:   9%|▉         | 96.0M/1.08G [00:05<00:49, 19.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  10%|▉         | 104M/1.08G [00:05<00:35, 27.3MB/s] \u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  10%|▉         | 108M/1.08G [00:06<00:45, 21.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  10%|█         | 112M/1.08G [00:06<00:46, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  11%|█         | 115M/1.08G [00:06<01:07, 14.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  11%|█▏        | 123M/1.08G [00:07<00:45, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  12%|█▏        | 128M/1.08G [00:07<00:51, 18.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  13%|█▎        | 136M/1.08G [00:07<00:36, 25.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  13%|█▎        | 143M/1.08G [00:07<00:28, 33.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  14%|█▎        | 148M/1.08G [00:08<00:42, 22.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  14%|█▍        | 157M/1.08G [00:08<00:30, 30.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  15%|█▍        | 162M/1.08G [00:08<00:38, 23.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  16%|█▌        | 170M/1.08G [00:08<00:30, 30.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  16%|█▋        | 176M/1.08G [00:08<00:35, 25.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  17%|█▋        | 184M/1.08G [00:09<00:27, 33.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  18%|█▊        | 192M/1.08G [00:09<00:31, 28.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  18%|█▊        | 200M/1.08G [00:09<00:27, 31.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  19%|█▉        | 204M/1.08G [00:10<00:40, 21.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  19%|█▉        | 207M/1.08G [00:10<00:41, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  19%|█▉        | 210M/1.08G [00:10<01:09, 12.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  20%|█▉        | 214M/1.08G [00:11<00:59, 14.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  20%|██        | 219M/1.08G [00:11<01:06, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  20%|██        | 221M/1.08G [00:11<01:04, 13.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  21%|██        | 224M/1.08G [00:12<01:36, 8.87MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  21%|██▏       | 231M/1.08G [00:12<00:59, 14.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  22%|██▏       | 238M/1.08G [00:12<00:38, 21.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  22%|██▏       | 243M/1.08G [00:12<00:40, 20.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  23%|██▎       | 248M/1.08G [00:12<00:34, 24.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  23%|██▎       | 252M/1.08G [00:13<00:42, 19.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  24%|██▎       | 254M/1.08G [00:13<00:40, 20.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  24%|██▍       | 257M/1.08G [00:13<01:08, 12.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  25%|██▍       | 266M/1.08G [00:13<00:39, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  25%|██▌       | 272M/1.08G [00:14<00:41, 19.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  26%|██▌       | 280M/1.08G [00:14<00:28, 27.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  27%|██▋       | 288M/1.08G [00:14<00:32, 24.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  27%|██▋       | 296M/1.08G [00:14<00:24, 31.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  28%|██▊       | 303M/1.08G [00:15<00:20, 38.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  29%|██▊       | 309M/1.08G [00:15<00:26, 29.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  29%|██▉       | 318M/1.08G [00:15<00:20, 37.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  30%|██▉       | 323M/1.08G [00:15<00:25, 29.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  31%|███       | 330M/1.08G [00:15<00:22, 33.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  31%|███       | 336M/1.08G [00:16<00:26, 27.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  32%|███▏      | 344M/1.08G [00:16<00:20, 35.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  33%|███▎      | 352M/1.08G [00:16<00:16, 43.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  33%|███▎      | 358M/1.08G [00:16<00:21, 34.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  34%|███▍      | 366M/1.08G [00:16<00:18, 39.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  34%|███▍      | 371M/1.08G [00:17<00:21, 33.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  35%|███▌      | 379M/1.08G [00:17<00:18, 38.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  35%|███▌      | 384M/1.08G [00:17<00:24, 28.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  36%|███▋      | 393M/1.08G [00:17<00:18, 37.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  37%|███▋      | 400M/1.08G [00:18<00:23, 28.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  38%|███▊      | 409M/1.08G [00:18<00:18, 37.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  38%|███▊      | 416M/1.08G [00:18<00:24, 27.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  39%|███▉      | 425M/1.08G [00:18<00:18, 35.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  40%|███▉      | 432M/1.08G [00:19<00:21, 29.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  41%|████      | 441M/1.08G [00:19<00:16, 37.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  41%|████▏     | 448M/1.08G [00:19<00:22, 28.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  42%|████▏     | 457M/1.08G [00:19<00:17, 35.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  43%|████▎     | 464M/1.08G [00:20<00:23, 25.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  44%|████▎     | 472M/1.08G [00:20<00:18, 33.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  44%|████▍     | 480M/1.08G [00:20<00:21, 27.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  45%|████▌     | 489M/1.08G [00:20<00:16, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  46%|████▌     | 496M/1.08G [00:21<00:19, 29.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  47%|████▋     | 505M/1.08G [00:21<00:15, 37.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  47%|████▋     | 512M/1.08G [00:21<00:20, 28.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  48%|████▊     | 520M/1.08G [00:21<00:15, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  49%|████▉     | 528M/1.08G [00:22<00:18, 29.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  50%|████▉     | 537M/1.08G [00:22<00:14, 38.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  50%|█████     | 544M/1.08G [00:23<00:40, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  51%|█████     | 548M/1.08G [00:23<00:36, 14.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  51%|█████▏    | 555M/1.08G [00:23<00:27, 19.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  52%|█████▏    | 560M/1.08G [00:24<00:33, 15.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  52%|█████▏    | 568M/1.08G [00:24<00:23, 22.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  53%|█████▎    | 576M/1.08G [00:24<00:22, 22.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  54%|█████▍    | 584M/1.08G [00:24<00:17, 28.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  55%|█████▍    | 592M/1.08G [00:25<00:20, 24.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  56%|█████▌    | 601M/1.08G [00:25<00:15, 31.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  56%|█████▌    | 608M/1.08G [00:26<00:22, 20.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  57%|█████▋    | 617M/1.08G [00:26<00:17, 27.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  58%|█████▊    | 624M/1.08G [00:26<00:19, 23.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  58%|█████▊    | 633M/1.08G [00:26<00:14, 30.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  59%|█████▉    | 640M/1.08G [00:27<00:16, 26.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  60%|█████▉    | 649M/1.08G [00:27<00:12, 34.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  61%|██████    | 656M/1.08G [00:27<00:14, 28.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  61%|██████▏   | 665M/1.08G [00:27<00:11, 37.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  62%|██████▏   | 672M/1.08G [00:28<00:12, 32.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  63%|██████▎   | 680M/1.08G [00:28<00:10, 40.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  64%|██████▎   | 688M/1.08G [00:28<00:13, 28.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  64%|██████▍   | 696M/1.08G [00:28<00:10, 36.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  65%|██████▌   | 704M/1.08G [00:29<00:12, 29.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  66%|██████▌   | 712M/1.08G [00:29<00:09, 37.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  67%|██████▋   | 720M/1.08G [00:29<00:11, 31.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  67%|██████▋   | 728M/1.08G [00:29<00:09, 38.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  68%|██████▊   | 736M/1.08G [00:29<00:10, 31.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  69%|██████▊   | 744M/1.08G [00:30<00:08, 38.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  69%|██████▉   | 752M/1.08G [00:30<00:10, 30.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  70%|███████   | 760M/1.08G [00:30<00:08, 36.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  71%|███████   | 768M/1.08G [00:31<00:11, 28.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  72%|███████▏  | 776M/1.08G [00:31<00:08, 35.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  72%|███████▏  | 784M/1.08G [00:31<00:10, 29.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  73%|███████▎  | 793M/1.08G [00:31<00:07, 36.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  74%|███████▍  | 800M/1.08G [00:31<00:09, 28.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  75%|███████▍  | 808M/1.08G [00:32<00:07, 35.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  75%|███████▌  | 816M/1.08G [00:32<00:09, 27.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  76%|███████▌  | 825M/1.08G [00:32<00:07, 35.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  77%|███████▋  | 832M/1.08G [00:33<00:09, 27.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  78%|███████▊  | 840M/1.08G [00:33<00:07, 34.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  78%|███████▊  | 848M/1.08G [00:33<00:08, 27.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  79%|███████▉  | 856M/1.08G [00:33<00:06, 35.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  80%|███████▉  | 864M/1.08G [00:33<00:07, 30.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  81%|████████  | 872M/1.08G [00:34<00:05, 37.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  81%|████████▏ | 880M/1.08G [00:34<00:06, 32.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  82%|████████▏ | 888M/1.08G [00:34<00:04, 40.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  83%|████████▎ | 896M/1.08G [00:34<00:06, 30.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  84%|████████▎ | 904M/1.08G [00:35<00:04, 38.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  84%|████████▍ | 912M/1.08G [00:35<00:05, 31.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  85%|████████▌ | 920M/1.08G [00:35<00:04, 39.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  86%|████████▌ | 928M/1.08G [00:35<00:04, 31.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  86%|████████▋ | 936M/1.08G [00:35<00:03, 37.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  87%|████████▋ | 944M/1.08G [00:36<00:03, 45.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  88%|████████▊ | 950M/1.08G [00:36<00:03, 33.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  89%|████████▊ | 958M/1.08G [00:36<00:03, 38.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  89%|████████▉ | 964M/1.08G [00:36<00:04, 28.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  90%|████████▉ | 971M/1.08G [00:36<00:03, 32.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  90%|█████████ | 976M/1.08G [00:37<00:04, 25.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  91%|█████████ | 985M/1.08G [00:37<00:02, 34.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  92%|█████████▏| 992M/1.08G [00:37<00:03, 28.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  92%|█████████▏| 1.00G/1.08G [00:37<00:02, 36.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  93%|█████████▎| 1.01G/1.08G [00:38<00:02, 29.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  94%|█████████▍| 1.02G/1.08G [00:38<00:01, 36.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  95%|█████████▍| 1.02G/1.08G [00:38<00:01, 31.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  95%|█████████▌| 1.03G/1.08G [00:38<00:01, 38.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  96%|█████████▌| 1.04G/1.08G [00:39<00:01, 27.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  97%|█████████▋| 1.05G/1.08G [00:39<00:00, 35.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  98%|█████████▊| 1.06G/1.08G [00:39<00:00, 28.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  98%|█████████▊| 1.06G/1.08G [00:39<00:00, 35.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin:  99%|█████████▉| 1.07G/1.08G [00:40<00:00, 30.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "adapter_model.bin: 100%|██████████| 1.08G/1.08G [00:40<00:00, 26.6MB/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/nvdenisov2002/llama-longLoRA-v2/commit/25186ac0cee59ead03b56ef117cddfcd942b26ad', commit_message='Upload model', commit_description='', oid='25186ac0cee59ead03b56ef117cddfcd942b26ad', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")\n",
    "model_id = \"nvdenisov2002/llama-longLoRA-v2\"\n",
    "peft_model.push_to_hub(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Infer learnt model on asessors questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T04:38:29.076418Z",
     "iopub.status.busy": "2024-05-09T04:38:29.075855Z",
     "iopub.status.idle": "2024-05-09T04:38:29.091223Z",
     "shell.execute_reply": "2024-05-09T04:38:29.090556Z",
     "shell.execute_reply.started": "2024-05-09T04:38:29.076392Z"
    }
   },
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T04:38:30.180318Z",
     "iopub.status.busy": "2024-05-09T04:38:30.179713Z",
     "iopub.status.idle": "2024-05-09T04:46:45.274142Z",
     "shell.execute_reply": "2024-05-09T04:46:45.273305Z",
     "shell.execute_reply.started": "2024-05-09T04:38:30.180299Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf614f488d44fb9a0f4f8f0925d12f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rows...:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 4316, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 4170, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 5251, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 5349, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 4808, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 5726, but `max_length` is set to 4096. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>diploma</th>\n",
       "      <th>abstract</th>\n",
       "      <th>study_field</th>\n",
       "      <th>degree</th>\n",
       "      <th>original_diploma_extension</th>\n",
       "      <th>raw_model</th>\n",
       "      <th>learnt</th>\n",
       "      <th>learnt_8k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>45042</td>\n",
       "      <td>2023</td>\n",
       "      <td>АЙВАЗЬЯН Аршак Владимирович\\nВыпускная квалифи...</td>\n",
       "      <td>В этой работе мы строим правую трансферную мод...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "      <td>В данной работе рассматривается модельная стру...</td>\n",
       "      <td>В работе рассматриваются алгебраические теории...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>45043</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Пусть 𝐾 выпуклое тело в ℝ^𝑛. Определим 𝑑𝑛,𝑛−1(...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Плотность решетки трансляций - это минимальная...</td>\n",
       "      <td>В работе рассматриваются плотности решеток тра...</td>\n",
       "      <td>В</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>45044</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Работа посвящена повышению производительности ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе представлены результаты исслед...</td>\n",
       "      <td>В работе рассматривается задача булевой выполн...</td>\n",
       "      <td>In this work we propose a method for improving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>45046</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются классы случайных проц...</td>\n",
       "      <td>В данной работе рассматривается энергетически-...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>152</td>\n",
       "      <td>45047</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы рассматривается подход ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе рассматривается задача настраи...</td>\n",
       "      <td>В данной работе рассматривается задача добавле...</td>\n",
       "      <td>В работе рассматривается применение добавления...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  ...                                          learnt_8k\n",
       "0          12  ...  В работе рассматриваются алгебраические теории...\n",
       "1          25  ...                                                  В\n",
       "2          37  ...  In this work we propose a method for improving...\n",
       "3         101  ...  В работе мы обобщаем результаты об энергии нат...\n",
       "4         152  ...  В работе рассматривается применение добавления...\n",
       "\n",
       "[5 rows x 11 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_df = pd.read_csv(\"/home/jupyter/mnt/datasets/diplomas/russian_dataset/russian_dataset_test.csv\")\n",
    "# with open(REPOSITOTY_DIR_PATH.joinpath(\"src/notebooks/junk/mcs_df_human_filled_processed.json\"), \"r\") as f:\n",
    "#     asessors_questions = json.load(f) \n",
    "# ids = [int(x['meta']['id']) for x in asessors_questions]\n",
    "test_df = pd.read_csv(ARTIFACTS_DIR_PATH.joinpath(\"diplomas_abstracts/mcs_raw_learnt_abstract.csv\"))\n",
    "diploma_prefix_len=8000\n",
    "\n",
    "def get_some_model_result(prefix_len, prefix_tokens, some_model):\n",
    "    some_model.eval()\n",
    "    generated = some_model.generate(prefix_tokens.reshape((1, -1)).to(device))\n",
    "    generated_continue = tokenizer.decode(generated.to('cpu').flatten()[prefix_len:])\n",
    "    return generated_continue\n",
    "\n",
    "def get_prefix_len_and_tokens(row):\n",
    "    prompt_input_diploma = PROMPT_DICT[\"prompt_input_diploma_special\"]\n",
    "    source = prompt_input_diploma.format(input=row[\"diploma\"][:diploma_prefix_len])\n",
    "\n",
    "    target = f\"{row['abstract']}{tokenizer.eos_token}\"\n",
    "\n",
    "    data_dict = preprocess([source], [target], tokenizer)\n",
    "    \n",
    "    prefix_len = np.sum(np.array(data_dict[\"labels\"][0]) == IGNORE_INDEX)\n",
    "    prefix_tokens = data_dict[\"input_ids\"][0][:prefix_len]\n",
    "\n",
    "    return prefix_len, prefix_tokens\n",
    "\n",
    "new_rows = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Rows...\"):\n",
    "    new_row = copy.deepcopy(row)\n",
    "    prefix_len, prefix_tokens = get_prefix_len_and_tokens(row)\n",
    "    new_row[\"learnt_8k\"] = get_some_model_result(prefix_len, prefix_tokens, peft_model)\n",
    "    new_rows.append(new_row)\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "new_df.to_csv(ARTIFACTS_DIR_PATH.joinpath(\"diplomas_abstracts/mcs_raw_learnt_abstract_learnt8k.csv\"))\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Comparing lernt vs learn_8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mbook_full_texts\u001b[m\u001b[m    \u001b[1m\u001b[36mdiplomas_abstracts\u001b[m\u001b[m \u001b[1m\u001b[36mmetrics\u001b[m\u001b[m            \u001b[1m\u001b[36mtokens\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mdatasets\u001b[m\u001b[m           \u001b[1m\u001b[36mjunk\u001b[m\u001b[m               \u001b[1m\u001b[36mparsing\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls ../../../artifacts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../../artifacts/diplomas_abstracts/mcs_raw_learnt_abstract_learnt8k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>diploma</th>\n",
       "      <th>abstract</th>\n",
       "      <th>study_field</th>\n",
       "      <th>degree</th>\n",
       "      <th>original_diploma_extension</th>\n",
       "      <th>raw_model</th>\n",
       "      <th>learnt</th>\n",
       "      <th>learnt_8k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>45042</td>\n",
       "      <td>2023</td>\n",
       "      <td>АЙВАЗЬЯН Аршак Владимирович\\nВыпускная квалифи...</td>\n",
       "      <td>В этой работе мы строим правую трансферную мод...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматривается модельная структура н...</td>\n",
       "      <td>В данной работе рассматривается модельная стру...</td>\n",
       "      <td>В работе рассматриваются алгебраические теории...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>45043</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Пусть 𝐾 выпуклое тело в ℝ^𝑛. Определим 𝑑𝑛,𝑛−1(...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Плотность решетки трансляций - это минимальная...</td>\n",
       "      <td>В работе рассматриваются плотности решеток тра...</td>\n",
       "      <td>В</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>45044</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>Работа посвящена повышению производительности ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе представлены результаты исслед...</td>\n",
       "      <td>В работе рассматривается задача булевой выполн...</td>\n",
       "      <td>In this work we propose a method for improving...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>101</td>\n",
       "      <td>45046</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт-Петербургский государственный университе...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В работе рассматриваются классы случайных проц...</td>\n",
       "      <td>В данной работе рассматривается энергетически-...</td>\n",
       "      <td>В работе мы обобщаем результаты об энергии нат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>152</td>\n",
       "      <td>45047</td>\n",
       "      <td>2023</td>\n",
       "      <td>Санкт–Петербургский государственный университе...</td>\n",
       "      <td>В рамках данной работы рассматривается подход ...</td>\n",
       "      <td>MATHEMATICS AND COMPUTER SCIENCE</td>\n",
       "      <td>BACHELOR STUDIES</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>В данной работе рассматривается задача настраи...</td>\n",
       "      <td>В данной работе рассматривается задача добавле...</td>\n",
       "      <td>В работе рассматривается применение добавления...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0     id  year  \\\n",
       "0             0          12  45042  2023   \n",
       "1             1          25  45043  2023   \n",
       "2             2          37  45044  2023   \n",
       "3             3         101  45046  2023   \n",
       "4             4         152  45047  2023   \n",
       "\n",
       "                                             diploma  \\\n",
       "0  АЙВАЗЬЯН Аршак Владимирович\\nВыпускная квалифи...   \n",
       "1  Санкт-Петербургский государственный университе...   \n",
       "2  Санкт-Петербургский государственный университе...   \n",
       "3  Санкт-Петербургский государственный университе...   \n",
       "4  Санкт–Петербургский государственный университе...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  В этой работе мы строим правую трансферную мод...   \n",
       "1  Пусть 𝐾 выпуклое тело в ℝ^𝑛. Определим 𝑑𝑛,𝑛−1(...   \n",
       "2  Работа посвящена повышению производительности ...   \n",
       "3  В работе мы обобщаем результаты об энергии нат...   \n",
       "4  В рамках данной работы рассматривается подход ...   \n",
       "\n",
       "                        study_field            degree  \\\n",
       "0  MATHEMATICS AND COMPUTER SCIENCE  BACHELOR STUDIES   \n",
       "1  MATHEMATICS AND COMPUTER SCIENCE  BACHELOR STUDIES   \n",
       "2  MATHEMATICS AND COMPUTER SCIENCE  BACHELOR STUDIES   \n",
       "3  MATHEMATICS AND COMPUTER SCIENCE  BACHELOR STUDIES   \n",
       "4  MATHEMATICS AND COMPUTER SCIENCE  BACHELOR STUDIES   \n",
       "\n",
       "  original_diploma_extension  \\\n",
       "0                       .pdf   \n",
       "1                       .pdf   \n",
       "2                       .pdf   \n",
       "3                       .pdf   \n",
       "4                       .pdf   \n",
       "\n",
       "                                           raw_model  \\\n",
       "0  В работе рассматривается модельная структура н...   \n",
       "1  Плотность решетки трансляций - это минимальная...   \n",
       "2  В данной работе представлены результаты исслед...   \n",
       "3  В работе рассматриваются классы случайных проц...   \n",
       "4  В данной работе рассматривается задача настраи...   \n",
       "\n",
       "                                              learnt  \\\n",
       "0  В данной работе рассматривается модельная стру...   \n",
       "1  В работе рассматриваются плотности решеток тра...   \n",
       "2  В работе рассматривается задача булевой выполн...   \n",
       "3  В данной работе рассматривается энергетически-...   \n",
       "4  В данной работе рассматривается задача добавле...   \n",
       "\n",
       "                                           learnt_8k  \n",
       "0  В работе рассматриваются алгебраические теории...  \n",
       "1                                                  В  \n",
       "2  In this work we propose a method for improving...  \n",
       "3  В работе мы обобщаем результаты об энергии нат...  \n",
       "4  В работе рассматривается применение добавления...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700.0428571428571, 446.9428571428571, 444.0285714285714, 587.1285714285714)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(list(map(len, df[\"learnt\"]))), np.mean(list(map(len, df[\"raw_model\"]))), np.mean(list(map(len, df[\"learnt_8k\"].astype(str)))), np.mean(list(map(len, df[\"abstract\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В данной работе рассматривается модельная структура алгебр над теорией Ловера.</s>\n",
      "В работе рассматриваются плотности решеток трансляций в трёхмерном пространстве. В частности, получается оценка плотности решеток трансляций для любой замкнутой выпуклой конвексной области 𝐾 в трёхмерном пространстве.</s>\n",
      "В работе рассматривается задача булевой выполнимости. Помимо известных методов решения, в работе предлагается использовать методы машинного обучения для поиска эвристических правил. Для этого используются методы сжатия данных и методы машинного обучения.</s>\n",
      "В данной работе рассматривается энергетически-эффективная аппроксимация многомерных случайных процессов. В качестве аппроксимационных процессов рассматриваются процессы с медленно убывающими вторыми моментами, а также их комбинации. В качестве энергетически-эффективной аппроксимации рассматривается процесс с медленно убывающими вторыми моментами, а также его комбинация с процессом с медленно убывающими вторыми моментами. В работе рассматривается, какие процессы с медленно убывающими вторыми моментами и их комбинации можно аппроксимировать энергетически-эффективными процессами с медленно убывающими вторыми моментами и их комбинациями.</s>\n",
      "В данной работе рассматривается задача добавления признаков композиций в функцию потерь модели MusicTransformer для настраиваемой генерации музыки. В работе рассматриваются различные способы представления композиций и их комбинации с RPR. Представленный способ позволяет добавлять признаки композиций в функцию потерь модели MusicTransformer, а также позволяет использовать в качестве признака композиции только один из компонентов RPR.</s>\n",
      "Обозначим группу \\(G\\) и действие \\(G\\) на пространстве \\(X\\).\n",
      "Действием \\(G\\) на пространстве \\(X\\) называется бинарная функция \\(G\\times X\\to X\\), определенная набором элементов \\(g\\in G\\) и элементов \\(x\\in X\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=x\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\), как множество элементов \\(G\\), входящих в последовательность \\(x\\).\n",
      "Обозначим множество элементов \\(g\\in G\\), для которых существует элемент \\(x\\in X\\) таков, что \\(g\\cdot x=x\\) и \\(g\\cdot y=y\\) для любого элемента \\(y\\in X\\), отличного от \\(x\\) и для любого элемента \\(z\\in X\\), такого, что \\(z\\cdot x=y\\) и \\(z\\cdot y=x\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(z\\cdot z=x\\) и \\(z\\cdot z=y\\) и \\(\n",
      "В данной работе рассмотрены методы реализации генератора запросов к иерархическим базам данных, а также описаны основные проблемы, связанные с их реализацией. В результате были предложены и реализованы алгоритмы реализации генератора запросов с регулируемыми оптимизациями.</s>\n",
      "Данная выпускная квалификационная работа посвящена исследованию идентификации пептидных природных соединений на основе масс-спектра. В работе рассматривается метод идентификации пептидных природных соединений, основанный на использовании ППС-графов, и его модификации. В работе также рассматривается задача идентификации пептидных природных соединений на основе масс-спектра. В работе рассматривается задача идентификации пептидных природных соединений на основе масс-спектра. В работе также рассматривается задача идентификации пептидных природных соединений на основе масс-спектра. В работе также рассматривается задача идентификации пептидных природных соединений на основе масс-спектра.</s>\n",
      "В выпускной квалификационной работе рассматривается задача фаззирования программного кода на языках C/C++ с использованием универсальной фаззинг платформы. В работе рассматриваются различные возможные реализации фаззинга, а также рассматриваются возможные способы оптимизации. Рассматривается возможность использования фаззинг платформы для программного обеспечения, реализованного на языках C/C++. В работе рассматривается возможность использования фаззинг платформы для программного обеспечения, реализованного на языках C/C++.</s>\n",
      "Данная работа посвящена изучению слабых решений параболического уравнения в частных производных с дрифтом, где под дрифтом подразумевается «скорость сноса» (то есть коэффициент при первой производной). Наиболее важными результатами данной работы являются:\n",
      "1. Теорема единственности в классе слабых решений и принцип максимума.\n",
      "2. Теорема аппроксимации.\n",
      "3. Слабая производная решения по времени.\n",
      "4. Невозрастание L1-нормы слабого решения по времени.</s>\n",
      "В данной работе описывается реализация алгоритма конгруэнтной замыкания для уравнений, выражающихся в выражениях с предикатами и константами.\n",
      "Реализация основана на методе сжатия уравнений.\n",
      "Описаны тесты, проведенные на примерах.</s>\n",
      "В данной работе рассматривается тропическая кривая на симплектическом многообразии. Затем определяется тропическая кривая на симплектическом многообразии, содержащем симплектическую тор. Затем определяется тропическая кривая на симплектическом многообразии, содержащем симплектическую тор, в котором есть нетривиальные торы.</s>\n",
      "Данная работа посвящена исследованию молекулярных структур в песочных паттернах. В работе рассматриваются структуры, которые можно увидеть в конусах, находящихся в границах множества Γ8, и описывается метод определения принадлежности конусов к множеству Γ8.</s>\n",
      "В данной выпускной квалификационной работе рассматривается проблема восстановления траектории мобильного устройства, а также предлагается решение этой задачи. Для решения этой задачи в работе рассмотрены существующие методы решения этой задачи, а также предложены новые методы, которые позволяют улучшить качество восстановленной траектории.</s>\n",
      "Данная работа посвящена поиску способов автоматического вывода инвариантов для символьной виртуальной машины KLEE. Для этого были проведены исследования по предметной области, а также рассмотрены и оценены различные подходы к автоматической генерации тестовых данных. В результате были выявлены недостатки предыдущих подходов, а также предложены новые методы для автоматического вывода инвариантов.</s>\n",
      "Данная работа посвящена исследованию коэффициентов дробных скручиваний Дена для кос, представляющих собой последовательности элементов из множества конечных целых чисел. В работе рассматривается группа кос, состоящая из всех кос, представляющих собой последовательности элементов из множества конечных целых чисел, и изучается влияние структуры группы кос на свойства коэффициентов дробных скручиваний Дена для кос. В частности, в работе показывается, что если группа кос не содержит косы, представляющих собой последовательности элементов из множества конечных целых чисел, то коэффициенты дробных скручиваний Дена для кос, представляющих собой последовательности элементов из множества конечных целых чисел, равны нулю.</s>\n",
      "В данной работе рассматривается задача определения чисел Кармайкла. Определяется понятие числа Кармайкла специального вида. Задача определения чисел Кармайкла специального вида решается при помощи метода распределения чисел Кармайкла и метода перестановки.</s>\n",
      "В работе рассматривается теория выпуклых точек, в частности, изучается явление межточечного пересечения в выпуклых точках. Получены новые оценки на максимальные и минимальные выпуклые точки с точностью до константы.</s>\n",
      "В работе рассматриваются задачи типа Варинга с простыми переменными. В частности, рассматриваются задачи, связанные с теоремой Миллера, теоремой Куммера, теоремой Крэмара и теоремой Крэмара-Ли.</s>\n",
      "В выпускной квалификационной работе рассматривается возможность создания программной системы на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений.\n",
      "В работе рассматривается встраивание функций, а также программная система LLVM, а также программные системы для встраивания функций, написанные на основе LLVM. Также рассматривается влияние встраивания функций на производительность приложений.\n",
      "Для разработки программной системы на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений были использованы методы программирования, описанные в работе.\n",
      "Для разработки программной системы на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений были использованы методы программирования, описанные в работе.\n",
      "В работе описана программная система на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений.\n",
      "В работе описана программная система на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений.\n",
      "В работе описана программная система на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений.</s>\n",
      "В данной работе рассматривается задача на решетке в области, заданной линейными неравенствами. Основным результатом работы является явная формула для числа путей на решетке, если ограничения на пути задаются линейными неравенствами.</s>\n",
      "Данная работа посвящена раскраскам графов. В ней мы рассматриваем раскраски, которые принимают только правильные конфигурации, а также раскраски, принимающие только динамические конфигурации.</s>\n",
      "Целью данной работы является разработка нового визуального интерфейса для настройки алгоритмов машинного обучения и настройки данных. В работе рассматриваются основные методы обучения и настройки алгоритмов машинного обучения, а также основные виды настройки данных. В результате проведенного анализа были выявлены проблемы, с которыми сталкиваются пользователи при настройке алгоритмов машинного обучения и настройке данных, а также был определен набор требований к новым интерфейсам. В работе также представлена концепция визуальной системы настройки алгоритмов машинного обучения и настройки данных, а также предложены возможные варианты реализации этой концепции.</s>\n",
      "Данная работа посвящена изучению метода условных нижних оценок (FPRAS) для задачи поиска путей с контекстно-свободными ограничениями. Предложенный алгоритм приводит к улучшению существующих методов и позволяет решить задачу на очень больших множествах возможных графиков.</s>\n",
      "В работе рассматривается задача о тензорном произведении представлений матроидов. На основе результатов, полученных для тензорного произведения представлений матроидов, получаются результаты для тензорного произведения представлений матроидов в случае, когда один из матроидов равномерный.</s>\n",
      "В данной работе рассматривается гипотеза МакКуэйга и Отта о стягиваемых множествах вершин в 3-связном графе. Построены алгоритмы, определяющие стягиваемые множества вершин в 3-связном графе с k вершинами и в 3-связном графе с k вершинами, допускающем циклы длиной не более 3.</s>\n",
      "Данная работа посвящена исследованию объёма ресурсов, используемых приложением при увеличении времени ответа от внешних источников данных. В работе рассматривается использование реактивного подхода к программированию, а также его влияние на объём ресурсов, используемых приложением. В результате исследования было обнаружено, что приложение, использующее реактивный подход к программированию, может использовать меньше ресурсов, чем приложение, использующее синхронный подход к программированию, при увеличении времени ответа от внешних источников данных.</s>\n",
      "В данной работе рассматривается задача построения прямой эмбеддинговой графики с планарным графиком на прямой. Практическое применение прямой эмбеддинговой графики состоит в использовании прямой эмбеддинговой графики в качестве отображения графа на прямую, что позволяет сэкономить время и ресурсы на решении задач, связанных с планарными графами.\n",
      "В работе рассматриваются различные подходы к решению задачи прямой эмбеддинговой графики, а также рассматривается класс планарных графов, для которых прямая эмбеддинговая графика может быть построена.</s>\n",
      "В данной работе рассматривается задача о суммах линейных образов выпуклых множеств в евклидовой и евклидовой плоскости. Данная задача имеет много приложений, например, в теории функций, теории множеств, теории вероятностей, теории дифференциальных уравнений.\n",
      "В работе дается оценка в евклидовой плоскости, приводится пример для выпуклых множеств в евклидовой плоскости, дается решение для выпуклых множеств в евклидовой плоскости. Также рассматривается задача о суммах линейных образов выпуклых множеств в евклидовой и евклидовой плоскости в евклидовой и евклидовой плоскости.\n",
      "Данная задача имеет много приложений, например, в теории функций, теории множеств, теории вероятностей, теории дифференциальных уравнений.\n",
      "В работе дается решение для выпуклых множеств в евклидовой плоскости.\n",
      "\n",
      "Для выпуклых множеств в евклидовой плоскости дается решение, основанное на методе максимального энтропий.\n",
      "Данная задача имеет много приложений, например, в теории функций, теории множеств, теории вероятностей, теории дифференциальных уравнений.\n",
      "В работе дается решение для выпуклых множеств в евклидовой плоскости.\n",
      "Данная задача имеет много приложений, например, в теории функций, теории множеств, теории вероятностей, теории дифференциальных уравнений.\n",
      "В работе дается решение для выпуклых множеств в евклидовой плоскости.</s>\n",
      "Данная работа посвящена исследованию возможности применения методов искусственного интеллекта к задачам проверки эквивалентности программ.\n",
      "В работе рассматривается задача проверки эквивалентности программ, реализуемых на языке Java, с помощью метода SAT. В качестве транслятора программ в схемы для языка Java используется Java-код.\n",
      "В работе приводится список 30 программ, представленных в виде текстовых файлов на языке Java. Для каждой программы на языке Java приводится соответствующая схема в формате CIRCUIT-SAT. В качестве реализации алгоритма для задачи проверки эквивалентности программ используется программное обеспечение Sat4j.\n",
      "В работе приводится описание метода трансляции программ на языке Java в схемы для языка CIRCUIT-SAT, а также результаты проверки эквивалентности программ.</s>\n",
      "В данной работе изучается вопрос, какая длина минимального принимаемого объекта имеет автомат, который принимает заданные им строки. В частности, в работе изучается длина минимального принимаемого объекта для двухстороннего конечного автомата, который запоминает направление последнего шага, и для двухстороннего конечного автомата, который запоминает направление последнего шага и запоминает строки.</s>\n",
      "В данной работе рассматриваются возможности применения статистических тестов для оценки эффективности улучшений мобильных игр. По результатам исследования определяется, какие из тестов наиболее применимы для мобильных игр и какие улучшения могут быть оценены с помощью данных, полученных при их применении.</s>\n",
      "В данной работе рассматриваются методы идентификации малых молекул по масс-спектрам. Описаны основные методы идентификации малых молекул, а также рассмотрены их достоинства и недостатки.</s>\n",
      "В данной работе рассматривается задача распознавания лицевых выражений человека. В качестве основной методики для решения этой задачи используется нейронная сеть с подпрограммой решения проблем машинного обучения. Также для улучшения результатов использовалась смешанная нейронная сеть. В качестве датасетов для обучения и тестирования использовались лицевые изображения из работ: [1], [2], [3], [4], [5], [6]. Результаты тестирования показали, что реализованная система имеет средний точность 87,5 % и точность 92,3 % для данных с лицевыми изображениями.</s>\n",
      "Данная работа посвящена разработке нового подхода к задаче трекинга лица, основанного на использовании деформируемой 3D-модели головы и метода Bundle Adjustment. Разработанный алгоритм позволяет производить трекинг лица с высокой точностью, а также изменяет лицо в реальном времени.</s>\n",
      "Данная работа посвящена изучению невырожденных раскрасок графов. В частности, мы доказываем, что в любом графе нет невырожденных раскрасок, в которых нет двух вершин с одинаковым числом красок.</s>\n",
      "В работе рассматривается спектральная теория функций с лакунами в прямой. Определяется понятие функции с лакунами, а также предлагается и реализуется алгоритм построения спектра функции с лакунами. Приведены примеры функций с лакунами и их спектра.</s>\n",
      "Данная работа посвящена разработке бекэнда для мобильного приложения для создания персонажа. В ней рассматриваются способы реализации системы способностей для создания персонажа, а также основные принципы и способы реализации системы способностей.</s>\n",
      "Данная работа посвящена построению упрощенной динамической модели для задачи оптимального управления микроклиматом помещения. Для решения данной задачи была построена динамическая модель куба. В работе приведены результаты расчета оптимального управления микроклиматом куба с помощью метода Монте-Карло.</s>\n",
      "В работе рассматривается проблема определения категории когомологий высшего порядка для категории строк с матрицей b. Определяется когомологическая категория строк с матрицей b, а также её категория когомологий высшего порядка. Рассматривается вопрос о возможности определения категории строк с матрицей b, если известна категория строк с матрицей b.</s>\n",
      "В работе рассматривается построение бесконечной серии трехмерных гиперболических многообразий, сложность которых известна. Для этого решается задача гомологической минимизации на классе гомологически минимальных триангуляций. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В работе рассматривается задача гомологической минимизации на классе гомологически минимальных триангуляций.\n",
      "В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры.\n",
      "В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры.\n",
      "В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры.\n",
      "В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры.\n",
      "В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гипер\n",
      "В данной работе рассматривается проблема больших и сверхбольших уклонений в Телеком-процессах, описывающих связные и несвязные системы трафика. Для рассмотрения уклонений в Телеком-процессах используется метод моделирования на основе интегральных представлений, предложенный И. Кая и М. Такку в работе [9]. В работе [16] описывается модель связной системы трафика, состоящей из n узлов, и приводится формула для среднего уклонения в этом случае. В работе [15] рассматривается модель несвязной системы трафика, состоящей из n узлов, и приводится формула для среднего уклонения в этом случае. В данной работе рассматривается модель несвязной системы трафика, состоящей из n узлов, и приводится формула для среднего уклонения в этом случае.</s>\n",
      "В работе рассматриваются оценки констант коэрцитивности дифференциальных операторов в пространствах L_p-норм. В частности, получены достаточные условия максимальности и координатной выпуклости для оценки констант коэрцитивности.</s>\n",
      "В работе рассматривается задача вывода комбинаторных принципов из OBDD, представляющих дизъюнкты КНФ формулы. Обозначим функцию, которая вводит в OBDD представляющие дизъюнкты КНФ формулы, и функцию, которая вводит в OBDD представляющие дизъюнкты КНФ формулы, и функцию, которая вводит в OBDD представляющие дизъюнкты КНФ формул.\n",
      "Определим систему доказательств, основанную на OBDD, как множество OBDD, таких что для любого OBDD из множества существует OBDD из множества, представляющий дизъюнкты функции, и для любого OBDD из множества существует OBDD из множества, представляющий дизъюнкты функции.\n",
      "Отметим, что, если OBDD представляющие дизъюнкты КНФ формул не содержат OBDD, представляющих дизъюнкты функции, то функция невыполнимо.\n",
      "Отметим, что, если OBDD представляющие дизъюнкты КНФ формул содержат OBDD, представляющих дизъюнкты функции, то функция невыполнимо.\n",
      "Таким образом, вывод комбинаторных принципов из OBDD представляющих дизъюнкты КНФ формул можно рассматривать как задачу поиска OBDD, представляющих дизъюнкты функции, которые не содержат OBDD, представляющих дизъюнкты КНФ формул.\n",
      "Отметим, что, если функция невыполнимо, то OBDD, представляющие дизъюнкты КНФ формул, и OBDD, представляющие дизъюнкты функции, являются эквивалентными.\n",
      "Таким образом, для вывода комбинаторных принципов из OBDD представляющих дизъюнкты КНФ формул можно использовать метод поиска эквивалентной OBDD.\n",
      "В работе изучается эффективность метода поиска эквивалентной OBDD для вывода комбинаторных принципов из OBDD представляющих дизъюнкты КНФ формул.</s>\n",
      "В данной работе рассматривается задача поиска оптимального ответа на специализированные вопросы, такие как темы на форумах. В качестве примера для анализа приводится StackOverflow. Представлены архитектуры трансформеров, которые используются в реализации StackOverflow Translator. Описана архитектура трансформера, который используется в StackOverflow Translator.</s>\n",
      "В данной работе рассматривается задача о нахождении множеств мономов, допускающих поверхность с особенностью данного порядка. \n",
      "Для решения данной задачи используется метод матроидов.\n",
      "В работе рассматривается геометрическая интерпретация множества мономов, допускающих поверхность с особенностью данного порядка.\n",
      "Для решения данной задачи используется метод матроидов.</s>\n",
      "Целью данной работы является создание мобильного приложения, которое позволяет пользователям создавать персонажей для игры «Комиксы». На примере данного проекта мы хотели показать, что можно использовать модели, обученные на компьютерных играх, для создания персонажей для игр на основе комиксов.\n",
      "\n",
      "В работе представлены результаты реализации мобильного приложения и его тестирования. В частности, были проведены исследования на основе выборки из 1500 комиксов, которые содержат персонажей для игры. В результате было выявлено, что модели, обученные на компьютерных играх, не смогли создать персонажей, которые соответствовали бы комиксу. Также было установлено, что наиболее удачным методом для создания комиксов является использование байесовской классификации.\n",
      "\n",
      "На основе полученных результатов было предложено несколько вариантов для дальнейшего развития проекта. В частности, было предложено использовать более подходящие данные для обучения моделей, а также использовать другие методы для создания комиксов.</s>\n",
      "В данной работе рассматривается задача о разложении функций на произвольные интервалы. Проанализированы существующие методы решения задачи и представлена новая методика решения.</s>\n",
      "В данной работе рассматривается задача автоматического создания модульных тестов для обнаружения уязвимостей в исходных кодах приложений. Разрабатывается и реализуется программная система автоматического создания модульных тестов для обнаружения уязвимостей в исходных кодах приложений. В систему входят три компонента: модуль автоматического создания тестов для обнаружения уязвимостей в исходных кодах приложений, модуль символьного исполнения и модуль SMT-решателя. В систему также входит программа для автоматического создания тестов для обнаружения уязвимостей в исходных кодах приложений, которая использует инструменты анализа исходного кода, а также программа для анализа исходного кода, которая использует инструменты символьного исполнения. Система автоматического создания модульных тестов для обнаружения уязвимостей в исходных кодах приложений реализована на языке Python и является свободно распространяемым открытым ПО.</s>\n",
      "В работе рассматривается проблема универсальной символьной виртуальной машины. В ходе исследования была выявлена проблема недостаточной универсальности существующих реализаций. Была предложена концепция универсальной символьной виртуальной машины, реализованная в виде программного пакета.</s>\n",
      "В данной работе рассматривается задача выделения сочинительных связей из текста на основе нейросетевых методов. В работе описаны основные подходы к нейронным сетам и описана модель для решения задачи, на основе которой проведены эксперименты с целью определения эффективности данной модели.</s>\n",
      "Целью данной работы является исследование факторов, влияющих на удержание курьеров, на примере сервиса доставки. В ходе исследования были описаны и проанализированы три подхода к моделированию: эвристическая модель, эмпирическая функция распределения и сравнение подходов. В результате работы была выделена эмпирическая функция распределения, которая наилучшим образом описывает данные.</s>\n",
      "В работе рассматривается асимптотическое поведение случайного блуждания на полупространстве с ограничением на границе. В частности, рассматривается вопрос о том, при каких условиях система имеет стационарное распределение.</s>\n",
      "В работе рассматривается асимптотическое поведение ВСБ на полупространстве с поглощением на границе.\n",
      "В частности, получаются оценки размерности, устойчивости и существования стационарной точки для случайных блужданий на полупространстве с поглощением на границе.</s>\n",
      "В работе рассматривается задача определения числа совершенных паросочетаний в графе. Приводится верхняя оценка для задачи 0-1 перманент.</s>\n",
      "В данной работе рассматривается алгоритмическая сложность игры Merge game на двумерной двумерной игровой карте. Полученная версия игры Merge game может быть использована при разработке игр на 2D-карте.</s>\n",
      "Динамическое символьное исполнение (DSL) - это способ программирования, который позволяет программистам запускать программы без необходимости предоставить исходный код. В данной работе представлены результаты разработки DSL для языков программирования Java и Kotlin.</s>\n",
      "В работе рассматривается задача об идеалах алгебры H^\\infty. В качестве метрики на пространстве идеалов алгебры H^\\infty рассматривается метрика на пространстве пространства идеалов алгебры H^\\infty, определенная в работе С. В. Кислякова и С. В. Руцкого. В работе доказываются теоремы о неподвижной точке для этой метрики.</s>\n",
      "Настоящая работа посвящена разработке алгоритмов валидации вариантных пептидов. В работе рассматриваются алгоритмы валидации, написанные на основе нейронных сетей, а также алгоритмы, написанные на основе программного обеспечения, предоставленного другими исследователями. Разработанные алгоритмы валидации были протестированы на тестах, содержащих известные вариантные пептиды, и на тестах, содержащих новые вариантные пептиды. В работе рассматривается программное обеспечение, предоставленное другими исследователями, и алгоритм валидации, написанный на основе этого программного обеспечения. Разработанные алгоритмы валидации были протестированы на тестах, содержащих известные вариантные пептиды, и на тестах, содержащих новые вариантные пептиды.</s>\n",
      "Работа посвящена исследованию частных случаев роста в песочных моделях. В работе рассматриваются 3 частных случая роста: функция числа обвалов, солитоны и существование паттерна. В работе представлены методы доказательства роста, а также теоремы, которые позволяют доказать рост.</s>\n",
      "Данная работа посвящена разработке статического анализатора на основе символьного исполнения. Рассматривается задача обнаружения ошибок в программах на языке Java. В качестве решения для данной задачи предлагается реализация статического анализатора на основе символьного исполнения. В работе рассматривается алгоритм статического анализатора на основе символьного исполнения. В работе также рассматривается алгоритм статического анализатора на основе символьного исполнения. В работе также рассматривается алгоритм статического анализатора на основе символьного исполнения. В работе также рассматривается алгоритм статического анализатора на основе символьного исполнения. В работе также рассматривается алгоритм статического анализатора на основе символьного исполнения.</s>\n",
      "В данной выпускной квалификационной работе рассматривается вопрос автоматической генерации модульных тестов на языке Python. В качестве средств автоматизации выбраны языки Python и Pytest. В качестве тестовых данных используются выводы, полученные из исходного кода на языке Python. В качестве тестовых данных используются выводы, полученные из исходного кода на языке Python. В качестве тестовых данных используются выводы, полученные из исходного кода на языке Python. В качестве тестовых данных используются выводы, полученные из исходного кода на языке Python.</s>\n",
      "В данной работе мы рассматриваем задачу распознавания сущностей на графе знаний об организациях. В качестве входных данных используется график знаний об организациях, полученный с помощью приложения YAGO. В качестве выходных данных используется описание сущностей, полученное с помощью приложения DBpedia.\n",
      "Для улучшения качества выходных данных используется процесс обогащения сущностей атрибутами.\n",
      "Для решения данной задачи мы применяем методы машинного обучения, такие как:\n",
      "\n",
      "- ML-эволюция,\n",
      "- графовые эмбеддинги,\n",
      "- байесовское моделирование,\n",
      "- социальные сети,\n",
      "- поисковая оптимизация,\n",
      "- модели с динамическими входными данными,\n",
      "- модели с динамическими выходными данными.\n",
      "\n",
      "Данная работа позволяет улучшить качество выходных данных и, соответственно, качество результатов.</s>\n",
      "В данной работе рассматриваются точки случайного распределения на границе выпуклого тела. Рассматривается задача о средней площади точек, принадлежащих выпуклому телу. В ходе работы получены следующие результаты: \n",
      "1. Проведено исследование существования точек случайного распределения на границе выпуклого тела.\n",
      "2. Получено доказательство непрерывности в метрике Хаусдорфа выпуклого тела.\n",
      "3. Получена формула средней площади точек, принадлежащих выпуклому телу.</s>\n",
      "В данной работе рассматривается глубокое обучение с подкреплением в высокочастотном трейдинге. \n",
      "Выполняется реализация алгоритмов TRPO и PPO, а также их модификаций, которые могут быть использованы в реальном трейдинге.</s>\n",
      "В работе рассматриваются некоторые гипотезы о морфических словах. В частности, предлагается обобщение известной теоремы о морфизмах, порождающих слова Линдона, и исследуются некоторые другие вопросы.</s>\n",
      "Данная работа посвящена поиску и классификации фрагментных ионов в белковых последовательностях. В работе рассматривается ряд различных подходов к проблеме, включая метод деконволюции спектров, метод максимального правдоподобия и метод сжатия ионов. Было проведено исследование, включающее в себя сравнение результатов различных алгоритмов с использованием 1000 белковых последовательностей, которые были выбраны на основе их профилей в различных базах данных.</s>\n",
      "Автор производит подсчёт асимптотики числа триангуляций проективной плоскости, дефектов которых равны 1, 1 и 4. Для этого рассматривается поднятие триангуляции на сферу. Также рассматривается развёртка октаэдра на плоскость, что позволяет упростить задачу.</s>\n",
      "Работа посвящена разработке программного средства для автоматической изоляции LLVM кода. В работе рассматривается задача символьного исполнения, а также разработан алгоритм для автоматической изоляции LLVM кода.</s>\n",
      "Работа посвящена разработке архитектуры и созданию мобильного приложения для создания игрового персонажа. Целью работы является разработка мобильного приложения для создания игрового персонажа.\n",
      "Приложение создано на основе фреймворка React Native и использует сервисы API для работы с базой данных. В работе приведены решения, которые были найдены в процессе решения задачи, а также рассмотрены вопросы, которые возникли во время работы.</s>\n"
     ]
    }
   ],
   "source": [
    "print(*df[\"learnt\"].tolist(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В работе рассматриваются алгебраические теории над конечнопорожденными свободными T -алгебрами. Определяется структура монад на T -объектах, которые оказываются финитарными. Предлагается определение алгебраической теории в терминах монад.</s>\n",
      "В\n",
      "In this work we propose a method for improving the performance of SAT solvers by embedding a graph neural network into a state-of-the-art solver MiniSat. The network predicts the next variable to branch on during each step based on the current state of the problem. The proposed method is compared with the original Graph-Q-SAT and other solvers on a set of SAT instances from the Uniform Random-3-SAT distribution and achieves better results in terms of wall-clock time and the number of branching decisions required to solve an instance.</s>\n",
      "В работе мы обобщаем результаты об энергии натянутых струн, сопровождающих Винеровский процесс, на случай многомерного Винеровского процесса.</s>\n",
      "В работе рассматривается применение добавления признаков композиций в функцию потерь модели MusicTransformer для настраиваемой генерации музыки. Помимо MusicTransformer, используется и базовая модель Transformer. Проверяется модификация оригинальной модели, а также предлагается модификация модели Transformer, которая позволяет генерировать музыку в формате MIDI.</s>\n",
      "В данной работе рассматривается проблема существования универсальной системы для класса всех нулевых-энтропийных систем на группах с амплитудой.</s>\n",
      "В данной работе предлагается генератор языка запросов для поиска в иерархических структурах. Предлагается схема генератора и код для парсера и исполнителя. Оптимизации запросов приводятся в виде алгоритмов.</s>\n",
      "В работе рассмотрена задача идентификации масс-спектров пептидных природных соединений с учётом двух возможных модификаций. При этом одна модификация предполагается распространённой (из небольшого списка конкретных высокочастотных модификаций), а вторая – произвольной, как это было изначально в алгоритме VarQuest.</s>\n",
      "В работе рассматривается фаззинг программ на языках C/C++. Разработана платформа для фаззинга, которая может быть использована для фаззинга программ на любом языке.</s>\n",
      "В данной работе изучается слабое решение параболического уравнения в частных производных с дрифтом, где под дрифтом подразумевается «скорость сноса» (то есть коэффициент при первых частных производных по пространственным переменными). Особенный интерес к изучению таких уравнений возникает в гидродинамике, где в качестве дрифта выступает само поле скоростей жидкости. Исторический обзор, а так же всеобщее изложение классических результатов, касающихся однозначной разрешимости задач в энергетическом классе, содержатся в монографии [7]. Большинство этих результатов получено в предположении о достаточной «гладкости» дрифта, например, его существенной ограниченности (в нашем случае существенно ограниченный дрифт будет называться регулярным). В случае сингулярного (то есть нерегулярного) дрифта Теорема единственности\n",
      "В данной работе описывается реализация алгоритма для ключевой задачи в теории неиспользуемых функций (QF UF), где решение проблемы равенства состоит в определении, какие выражения равны по отношению к неиспользуемым функциям. Реализация основывается на алгоритме, описанном в [Kap19], который определяет ключевые элементы для равенства по отношению к неиспользуемым функциям. В реализации применяется метод сжатия, который позволяет избежать использования SAT-соглашений при проверке равенства.</s>\n",
      "В данной работе рассматривается потенциал для симплектических тороических многообразий.</s>\n",
      "В данной работе рассматривается регулярный граф G8 = (Z2 , E), вершины (x1 , x2 ), (y1 , y2 ) в нём соединены ребром тогда и только тогда, когда |x1 − y1 | + |x2 − y2 | = 1. В работе рассматривается область, замощённая повторяющимися паттернами, для каждой области – свой паттерн (детали ниже). Каждой области соответствует вершина конуса в Рис. 5 и паттерны для некоторых вершин указаны в Таблице 6.</s>\n",
      "В данной работе описано решение проблемы восстановления траектории смартфона в пространстве, используя данные инерциальных сенсоров. В результате работы был создан мобильный приложение для реконструкции 3D сцены, которое позволяет оценить качество снимков и предложить пользователю рекомендации по повторному съемке объекта.</s>\n",
      "Данная работа посвящена улучшению анализа кода символьной виртуальной машине KLEE путём внедрения вывода индуктивных инвариантов при помощи стратегии двунаправленного исполнения. В рамках данной работы были разработаны и реализованы алгоритмы вывода индуктивных инвариантов, которые позволяют KLEE улучшить анализ кода.</s>\n",
      "В работе представлен алгоритм вычисления коэффициентов дробных скручиваний Дена для косов.</s>\n",
      "Данная работа посвящена исследованию множества K, которое определяется как K = {k : существует n, такое, что N = 2n k + 1 – число Кармайкла}. В [8] показано, что K имеет нулевую асимптотическую плотность, то есть, K ∩ [1; n] = 0. n→∞\n",
      "n\n",
      "lim\n",
      "В то же время, определить принадлежность заданного составного числа k множеству K практически невозможно, за исключением тех случаев, когда число Кармайкла N = 2n k + 1 найдено явно. В [5] показано, что 9, 15, 21 и 25 не принадлежат K.\n",
      "В этой работе построена армфметическая прогрессия, члены которой, являющиеся утроенными простыми числами, не являются элементами K. Прогрессия построена таким образом, что она содержит бесконечно много подходящих чисел, поэтому главный результат работы позволяет явно привести пример сколь угодно большого числа k, не входящего в K.</s>\n",
      "В данной работе рассматриваются выражения для вложенных сфер в n-мерных ортогональных векторах и для их смежных сфер.</s>\n",
      "В\n",
      "В данной работе представлена программная система на основе LLVM, которая позволяет оптимизировать задержку в высоконагруженных секциях кода C++ приложений путем регулируемого агрессивного встраивания функций.</s>\n",
      "Данная работа посвящена рассматриванию путей на решетке Zd , в которых каждый шаг соответствует увеличению одной из координат на 1. Количество путей между двумя вершинами равно, как известно, мультиномиальному коэффициенту P(bi −ai ) \u0001 b1 −a1 ,...,bd −ad . В случае, когда начало пути совпадает с началом координат, речь об обычных таблицах Юнга, количество которых считается по формуле крюков. Теперь пути между двумя вершинами можно отождествить с косыми таблицами Юнга. В случае, когда первая координата всегда не меньше двух других, мы рассматриваем такие пути, в которых в любой точке пути (x1 , x2 , x3 ) выполняется x1 ≥ x2 . Если рассматривать такие пути, то получаем формулу для Ca1 ,a2 ,...,ak . Далее, рассматриваем такие пути, в которых в каждой точке пути (x1 , x2 , x3 ) выполняется x1 ≥ x2 + x3 .</s>\n",
      "В работе рассматриваются динамические раскраски графа, описанные в [1], [2], [3], [4], [5], [6], [7]. Рассматриваются различные способы улучшения этих оценок.</s>\n",
      "В данной работе представлен программный продукт Learn2Learn, позволяющий создавать и настраивать модели машинного обучения с помощью графического интерфейса интуитивно понятного пользователям, а также исходный код и пояснения к нему. Мы продемонстрируем методы, позволяющие интегрировать программирование в нашу систему, благодаря чему она не потеряет в гибкости по сравнению с традиционными способами настройки нейросетей, а также создадим сложную нейросеть, решающую задачу обнаружения объектов на изображении, не написав при этом ни единой строчки кода.</s>\n",
      "Цель работы — изучение границы сложности проблемы CFL reachability с контекстно-свободными ограничениями. В данной работе обсуждаются основные алгоритмы и существующие проблемы, даются новые оценки сложности, а также предлагаются возможные пути к улучшению существующих алгоритмов.</s>\n",
      "Цель дипломной работы — исследовать разные подходы к понятию тензорного произведения матроидов. Рассмотрены два важных подхода к построению матроидов: представление матроида и максимальный матроид.</s>\n",
      "В работе доказывается, что гипотеза о стягиваемых множествах вершин в 3-связном графе является верной для любого k ⩾ 5.</s>\n",
      "В работе рассматривается эффективность и производительность приложений, работающих с базами данных.</s>\n",
      "В данной работе рассматривается задача поиска upward straight-line embedding для планарных динамических графов. В качестве набора вершин используются точки в двухмерной плоскости. Сначала определяется, какие классы планарных динамических графов имеют upward straight-line embedding, а также, какие классы планарных динамических графов не имеют upward straight-line embedding. Затем определяется, какие классы планарных динамических графов имеют upward straight-line embedding в точном соответствии с их би-нетривиальными свойствами.</s>\n",
      "В данной работе продолжается изучение экстремальных задач вида\n",
      "|\n",
      "\n",
      "N\n",
      "X\n",
      "\n",
      "Ti A| → min\n",
      "\n",
      "i=1\n",
      "\n",
      "для компактов A единчиной меры и фиксированных линейных операторов T1 , . . . , TN .</s>\n",
      "В данной работе рассматривается возможность трансляции программ для виртуальной машины Java в булевые схемы и задача их проверки эквивалентности. Построена система трансляции байткода виртуальной машины Java в булевые схемы, а также описаны основные особенности её работы. Используется программный комплекс transbyte для кодирования программ в булевые схемы. Рассматривается задача распознавания семантических клонов с помощью булевых схем и SAT-решателя.</s>\n",
      "В данной работе рассматриваются вопросы о максимальном размере принимаемой объектом автоматов. В частности, определяется, как велик может быть максимальный размер принимаемой строки для двухстороннего конечного автомата, который запоминает направление последнего шага. Этот результат доказывается точностью. Доказано, что максимальное число вершин в минимальном принимаемом дереве для недетерминированных древесных автоматов определяется точно. Для древоходных автоматов показывается, что максимальный размер минимального принимаемого дерева — двойной экспоненциальный от числа состояний. Кроме того, доказывается разрешимость задачи пустоты для двух видов автоматов на графах. Задача непустоты для графоходных автоматов, которые ходят по рёбрам графа, оказывается NEXP-полной, а для замощений графов подграфами-звёздами, оказывается NP-полной.</s>\n",
      "В данной работе представлены результаты по следующим задачам:\n",
      "1. Проверка применимости статистических тестов.\n",
      "2. А/B тест без непосредственного проведения А/B теста.\n",
      "3. Влияние распределения улучшения от фичи на A/B тесты.\n",
      "\n",
      "Выводы: Без выкидывания китов – видно, что отвергаем гипотезу чаще, чем нужно, для платежей больших дней, для платежа первого дня – чаше принимаем.\n",
      "При выкидывании китов – видно, что при более крупных разбиениях мы (при уровне значимости 0.05 обычно) гипотезу чаще принимаем, чем нужно, а при более малых – отвергаем больше, чем нужно.\n",
      "Далее были проведены аналогичные исследования, но со скорректированным тестом Стьюдента:\n",
      "• для неравных дисперсий тест Стьюдента с коррекцией Уэлча, устанавливаем параметр equal_var=False. Тогда p-value вычисляется так:\n",
      "p = ttest_ind(group_a, group_b, equal_var=False)[1]\n",
      "• для несимметричного распределения тест Стьюдента с коррекцией Саттертуэйта, устанавливаем параметр nan_policy=’omit’. Тогда p-value вычисляется так:\n",
      "p = ttest_ind(group_a, group_b, nan_policy=’omit’)[1]\n",
      "В целом, как и предполагалось, данные корректировки не улучшили ситуацию:\n",
      "Рис. 1. Платежи за 30 дней с момента установки\n",
      "\n",
      "Коррекция Уэлча\n",
      "\n",
      "Коррекция Саттертуэйта\n",
      "9\n",
      "\n",
      "Вероятно, можно продолжить исследование, проведя аналогичные исследования для разбивки на группы по модулю 4 и 8.</s>\n",
      "В работе описывается программа NPD-Quast, которая выполняет сравнение работы дерепликаторов. Программа может быть использована как инструмент для сравнения работы дерепликаторов, так и для создания базы данных для использования в других программах.</s>\n",
      "Целью данной работы является разработка системы распознавания лицевых выражений человека и сгенерированных лиц с помощью методов машинного обучения.  Кроме того, в работе представлены методы детектирования аномалий в изображениях лиц и их аватарах.</s>\n",
      "В данной работе рассматривается задача трекинга лица. В качестве входных данных требуется видео, на котором находится голова человека, трёхмерная модель головы и параметры деформации. Целью работы является улучшение алгоритма трекинга, для чего рассматриваются два подхода: добавление в него 2D ландмарок и использование Bundle Adjustment. Ландмарки — определенные точки на лице, определяемые нейросетью — призваны уменьшить возникающую в процессе работы алгоритма ошибку сползания, а Bundle Adjustment — метод уточнения решения благодаря одновременной оптимизации всех кадров — эксплуатирует ту особенность рассматриваемой задачи, что все кадры известны до начала трекинга.</s>\n",
      "В работе доказывается, что если граф не содержит Kd+1 , то он имеет невырожденную раскраску с не более чем d18v − 10 dv ln d вершин.</s>\n",
      "nan\n",
      "В данной работе представлена модель персонажа для настольной ролевой игры DnD.</s>\n",
      "Данное исследование является частью проекта по созданию централизованной системы управления потреблением энергии в зданиях общего назначения, тепловой и электрической. Одной из уникальных особенностей данной системы предполагается разработка и последующее применение интеллектуальных методов предсказания энергопотребления. Для предсказания энергопотребления надо понимать и уметь измерять те факторы, которые определяют необходимое количество электричества и тепла для здания в этот момент и в ближайшие несколько часов.\n",
      "При постановке задачи регулирования для систем отопления, кондиционирования и вентиляции (ОВК), эксплуатирующие службы руководствуются двумя конкурирующими критериями: экономичностью работы системы и комфортом внутренней среды. По различным оценкам, от 50 до 70 процентов всей расходуемой энергии приходится на ОВК. Таким образом, оптимизация потребления этого класса устройств, пусть даже на 5-10 процентов,\n",
      "повлечет за собой ощутимое снижение общего уровня расхода энергии.\n",
      "В основе данной работы лежит идея скомбинированного подхода к моделированию микроклимата помещения: методы вычислительной термодинамики [1](CFD) и методы сетевых воздушных потоков (NAF). Точное решение задачи CFD на небольшом временном промежутке позволит смоделировать работу измерительного комплекса, оптимизировать вектор измеряемых величин, количество датчиков и их расположение в помещении.\n",
      "В стандартных офисных зданиях число отдельных зон может доходить до нескольких сотен, поэтому расстановка датчиков и считывание показаний очень трудоемкий процесс.\n",
      "В то же время, используя сгенерированную модель, мы можем определить значения на датчиках одновременно для всех интересующих расположений.\n",
      "Для сокращения времени обследования здания, ускорения процесса\n",
      "расвертывания сети, было предложено выделить в проекте два этапа - численное моделирование исследуемого помещения и анализ полученных данных.\n",
      "На первом этапе будет смоделирован \"Демонстрационный стенд Умного дома\". Геометрия помещения задана в формате CAD. Моделирование будет производиться в COMSOL Multyphysics [2]. Целью является моделирование некоторого промежутка времени с хорошей точностью. Будет смоделирован процесс теплообмена внутри помещения и с окружающим воздухом, а также солнечная радиация. Значения внешней температуры будут взяты из собранного ранее датасета. Полученная модель будет использована далее.\n",
      "На втором этапе будут выбраны точки возможного расположения датчика. Температуры в них будут импортированы в таблицу. Дальнейшая работа с полученными таблицами будет реализована на языке Python. Проанализировав потенциальные места расположения датчика методом, который предлагают в своей статье Пащенко А.Ф., Рассадин Ю.М. [3] (подробней об этом методе в главе 2.2), сравнительным анализом найдем наилучшую точку размещения микроклиматического датчика. Для небольших помещений достаточно перебора потенциальных точек размещения, для больших помещений со сложной конфигурацией возможно применение градиентного метода поиска.</s>\n",
      "\n",
      "\n",
      "В данной работе рассматриваются исключительно компактные 3-многообразия с непустым краем. Пусть ∆ обозначает стандартный тетраэдр. Идеальной триангуляцией компактного 3-многообразия M с непустым краем называется реализация внутренности M в виде склейки конечного числа копий ∆ с удалёнными вершинами по аффинным гомеоморфизмам их граней. Идеальная триангуляция многообразия M называется минимальной, если она содержит наименьшее число тетраэдров среди всех идеальных триангуляций данного многообразия. Число тетраэдров в минимальной идеальной триангуляции многообразия M называется триангуляционной сложностью и обозначается через c∆ (M ). Триангуляционная сложность, как и многие другие инварианты аналогичного типа, довольно трудно вычислять. В перву- ой очередь точные значения триангуляционной сложности известны для многообразий, табулированных при помощи компьютера. Полная таблица ориентируемых гиперболических многообразий с каспами до сложности 9 включительно, описанная в [1], содержит 162 182 минимальных идеальных триангуляций для 61 911 многообразий. Все эти многообразия вместе с триангуляциями включены в компьютерные программы SnapPy [2] и Regina [3]. В [4] перечисляются все гиперболические многообразия с каспами, получающиеся склейкой правильных гиперболических идеальных тетраэдров, до сложности 25 включительно. М. Фуджи в [5] показал, что имеется лишь 8 различных ориентируемых гиперболических 3-многообразий с вполне геодезическим краем сложности 2. В последствии Р. Фриджерио, Б. Мартелли и К. Петронио классифицировали в [6] все компактные ориентируемые гиперболические 3-многообразия конечного объема с непустым вполне геодезическим краем до сложности 4 включительно. На данный момент известны лишь несколько бесконечных серий компактных связных 3-многообразий с краем, для которых удалось установить точное значение триангуляционной сложности. Первая бесконечная серия была описана Р. Фриджерио, Б. Мартелли и К. Петронио в работе [7]. Многообразия этой серии обладают идеальными триангуляциями с единственным ребром. Вопрос минимальности идеальных триангуляций, обладающих ровно двумя рёбрами был исследован А. Ю. Весниным,\n",
      "В данной работе рассматриваются свойства пуассоновского Телеком-процесса, его предельные теоремы и вероятности больших уклонений.</s>\n",
      "В данной работе изучается задача о порядке величины rpn при p → 1.</s>\n",
      "В нашей работе мы рассматриваем пропозициональные системы доказательств, основанные на OBDD(∧), OBDD(∧, reordering) и их древовидные версии. В нашей работе мы показали, что OBDD(∧), OBDD(∧, reordering), а также их древовидные версии не балансируемые.</s>\n",
      "Данная работа посвящена исследованию потенциала имеющихся языковых моделей в задаче поиска ответа на вопросы, заданные на платформе Stack Overflow. В рамках исследования будет предложен новый метод дообучения таких моделей с учетом специфики данной задачи. После этого будут проведены эксперименты, направленные на подтверждение эффективности предложенного метода.</s>\n",
      "В работе рассматривается задача о том, какие множества A допускают особенность порядка m. Для этого рассматривается однородная линейная система на допускаемость множества A. Мы рассматриваем всевозможные многочлены с вещественными коэффициентами αijk , состоящие только из мономов A:\n",
      "X\n",
      "P (x, y, z) =\n",
      "αijk xi y j z k .\n",
      "xi y j z k ∈A\n",
      "\n",
      "Данная система может быть представлена в виде матричной системы. В случае, если A допускает особенность порядка m, матрица этой системы вырождена.\n",
      "Определение 1. Мы называем A множеством, допускающим особенность порядка m, если существует нетривиальный набор коэффициентов αijk , такой что поверхность C, заданная уравнением P (x, y, z) = 0, имеет в некоторой точке особенность порядка m.\n",
      "\n",
      "Данное определение позволяет рассматривать всевозможные многочлены с вещественными коэффициентами. В частности, мы рассматриваем многочлены, состоящие только из мономов A.\n",
      "\n",
      "Предложение 1. В системе (1) Ĉ4m = Cm+3\n",
      "уравнений, где через Ĉnk = Ck+n−1\n",
      "обозначается количество сочетаний с повторениями из n по k.\n",
      "\n",
      "Доказательство. Ясно, что количество урав\n",
      "В данной работе представлен мобильный приложение, которое помогает игрокам D&D упростить процесс создания, хранения и управления персонажей. Наш проект включает в себя модели машинного обучения, способные автоматически генерировать уникальные предыстории и визуальные представления персонажей.</s>\n",
      "Данная работа посвящена разработке новых методов для автоматического мониторинга производительности. В работе рассмотрены методы для поиска интервалов изменений производительности в серии измерений. Для этого были разработаны алгоритмы поиска двоичной сегментации, скользящего окна и объединения отрезков снизу вверх. Для выбора метрики для использования в алгоритмах была проведена оценка качества метрик на искусственных и реальных данных.</s>\n",
      "Анализ программного обеспечения — это сложная задача, требующая много времени и нередко не дающая ожидаемого результата. В работе рассматривается возможность автоматизировать эту процедуру, используя символьное исполнение и модульные тесты.</s>\n",
      "В данной работе рассматривается проблема создания универсальной символьной виртуальной машины. Для решения этой проблемы были выделены основные задачи: анализ существующих символьных машин и различных техник в символьном исполнении, проектирование и разработка основных компонентов универсальной символьной виртуальной машины, подходящей для анализа языков с автоматическим управлением памятью.</s>\n",
      "В данной работе описывается новый подход к выделению сочинительных связей. Цель работы заключается в том, чтобы построить модель, которая позволит выделить сочинительные связи в предложениях на русском языке. Для этого был использован метод, основанный на модели, учитывающей грамматические свойства сочинительных связей.</s>\n",
      "В данной работе рассматривается задача определения факторов, влияющих на удержание курьеров, на примере сервиса доставки. Для решения этой задачи был сделан выбор между двумя методами, наиболее распространенными в задачах такого рода. В результате было предложено решение, основанное на эвристической модели.</s>\n",
      "\n",
      "\n",
      "1\n",
      "В данной работе рассматриваются некоторые задачи, которые можно решить недетерминированно.</s>\n",
      "В работе рассматривается задача о поиске максимального объединения героев на двухмерной двумерной карте.</s>\n",
      "В данном диплом представлена разработка виртуальной машины динамического символьного исполнения для языков программирования Java и Kotlin, расширяющая уже реализованную подсистему символьного исполнения подсистемой конкретного исполнения с целью уменьшения количества ложных срабатываний и увеличения покрытия кода. Виртуальная машина реализует подход, который объединяет оба подхода, чтобы повысить эффективность и надежность анализа кода. Такая комбинация позволяет выполнять программу символьно и конкретно, уменьшая неточности чистого символьного исполнения и повышая качество анализа и покрытие анализируемой программы. Все это делает анализ более практичным и применимым для тестирования и анализа программного обеспечения.</s>\n",
      "В работе рассматриваются классы Харди, в частности класс Харди Hp (с p ≥ 1) и класс Смирнова. Для них построены неравенства, обеспечивающие существование ограниченных в круге аналитических функций.</s>\n",
      "В данной работе рассматриваются различные модели для предсказания времени удерживания пептидов, основанные на глубоком и машинном обучении. Для каждой модели были сделаны предсказания на 70986 последовательностей, взятых из раковых клеток, и на 7887 последовательностей, взятых из раковых клеток с EV<0.1. Полученные результаты были сравнены друг с другом и с результатами баз данных для предсказания времени удерживания.</s>\n",
      "В данной работе мы рассмотрим песочные модели на подмножестве плоскости, а потому все определения будем давать для них. Введём граф, у которого вершинами будут являться клеточки плоскости, и две вершины соединены ребром, если соответствующие им клеточки имеют общую сторону. Клеточка(вершина) (i, j) это клеточка с центром (i + 0.5, j + 0.5). Рассмотрим на некотором подмножестве плоскости граф Γ с целочисленными вершинами.\n",
      "Определение 1.1. Состояние - это функция ϕ: Γ → Z⩾0 , которая будет вершинам графа ставить в соответствие количество песчинок в них.\n",
      "Если количество песчинок в вершине больше 4, то мы называем такую вершину нестабильной и можем в ней произвести обвал.\n",
      "При обвале в некоторой вершине (i, j) происходит следующее: количество песчинок в данной вершине уменьшается на 4, а в смежных с ней вершинах увеличивается на 1.\n",
      "Все вершины Z2 ∖ Γ мы обозначаем стоками, в которых запрещено делать обвалы и находящиеся в них песчинки мы не учитываем.\n",
      "Определение 1.2. Релаксация - это процесс выполнения обвалов нестабильных вершин до тех пор, пока таковых не останется. Состояние, получившиеся после релаксации содержат в себе стабильные состояния, обозначается ϕ○ .\n",
      "Релаксация не зависит от порядка, в котором происходят обвалы и конечна при конечном количестве песчи\n",
      "В данной работе описан статический анализ кода на основе символьного исполнения. При помощи taint-анализа был добавлен механизм поиска уязвимостей в безопасности. Кроме того, был разработан инструмент для IntelliJ IDEA, который позволяет автоматически создавать отчёты по сгенерированным тестам.</s>\n",
      "Работа посвящена разработке программной системы для автоматической генерации модульных тестов на языке программирования Python.</s>\n",
      "В данной работе рассматриваются три варианта задачи распознавания сущностей на графе знаний об организациях.</s>\n",
      "В работе рассматривается непрерывность функционалов, определяемых на выпуклых телах в метрике Хаусдорфа.</s>\n",
      "Высокочастотная торговля (High-Frequency Trading, HFT) стала неотъемлемой частью современных финансовых рынков, где десятки тысяч транзакций могут быть выполнены за доли секунды. Эта форма торговли характеризуется высокой скоростью выполнения сделок, часто измеряемой в микросекундах, и зависит от мощных вычислительных систем, передовых алгоритмов и современных технологий передачи данных. В своей стремительной природе высокочастотная торговля открывает новые возможности для получения прибыли, однако требует участия активных и адаптивных торговых алгоритмов.\n",
      "\n",
      "Именно здесь вступает в игру глубокое обучение с подкреплением, один из ключевых подходов в области машинного обучения. Глубокое обучение с подкреплением предоставляет возможность разработки алгоритмов, которые способны обучаться на основе опыта и взаимодействия с окружающей средой. Этот подход позволяет автоматически оптимизировать параметры торгового алгоритма на основе полученных наград и обратной связи от рынка в режиме реального времени. Внедрение глубокого обучения с подкреплением в сферу высокочастотной торговли может принести значительные преимущества, такие как повышение эффективности торговых стратегий, адаптация к изменяющимся рыночным условиям и снижение воздействия человеческого фактора на процесс принимания решений.\n",
      "\n",
      "Целью данной дипломной работы является создание практического применения глубокого обучения с подкреплением для оптимизации параметров торгового алгоритма компании «Спектральные технологии» в контексте высокочастотной торговли. Мы стремимся разработать инновационный подход, который позволит автоматически оптимизировать параметры торгового алгоритма на основе полученного опыта и обратной связи от рынка в режиме реального времени.\n",
      "\n",
      "Для достижения этой цели перед нами стоят следующие задачи:\n",
      "1. Выбор подходящего алгоритма обучения: исследовать различные алгоритмы глубокого обучения с подкреплением, провести сравнение эффективности их применения в контексте оптимизации параметров торгового алгоритма.\n",
      "2. Создание пайплайна обучения: разработать эффективный и гибкий пайплайн, который позволит проводить эксперименты с различными модификациями алгоритмов. Этот пайплайн будет обеспечивать сбор и предобработку данных, обучение и оценку модели.\n",
      "3. Построение информативных признаков для обучения: определить набор признаков, которые будут использоваться для обучения модели глубокого обучения с подкреплением. Эти признаки должны содержать достаточно информации о состоянии рынка и текущих условиях торговли, чтобы модель могла эффективно их использовать для оптимизации параметров торгового алгоритма.\n",
      "4. Подбор хорошей архитектуры нейросети: провести исследование различных архитектур нейронных сетей, чтобы найти наиболее подходящую структуру для нашей задачи. Это включает выбор типов слоев, настроек оптимизаторов и других параметров, которые будут влиять на производительность и обучаемость модели.\n",
      "\n",
      "Выполнение поставленных задач представляет собой сложный и трудоемкий процесс. Достижение оптимальных результатов требует нахождения лучших практик, адаптированных к специфике высокочастотной торговли, что требует проведения обширного исследования и множества экспериментов.\n",
      "Данная работа вносит свой вклад в область высокочастотной торговли, предлагая новые инструменты и методы для оптимизации торговых стратегий с использованием глубокого обучения с подкреплением. Результаты исследования могут быть полезны для HFT-компаний, трейдеров и исследователей, которые заинтересованы в повышении эффективности и прибыльности своих торговых операций.</s>\n",
      "В данной работе изучаются некоторые гипотезы о морфических словах.</s>\n",
      "Данная работа посвящена исследованию поведения внутренних фрагментных ионов в масс-спектрах белков. Для каждого масс-спектра требуется понять, из какого фрагмента белка он был получен, а также разметить пики в этом масс-спектре. Пики могут быть отнесены к начальным, конечным, а также внутренним фрагментам ионов. Кроме того, необходимо проанализировать полученные результаты и попытаться найти закономерности в том, в каких позициях последовательности аминокислот чаще всего начинаются или заканчиваются внутренние ионы.</s>\n",
      "Рассматриваются выпуклые триангуляции, то есть триангуляции с дефектами 1, 1 и 4. В работе изучаются триангуляции с дефектами {1, 1, 4}, для которых получается оценка снизу и сверху количества классов изометрий. Для триангуляций с дефектами {1, 1, 2, 2} получены\n",
      "В работе рассмотрена проблема автоматической изоляции кода, содержащего внешние вызовы. На основе анализа существующих решений были выбраны основные инструменты для реализации автоматической изоляции кода. Была разработана и реализована стратегия изоляции, работающая на основе моделирования символьных моделей. Также была разработана и реализована архитектура программного продукта, позволяющая воспроизводить сгенерированные им тесты. Тестирование предложенного решения на промышленном проекте Huawei показало, что решение работает успешно.</s>\n",
      "Работа состоит из 4 глав:\n",
      "• Первая глава – введение, в котором рассказывается о нашей цели, задачах и основных принципах работы.\n",
      "• Вторая глава – предлагает решение нашей задачи. В ней описываются решения, принимаемые при создании приложения, а также указываются недостатки и ограничения, с которыми столкнулись разработчики при создании приложения.\n",
      "• Третья глава – представляет нашу архитектуру и структуру базы данных. В ней описываются основные модули и компоненты, а также рассказывается о решениях, принимаемых при их реализации.\n",
      "• Четвертая глава – представляет нашу реализацию. В ней описывается реализация нашего приложения, а также рассказывается о решениях, принимаемых при их реализации.</s>\n"
     ]
    }
   ],
   "source": [
    "print(*df[\"learnt_8k\"].tolist(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('В данной работе рассматривается модельная структура алгебр над теорией Ловера.</s>', 'В работе рассматриваются алгебраические теории над конечнопорожденными свободными T -алгебрами. Определяется структура монад на T -объектах, которые оказываются финитарными. Предлагается определение алгебраической теории в терминах монад.</s>')\n",
      "\n",
      "('В работе рассматриваются плотности решеток трансляций в трёхмерном пространстве. В частности, получается оценка плотности решеток трансляций для любой замкнутой выпуклой конвексной области 𝐾 в трёхмерном пространстве.</s>', 'В')\n",
      "\n",
      "('В работе рассматривается задача булевой выполнимости. Помимо известных методов решения, в работе предлагается использовать методы машинного обучения для поиска эвристических правил. Для этого используются методы сжатия данных и методы машинного обучения.</s>', 'In this work we propose a method for improving the performance of SAT solvers by embedding a graph neural network into a state-of-the-art solver MiniSat. The network predicts the next variable to branch on during each step based on the current state of the problem. The proposed method is compared with the original Graph-Q-SAT and other solvers on a set of SAT instances from the Uniform Random-3-SAT distribution and achieves better results in terms of wall-clock time and the number of branching decisions required to solve an instance.</s>')\n",
      "\n",
      "('В данной работе рассматривается энергетически-эффективная аппроксимация многомерных случайных процессов. В качестве аппроксимационных процессов рассматриваются процессы с медленно убывающими вторыми моментами, а также их комбинации. В качестве энергетически-эффективной аппроксимации рассматривается процесс с медленно убывающими вторыми моментами, а также его комбинация с процессом с медленно убывающими вторыми моментами. В работе рассматривается, какие процессы с медленно убывающими вторыми моментами и их комбинации можно аппроксимировать энергетически-эффективными процессами с медленно убывающими вторыми моментами и их комбинациями.</s>', 'В работе мы обобщаем результаты об энергии натянутых струн, сопровождающих Винеровский процесс, на случай многомерного Винеровского процесса.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача добавления признаков композиций в функцию потерь модели MusicTransformer для настраиваемой генерации музыки. В работе рассматриваются различные способы представления композиций и их комбинации с RPR. Представленный способ позволяет добавлять признаки композиций в функцию потерь модели MusicTransformer, а также позволяет использовать в качестве признака композиции только один из компонентов RPR.</s>', 'В работе рассматривается применение добавления признаков композиций в функцию потерь модели MusicTransformer для настраиваемой генерации музыки. Помимо MusicTransformer, используется и базовая модель Transformer. Проверяется модификация оригинальной модели, а также предлагается модификация модели Transformer, которая позволяет генерировать музыку в формате MIDI.</s>')\n",
      "\n",
      "('Обозначим группу \\\\(G\\\\) и действие \\\\(G\\\\) на пространстве \\\\(X\\\\).\\nДействием \\\\(G\\\\) на пространстве \\\\(X\\\\) называется бинарная функция \\\\(G\\\\times X\\\\to X\\\\), определенная набором элементов \\\\(g\\\\in G\\\\) и элементов \\\\(x\\\\in X\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=x\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\), как множество элементов \\\\(G\\\\), входящих в последовательность \\\\(x\\\\).\\nОбозначим множество элементов \\\\(g\\\\in G\\\\), для которых существует элемент \\\\(x\\\\in X\\\\) таков, что \\\\(g\\\\cdot x=x\\\\) и \\\\(g\\\\cdot y=y\\\\) для любого элемента \\\\(y\\\\in X\\\\), отличного от \\\\(x\\\\) и для любого элемента \\\\(z\\\\in X\\\\), такого, что \\\\(z\\\\cdot x=y\\\\) и \\\\(z\\\\cdot y=x\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(z\\\\cdot z=x\\\\) и \\\\(z\\\\cdot z=y\\\\) и \\\\(', 'В данной работе рассматривается проблема существования универсальной системы для класса всех нулевых-энтропийных систем на группах с амплитудой.</s>')\n",
      "\n",
      "('В данной работе рассмотрены методы реализации генератора запросов к иерархическим базам данных, а также описаны основные проблемы, связанные с их реализацией. В результате были предложены и реализованы алгоритмы реализации генератора запросов с регулируемыми оптимизациями.</s>', 'В данной работе предлагается генератор языка запросов для поиска в иерархических структурах. Предлагается схема генератора и код для парсера и исполнителя. Оптимизации запросов приводятся в виде алгоритмов.</s>')\n",
      "\n",
      "('Данная выпускная квалификационная работа посвящена исследованию идентификации пептидных природных соединений на основе масс-спектра. В работе рассматривается метод идентификации пептидных природных соединений, основанный на использовании ППС-графов, и его модификации. В работе также рассматривается задача идентификации пептидных природных соединений на основе масс-спектра. В работе рассматривается задача идентификации пептидных природных соединений на основе масс-спектра. В работе также рассматривается задача идентификации пептидных природных соединений на основе масс-спектра. В работе также рассматривается задача идентификации пептидных природных соединений на основе масс-спектра.</s>', 'В работе рассмотрена задача идентификации масс-спектров пептидных природных соединений с учётом двух возможных модификаций. При этом одна модификация предполагается распространённой (из небольшого списка конкретных высокочастотных модификаций), а вторая – произвольной, как это было изначально в алгоритме VarQuest.</s>')\n",
      "\n",
      "('В выпускной квалификационной работе рассматривается задача фаззирования программного кода на языках C/C++ с использованием универсальной фаззинг платформы. В работе рассматриваются различные возможные реализации фаззинга, а также рассматриваются возможные способы оптимизации. Рассматривается возможность использования фаззинг платформы для программного обеспечения, реализованного на языках C/C++. В работе рассматривается возможность использования фаззинг платформы для программного обеспечения, реализованного на языках C/C++.</s>', 'В работе рассматривается фаззинг программ на языках C/C++. Разработана платформа для фаззинга, которая может быть использована для фаззинга программ на любом языке.</s>')\n",
      "\n",
      "('Данная работа посвящена изучению слабых решений параболического уравнения в частных производных с дрифтом, где под дрифтом подразумевается «скорость сноса» (то есть коэффициент при первой производной). Наиболее важными результатами данной работы являются:\\n1. Теорема единственности в классе слабых решений и принцип максимума.\\n2. Теорема аппроксимации.\\n3. Слабая производная решения по времени.\\n4. Невозрастание L1-нормы слабого решения по времени.</s>', 'В данной работе изучается слабое решение параболического уравнения в частных производных с дрифтом, где под дрифтом подразумевается «скорость сноса» (то есть коэффициент при первых частных производных по пространственным переменными). Особенный интерес к изучению таких уравнений возникает в гидродинамике, где в качестве дрифта выступает само поле скоростей жидкости. Исторический обзор, а так же всеобщее изложение классических результатов, касающихся однозначной разрешимости задач в энергетическом классе, содержатся в монографии [7]. Большинство этих результатов получено в предположении о достаточной «гладкости» дрифта, например, его существенной ограниченности (в нашем случае существенно ограниченный дрифт будет называться регулярным). В случае сингулярного (то есть нерегулярного) дрифта Теорема единственности')\n",
      "\n",
      "('В данной работе описывается реализация алгоритма конгруэнтной замыкания для уравнений, выражающихся в выражениях с предикатами и константами.\\nРеализация основана на методе сжатия уравнений.\\nОписаны тесты, проведенные на примерах.</s>', 'В данной работе описывается реализация алгоритма для ключевой задачи в теории неиспользуемых функций (QF UF), где решение проблемы равенства состоит в определении, какие выражения равны по отношению к неиспользуемым функциям. Реализация основывается на алгоритме, описанном в [Kap19], который определяет ключевые элементы для равенства по отношению к неиспользуемым функциям. В реализации применяется метод сжатия, который позволяет избежать использования SAT-соглашений при проверке равенства.</s>')\n",
      "\n",
      "('В данной работе рассматривается тропическая кривая на симплектическом многообразии. Затем определяется тропическая кривая на симплектическом многообразии, содержащем симплектическую тор. Затем определяется тропическая кривая на симплектическом многообразии, содержащем симплектическую тор, в котором есть нетривиальные торы.</s>', 'В данной работе рассматривается потенциал для симплектических тороических многообразий.</s>')\n",
      "\n",
      "('Данная работа посвящена исследованию молекулярных структур в песочных паттернах. В работе рассматриваются структуры, которые можно увидеть в конусах, находящихся в границах множества Γ8, и описывается метод определения принадлежности конусов к множеству Γ8.</s>', 'В данной работе рассматривается регулярный граф G8 = (Z2 , E), вершины (x1 , x2 ), (y1 , y2 ) в нём соединены ребром тогда и только тогда, когда |x1 − y1 | + |x2 − y2 | = 1. В работе рассматривается область, замощённая повторяющимися паттернами, для каждой области – свой паттерн (детали ниже). Каждой области соответствует вершина конуса в Рис. 5 и паттерны для некоторых вершин указаны в Таблице 6.</s>')\n",
      "\n",
      "('В данной выпускной квалификационной работе рассматривается проблема восстановления траектории мобильного устройства, а также предлагается решение этой задачи. Для решения этой задачи в работе рассмотрены существующие методы решения этой задачи, а также предложены новые методы, которые позволяют улучшить качество восстановленной траектории.</s>', 'В данной работе описано решение проблемы восстановления траектории смартфона в пространстве, используя данные инерциальных сенсоров. В результате работы был создан мобильный приложение для реконструкции 3D сцены, которое позволяет оценить качество снимков и предложить пользователю рекомендации по повторному съемке объекта.</s>')\n",
      "\n",
      "('Данная работа посвящена поиску способов автоматического вывода инвариантов для символьной виртуальной машины KLEE. Для этого были проведены исследования по предметной области, а также рассмотрены и оценены различные подходы к автоматической генерации тестовых данных. В результате были выявлены недостатки предыдущих подходов, а также предложены новые методы для автоматического вывода инвариантов.</s>', 'Данная работа посвящена улучшению анализа кода символьной виртуальной машине KLEE путём внедрения вывода индуктивных инвариантов при помощи стратегии двунаправленного исполнения. В рамках данной работы были разработаны и реализованы алгоритмы вывода индуктивных инвариантов, которые позволяют KLEE улучшить анализ кода.</s>')\n",
      "\n",
      "('Данная работа посвящена исследованию коэффициентов дробных скручиваний Дена для кос, представляющих собой последовательности элементов из множества конечных целых чисел. В работе рассматривается группа кос, состоящая из всех кос, представляющих собой последовательности элементов из множества конечных целых чисел, и изучается влияние структуры группы кос на свойства коэффициентов дробных скручиваний Дена для кос. В частности, в работе показывается, что если группа кос не содержит косы, представляющих собой последовательности элементов из множества конечных целых чисел, то коэффициенты дробных скручиваний Дена для кос, представляющих собой последовательности элементов из множества конечных целых чисел, равны нулю.</s>', 'В работе представлен алгоритм вычисления коэффициентов дробных скручиваний Дена для косов.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача определения чисел Кармайкла. Определяется понятие числа Кармайкла специального вида. Задача определения чисел Кармайкла специального вида решается при помощи метода распределения чисел Кармайкла и метода перестановки.</s>', 'Данная работа посвящена исследованию множества K, которое определяется как K = {k : существует n, такое, что N = 2n k + 1 – число Кармайкла}. В [8] показано, что K имеет нулевую асимптотическую плотность, то есть, K ∩ [1; n] = 0. n→∞\\nn\\nlim\\nВ то же время, определить принадлежность заданного составного числа k множеству K практически невозможно, за исключением тех случаев, когда число Кармайкла N = 2n k + 1 найдено явно. В [5] показано, что 9, 15, 21 и 25 не принадлежат K.\\nВ этой работе построена армфметическая прогрессия, члены которой, являющиеся утроенными простыми числами, не являются элементами K. Прогрессия построена таким образом, что она содержит бесконечно много подходящих чисел, поэтому главный результат работы позволяет явно привести пример сколь угодно большого числа k, не входящего в K.</s>')\n",
      "\n",
      "('В работе рассматривается теория выпуклых точек, в частности, изучается явление межточечного пересечения в выпуклых точках. Получены новые оценки на максимальные и минимальные выпуклые точки с точностью до константы.</s>', 'В данной работе рассматриваются выражения для вложенных сфер в n-мерных ортогональных векторах и для их смежных сфер.</s>')\n",
      "\n",
      "('В работе рассматриваются задачи типа Варинга с простыми переменными. В частности, рассматриваются задачи, связанные с теоремой Миллера, теоремой Куммера, теоремой Крэмара и теоремой Крэмара-Ли.</s>', 'В')\n",
      "\n",
      "('В выпускной квалификационной работе рассматривается возможность создания программной системы на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений.\\nВ работе рассматривается встраивание функций, а также программная система LLVM, а также программные системы для встраивания функций, написанные на основе LLVM. Также рассматривается влияние встраивания функций на производительность приложений.\\nДля разработки программной системы на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений были использованы методы программирования, описанные в работе.\\nДля разработки программной системы на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений были использованы методы программирования, описанные в работе.\\nВ работе описана программная система на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений.\\nВ работе описана программная система на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений.\\nВ работе описана программная система на основе LLVM для оптимизации задержки в высоконагруженных секциях кода C++-приложений.</s>', 'В данной работе представлена программная система на основе LLVM, которая позволяет оптимизировать задержку в высоконагруженных секциях кода C++ приложений путем регулируемого агрессивного встраивания функций.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача на решетке в области, заданной линейными неравенствами. Основным результатом работы является явная формула для числа путей на решетке, если ограничения на пути задаются линейными неравенствами.</s>', 'Данная работа посвящена рассматриванию путей на решетке Zd , в которых каждый шаг соответствует увеличению одной из координат на 1. Количество путей между двумя вершинами равно, как известно, мультиномиальному коэффициенту P(bi −ai ) \\x01 b1 −a1 ,...,bd −ad . В случае, когда начало пути совпадает с началом координат, речь об обычных таблицах Юнга, количество которых считается по формуле крюков. Теперь пути между двумя вершинами можно отождествить с косыми таблицами Юнга. В случае, когда первая координата всегда не меньше двух других, мы рассматриваем такие пути, в которых в любой точке пути (x1 , x2 , x3 ) выполняется x1 ≥ x2 . Если рассматривать такие пути, то получаем формулу для Ca1 ,a2 ,...,ak . Далее, рассматриваем такие пути, в которых в каждой точке пути (x1 , x2 , x3 ) выполняется x1 ≥ x2 + x3 .</s>')\n",
      "\n",
      "('Данная работа посвящена раскраскам графов. В ней мы рассматриваем раскраски, которые принимают только правильные конфигурации, а также раскраски, принимающие только динамические конфигурации.</s>', 'В работе рассматриваются динамические раскраски графа, описанные в [1], [2], [3], [4], [5], [6], [7]. Рассматриваются различные способы улучшения этих оценок.</s>')\n",
      "\n",
      "('Целью данной работы является разработка нового визуального интерфейса для настройки алгоритмов машинного обучения и настройки данных. В работе рассматриваются основные методы обучения и настройки алгоритмов машинного обучения, а также основные виды настройки данных. В результате проведенного анализа были выявлены проблемы, с которыми сталкиваются пользователи при настройке алгоритмов машинного обучения и настройке данных, а также был определен набор требований к новым интерфейсам. В работе также представлена концепция визуальной системы настройки алгоритмов машинного обучения и настройки данных, а также предложены возможные варианты реализации этой концепции.</s>', 'В данной работе представлен программный продукт Learn2Learn, позволяющий создавать и настраивать модели машинного обучения с помощью графического интерфейса интуитивно понятного пользователям, а также исходный код и пояснения к нему. Мы продемонстрируем методы, позволяющие интегрировать программирование в нашу систему, благодаря чему она не потеряет в гибкости по сравнению с традиционными способами настройки нейросетей, а также создадим сложную нейросеть, решающую задачу обнаружения объектов на изображении, не написав при этом ни единой строчки кода.</s>')\n",
      "\n",
      "('Данная работа посвящена изучению метода условных нижних оценок (FPRAS) для задачи поиска путей с контекстно-свободными ограничениями. Предложенный алгоритм приводит к улучшению существующих методов и позволяет решить задачу на очень больших множествах возможных графиков.</s>', 'Цель работы — изучение границы сложности проблемы CFL reachability с контекстно-свободными ограничениями. В данной работе обсуждаются основные алгоритмы и существующие проблемы, даются новые оценки сложности, а также предлагаются возможные пути к улучшению существующих алгоритмов.</s>')\n",
      "\n",
      "('В работе рассматривается задача о тензорном произведении представлений матроидов. На основе результатов, полученных для тензорного произведения представлений матроидов, получаются результаты для тензорного произведения представлений матроидов в случае, когда один из матроидов равномерный.</s>', 'Цель дипломной работы — исследовать разные подходы к понятию тензорного произведения матроидов. Рассмотрены два важных подхода к построению матроидов: представление матроида и максимальный матроид.</s>')\n",
      "\n",
      "('В данной работе рассматривается гипотеза МакКуэйга и Отта о стягиваемых множествах вершин в 3-связном графе. Построены алгоритмы, определяющие стягиваемые множества вершин в 3-связном графе с k вершинами и в 3-связном графе с k вершинами, допускающем циклы длиной не более 3.</s>', 'В работе доказывается, что гипотеза о стягиваемых множествах вершин в 3-связном графе является верной для любого k ⩾ 5.</s>')\n",
      "\n",
      "('Данная работа посвящена исследованию объёма ресурсов, используемых приложением при увеличении времени ответа от внешних источников данных. В работе рассматривается использование реактивного подхода к программированию, а также его влияние на объём ресурсов, используемых приложением. В результате исследования было обнаружено, что приложение, использующее реактивный подход к программированию, может использовать меньше ресурсов, чем приложение, использующее синхронный подход к программированию, при увеличении времени ответа от внешних источников данных.</s>', 'В работе рассматривается эффективность и производительность приложений, работающих с базами данных.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача построения прямой эмбеддинговой графики с планарным графиком на прямой. Практическое применение прямой эмбеддинговой графики состоит в использовании прямой эмбеддинговой графики в качестве отображения графа на прямую, что позволяет сэкономить время и ресурсы на решении задач, связанных с планарными графами.\\nВ работе рассматриваются различные подходы к решению задачи прямой эмбеддинговой графики, а также рассматривается класс планарных графов, для которых прямая эмбеддинговая графика может быть построена.</s>', 'В данной работе рассматривается задача поиска upward straight-line embedding для планарных динамических графов. В качестве набора вершин используются точки в двухмерной плоскости. Сначала определяется, какие классы планарных динамических графов имеют upward straight-line embedding, а также, какие классы планарных динамических графов не имеют upward straight-line embedding. Затем определяется, какие классы планарных динамических графов имеют upward straight-line embedding в точном соответствии с их би-нетривиальными свойствами.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача о суммах линейных образов выпуклых множеств в евклидовой и евклидовой плоскости. Данная задача имеет много приложений, например, в теории функций, теории множеств, теории вероятностей, теории дифференциальных уравнений.\\nВ работе дается оценка в евклидовой плоскости, приводится пример для выпуклых множеств в евклидовой плоскости, дается решение для выпуклых множеств в евклидовой плоскости. Также рассматривается задача о суммах линейных образов выпуклых множеств в евклидовой и евклидовой плоскости в евклидовой и евклидовой плоскости.\\nДанная задача имеет много приложений, например, в теории функций, теории множеств, теории вероятностей, теории дифференциальных уравнений.\\nВ работе дается решение для выпуклых множеств в евклидовой плоскости.\\n\\nДля выпуклых множеств в евклидовой плоскости дается решение, основанное на методе максимального энтропий.\\nДанная задача имеет много приложений, например, в теории функций, теории множеств, теории вероятностей, теории дифференциальных уравнений.\\nВ работе дается решение для выпуклых множеств в евклидовой плоскости.\\nДанная задача имеет много приложений, например, в теории функций, теории множеств, теории вероятностей, теории дифференциальных уравнений.\\nВ работе дается решение для выпуклых множеств в евклидовой плоскости.</s>', 'В данной работе продолжается изучение экстремальных задач вида\\n|\\n\\nN\\nX\\n\\nTi A| → min\\n\\ni=1\\n\\nдля компактов A единчиной меры и фиксированных линейных операторов T1 , . . . , TN .</s>')\n",
      "\n",
      "('Данная работа посвящена исследованию возможности применения методов искусственного интеллекта к задачам проверки эквивалентности программ.\\nВ работе рассматривается задача проверки эквивалентности программ, реализуемых на языке Java, с помощью метода SAT. В качестве транслятора программ в схемы для языка Java используется Java-код.\\nВ работе приводится список 30 программ, представленных в виде текстовых файлов на языке Java. Для каждой программы на языке Java приводится соответствующая схема в формате CIRCUIT-SAT. В качестве реализации алгоритма для задачи проверки эквивалентности программ используется программное обеспечение Sat4j.\\nВ работе приводится описание метода трансляции программ на языке Java в схемы для языка CIRCUIT-SAT, а также результаты проверки эквивалентности программ.</s>', 'В данной работе рассматривается возможность трансляции программ для виртуальной машины Java в булевые схемы и задача их проверки эквивалентности. Построена система трансляции байткода виртуальной машины Java в булевые схемы, а также описаны основные особенности её работы. Используется программный комплекс transbyte для кодирования программ в булевые схемы. Рассматривается задача распознавания семантических клонов с помощью булевых схем и SAT-решателя.</s>')\n",
      "\n",
      "('В данной работе изучается вопрос, какая длина минимального принимаемого объекта имеет автомат, который принимает заданные им строки. В частности, в работе изучается длина минимального принимаемого объекта для двухстороннего конечного автомата, который запоминает направление последнего шага, и для двухстороннего конечного автомата, который запоминает направление последнего шага и запоминает строки.</s>', 'В данной работе рассматриваются вопросы о максимальном размере принимаемой объектом автоматов. В частности, определяется, как велик может быть максимальный размер принимаемой строки для двухстороннего конечного автомата, который запоминает направление последнего шага. Этот результат доказывается точностью. Доказано, что максимальное число вершин в минимальном принимаемом дереве для недетерминированных древесных автоматов определяется точно. Для древоходных автоматов показывается, что максимальный размер минимального принимаемого дерева — двойной экспоненциальный от числа состояний. Кроме того, доказывается разрешимость задачи пустоты для двух видов автоматов на графах. Задача непустоты для графоходных автоматов, которые ходят по рёбрам графа, оказывается NEXP-полной, а для замощений графов подграфами-звёздами, оказывается NP-полной.</s>')\n",
      "\n",
      "('В данной работе рассматриваются возможности применения статистических тестов для оценки эффективности улучшений мобильных игр. По результатам исследования определяется, какие из тестов наиболее применимы для мобильных игр и какие улучшения могут быть оценены с помощью данных, полученных при их применении.</s>', 'В данной работе представлены результаты по следующим задачам:\\n1. Проверка применимости статистических тестов.\\n2. А/B тест без непосредственного проведения А/B теста.\\n3. Влияние распределения улучшения от фичи на A/B тесты.\\n\\nВыводы: Без выкидывания китов – видно, что отвергаем гипотезу чаще, чем нужно, для платежей больших дней, для платежа первого дня – чаше принимаем.\\nПри выкидывании китов – видно, что при более крупных разбиениях мы (при уровне значимости 0.05 обычно) гипотезу чаще принимаем, чем нужно, а при более малых – отвергаем больше, чем нужно.\\nДалее были проведены аналогичные исследования, но со скорректированным тестом Стьюдента:\\n• для неравных дисперсий тест Стьюдента с коррекцией Уэлча, устанавливаем параметр equal_var=False. Тогда p-value вычисляется так:\\np = ttest_ind(group_a, group_b, equal_var=False)[1]\\n• для несимметричного распределения тест Стьюдента с коррекцией Саттертуэйта, устанавливаем параметр nan_policy=’omit’. Тогда p-value вычисляется так:\\np = ttest_ind(group_a, group_b, nan_policy=’omit’)[1]\\nВ целом, как и предполагалось, данные корректировки не улучшили ситуацию:\\nРис. 1. Платежи за 30 дней с момента установки\\n\\nКоррекция Уэлча\\n\\nКоррекция Саттертуэйта\\n9\\n\\nВероятно, можно продолжить исследование, проведя аналогичные исследования для разбивки на группы по модулю 4 и 8.</s>')\n",
      "\n",
      "('В данной работе рассматриваются методы идентификации малых молекул по масс-спектрам. Описаны основные методы идентификации малых молекул, а также рассмотрены их достоинства и недостатки.</s>', 'В работе описывается программа NPD-Quast, которая выполняет сравнение работы дерепликаторов. Программа может быть использована как инструмент для сравнения работы дерепликаторов, так и для создания базы данных для использования в других программах.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача распознавания лицевых выражений человека. В качестве основной методики для решения этой задачи используется нейронная сеть с подпрограммой решения проблем машинного обучения. Также для улучшения результатов использовалась смешанная нейронная сеть. В качестве датасетов для обучения и тестирования использовались лицевые изображения из работ: [1], [2], [3], [4], [5], [6]. Результаты тестирования показали, что реализованная система имеет средний точность 87,5 % и точность 92,3 % для данных с лицевыми изображениями.</s>', 'Целью данной работы является разработка системы распознавания лицевых выражений человека и сгенерированных лиц с помощью методов машинного обучения.  Кроме того, в работе представлены методы детектирования аномалий в изображениях лиц и их аватарах.</s>')\n",
      "\n",
      "('Данная работа посвящена разработке нового подхода к задаче трекинга лица, основанного на использовании деформируемой 3D-модели головы и метода Bundle Adjustment. Разработанный алгоритм позволяет производить трекинг лица с высокой точностью, а также изменяет лицо в реальном времени.</s>', 'В данной работе рассматривается задача трекинга лица. В качестве входных данных требуется видео, на котором находится голова человека, трёхмерная модель головы и параметры деформации. Целью работы является улучшение алгоритма трекинга, для чего рассматриваются два подхода: добавление в него 2D ландмарок и использование Bundle Adjustment. Ландмарки — определенные точки на лице, определяемые нейросетью — призваны уменьшить возникающую в процессе работы алгоритма ошибку сползания, а Bundle Adjustment — метод уточнения решения благодаря одновременной оптимизации всех кадров — эксплуатирует ту особенность рассматриваемой задачи, что все кадры известны до начала трекинга.</s>')\n",
      "\n",
      "('Данная работа посвящена изучению невырожденных раскрасок графов. В частности, мы доказываем, что в любом графе нет невырожденных раскрасок, в которых нет двух вершин с одинаковым числом красок.</s>', 'В работе доказывается, что если граф не содержит Kd+1 , то он имеет невырожденную раскраску с не более чем d18v − 10 dv ln d вершин.</s>')\n",
      "\n",
      "('В работе рассматривается спектральная теория функций с лакунами в прямой. Определяется понятие функции с лакунами, а также предлагается и реализуется алгоритм построения спектра функции с лакунами. Приведены примеры функций с лакунами и их спектра.</s>', nan)\n",
      "\n",
      "('Данная работа посвящена разработке бекэнда для мобильного приложения для создания персонажа. В ней рассматриваются способы реализации системы способностей для создания персонажа, а также основные принципы и способы реализации системы способностей.</s>', 'В данной работе представлена модель персонажа для настольной ролевой игры DnD.</s>')\n",
      "\n",
      "('Данная работа посвящена построению упрощенной динамической модели для задачи оптимального управления микроклиматом помещения. Для решения данной задачи была построена динамическая модель куба. В работе приведены результаты расчета оптимального управления микроклиматом куба с помощью метода Монте-Карло.</s>', 'Данное исследование является частью проекта по созданию централизованной системы управления потреблением энергии в зданиях общего назначения, тепловой и электрической. Одной из уникальных особенностей данной системы предполагается разработка и последующее применение интеллектуальных методов предсказания энергопотребления. Для предсказания энергопотребления надо понимать и уметь измерять те факторы, которые определяют необходимое количество электричества и тепла для здания в этот момент и в ближайшие несколько часов.\\nПри постановке задачи регулирования для систем отопления, кондиционирования и вентиляции (ОВК), эксплуатирующие службы руководствуются двумя конкурирующими критериями: экономичностью работы системы и комфортом внутренней среды. По различным оценкам, от 50 до 70 процентов всей расходуемой энергии приходится на ОВК. Таким образом, оптимизация потребления этого класса устройств, пусть даже на 5-10 процентов,\\nповлечет за собой ощутимое снижение общего уровня расхода энергии.\\nВ основе данной работы лежит идея скомбинированного подхода к моделированию микроклимата помещения: методы вычислительной термодинамики [1](CFD) и методы сетевых воздушных потоков (NAF). Точное решение задачи CFD на небольшом временном промежутке позволит смоделировать работу измерительного комплекса, оптимизировать вектор измеряемых величин, количество датчиков и их расположение в помещении.\\nВ стандартных офисных зданиях число отдельных зон может доходить до нескольких сотен, поэтому расстановка датчиков и считывание показаний очень трудоемкий процесс.\\nВ то же время, используя сгенерированную модель, мы можем определить значения на датчиках одновременно для всех интересующих расположений.\\nДля сокращения времени обследования здания, ускорения процесса\\nрасвертывания сети, было предложено выделить в проекте два этапа - численное моделирование исследуемого помещения и анализ полученных данных.\\nНа первом этапе будет смоделирован \"Демонстрационный стенд Умного дома\". Геометрия помещения задана в формате CAD. Моделирование будет производиться в COMSOL Multyphysics [2]. Целью является моделирование некоторого промежутка времени с хорошей точностью. Будет смоделирован процесс теплообмена внутри помещения и с окружающим воздухом, а также солнечная радиация. Значения внешней температуры будут взяты из собранного ранее датасета. Полученная модель будет использована далее.\\nНа втором этапе будут выбраны точки возможного расположения датчика. Температуры в них будут импортированы в таблицу. Дальнейшая работа с полученными таблицами будет реализована на языке Python. Проанализировав потенциальные места расположения датчика методом, который предлагают в своей статье Пащенко А.Ф., Рассадин Ю.М. [3] (подробней об этом методе в главе 2.2), сравнительным анализом найдем наилучшую точку размещения микроклиматического датчика. Для небольших помещений достаточно перебора потенциальных точек размещения, для больших помещений со сложной конфигурацией возможно применение градиентного метода поиска.</s>')\n",
      "\n",
      "('В работе рассматривается проблема определения категории когомологий высшего порядка для категории строк с матрицей b. Определяется когомологическая категория строк с матрицей b, а также её категория когомологий высшего порядка. Рассматривается вопрос о возможности определения категории строк с матрицей b, если известна категория строк с матрицей b.</s>', '\\n')\n",
      "\n",
      "('В работе рассматривается построение бесконечной серии трехмерных гиперболических многообразий, сложность которых известна. Для этого решается задача гомологической минимизации на классе гомологически минимальных триангуляций. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В работе рассматривается задача гомологической минимизации на классе гомологически минимальных триангуляций.\\nВ качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры.\\nВ качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры.\\nВ качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры.\\nВ качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры.\\nВ качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гиперболических многообразий приводятся идеальные триангуляции и специальные полиэдры. В качестве начальных гипер', 'В данной работе рассматриваются исключительно компактные 3-многообразия с непустым краем. Пусть ∆ обозначает стандартный тетраэдр. Идеальной триангуляцией компактного 3-многообразия M с непустым краем называется реализация внутренности M в виде склейки конечного числа копий ∆ с удалёнными вершинами по аффинным гомеоморфизмам их граней. Идеальная триангуляция многообразия M называется минимальной, если она содержит наименьшее число тетраэдров среди всех идеальных триангуляций данного многообразия. Число тетраэдров в минимальной идеальной триангуляции многообразия M называется триангуляционной сложностью и обозначается через c∆ (M ). Триангуляционная сложность, как и многие другие инварианты аналогичного типа, довольно трудно вычислять. В перву- ой очередь точные значения триангуляционной сложности известны для многообразий, табулированных при помощи компьютера. Полная таблица ориентируемых гиперболических многообразий с каспами до сложности 9 включительно, описанная в [1], содержит 162 182 минимальных идеальных триангуляций для 61 911 многообразий. Все эти многообразия вместе с триангуляциями включены в компьютерные программы SnapPy [2] и Regina [3]. В [4] перечисляются все гиперболические многообразия с каспами, получающиеся склейкой правильных гиперболических идеальных тетраэдров, до сложности 25 включительно. М. Фуджи в [5] показал, что имеется лишь 8 различных ориентируемых гиперболических 3-многообразий с вполне геодезическим краем сложности 2. В последствии Р. Фриджерио, Б. Мартелли и К. Петронио классифицировали в [6] все компактные ориентируемые гиперболические 3-многообразия конечного объема с непустым вполне геодезическим краем до сложности 4 включительно. На данный момент известны лишь несколько бесконечных серий компактных связных 3-многообразий с краем, для которых удалось установить точное значение триангуляционной сложности. Первая бесконечная серия была описана Р. Фриджерио, Б. Мартелли и К. Петронио в работе [7]. Многообразия этой серии обладают идеальными триангуляциями с единственным ребром. Вопрос минимальности идеальных триангуляций, обладающих ровно двумя рёбрами был исследован А. Ю. Весниным,')\n",
      "\n",
      "('В данной работе рассматривается проблема больших и сверхбольших уклонений в Телеком-процессах, описывающих связные и несвязные системы трафика. Для рассмотрения уклонений в Телеком-процессах используется метод моделирования на основе интегральных представлений, предложенный И. Кая и М. Такку в работе [9]. В работе [16] описывается модель связной системы трафика, состоящей из n узлов, и приводится формула для среднего уклонения в этом случае. В работе [15] рассматривается модель несвязной системы трафика, состоящей из n узлов, и приводится формула для среднего уклонения в этом случае. В данной работе рассматривается модель несвязной системы трафика, состоящей из n узлов, и приводится формула для среднего уклонения в этом случае.</s>', 'В данной работе рассматриваются свойства пуассоновского Телеком-процесса, его предельные теоремы и вероятности больших уклонений.</s>')\n",
      "\n",
      "('В работе рассматриваются оценки констант коэрцитивности дифференциальных операторов в пространствах L_p-норм. В частности, получены достаточные условия максимальности и координатной выпуклости для оценки констант коэрцитивности.</s>', 'В данной работе изучается задача о порядке величины rpn при p → 1.</s>')\n",
      "\n",
      "('В работе рассматривается задача вывода комбинаторных принципов из OBDD, представляющих дизъюнкты КНФ формулы. Обозначим функцию, которая вводит в OBDD представляющие дизъюнкты КНФ формулы, и функцию, которая вводит в OBDD представляющие дизъюнкты КНФ формулы, и функцию, которая вводит в OBDD представляющие дизъюнкты КНФ формул.\\nОпределим систему доказательств, основанную на OBDD, как множество OBDD, таких что для любого OBDD из множества существует OBDD из множества, представляющий дизъюнкты функции, и для любого OBDD из множества существует OBDD из множества, представляющий дизъюнкты функции.\\nОтметим, что, если OBDD представляющие дизъюнкты КНФ формул не содержат OBDD, представляющих дизъюнкты функции, то функция невыполнимо.\\nОтметим, что, если OBDD представляющие дизъюнкты КНФ формул содержат OBDD, представляющих дизъюнкты функции, то функция невыполнимо.\\nТаким образом, вывод комбинаторных принципов из OBDD представляющих дизъюнкты КНФ формул можно рассматривать как задачу поиска OBDD, представляющих дизъюнкты функции, которые не содержат OBDD, представляющих дизъюнкты КНФ формул.\\nОтметим, что, если функция невыполнимо, то OBDD, представляющие дизъюнкты КНФ формул, и OBDD, представляющие дизъюнкты функции, являются эквивалентными.\\nТаким образом, для вывода комбинаторных принципов из OBDD представляющих дизъюнкты КНФ формул можно использовать метод поиска эквивалентной OBDD.\\nВ работе изучается эффективность метода поиска эквивалентной OBDD для вывода комбинаторных принципов из OBDD представляющих дизъюнкты КНФ формул.</s>', 'В нашей работе мы рассматриваем пропозициональные системы доказательств, основанные на OBDD(∧), OBDD(∧, reordering) и их древовидные версии. В нашей работе мы показали, что OBDD(∧), OBDD(∧, reordering), а также их древовидные версии не балансируемые.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача поиска оптимального ответа на специализированные вопросы, такие как темы на форумах. В качестве примера для анализа приводится StackOverflow. Представлены архитектуры трансформеров, которые используются в реализации StackOverflow Translator. Описана архитектура трансформера, который используется в StackOverflow Translator.</s>', 'Данная работа посвящена исследованию потенциала имеющихся языковых моделей в задаче поиска ответа на вопросы, заданные на платформе Stack Overflow. В рамках исследования будет предложен новый метод дообучения таких моделей с учетом специфики данной задачи. После этого будут проведены эксперименты, направленные на подтверждение эффективности предложенного метода.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача о нахождении множеств мономов, допускающих поверхность с особенностью данного порядка. \\nДля решения данной задачи используется метод матроидов.\\nВ работе рассматривается геометрическая интерпретация множества мономов, допускающих поверхность с особенностью данного порядка.\\nДля решения данной задачи используется метод матроидов.</s>', 'В работе рассматривается задача о том, какие множества A допускают особенность порядка m. Для этого рассматривается однородная линейная система на допускаемость множества A. Мы рассматриваем всевозможные многочлены с вещественными коэффициентами αijk , состоящие только из мономов A:\\nX\\nP (x, y, z) =\\nαijk xi y j z k .\\nxi y j z k ∈A\\n\\nДанная система может быть представлена в виде матричной системы. В случае, если A допускает особенность порядка m, матрица этой системы вырождена.\\nОпределение 1. Мы называем A множеством, допускающим особенность порядка m, если существует нетривиальный набор коэффициентов αijk , такой что поверхность C, заданная уравнением P (x, y, z) = 0, имеет в некоторой точке особенность порядка m.\\n\\nДанное определение позволяет рассматривать всевозможные многочлены с вещественными коэффициентами. В частности, мы рассматриваем многочлены, состоящие только из мономов A.\\n\\nПредложение 1. В системе (1) Ĉ4m = Cm+3\\nуравнений, где через Ĉnk = Ck+n−1\\nобозначается количество сочетаний с повторениями из n по k.\\n\\nДоказательство. Ясно, что количество урав')\n",
      "\n",
      "('Целью данной работы является создание мобильного приложения, которое позволяет пользователям создавать персонажей для игры «Комиксы». На примере данного проекта мы хотели показать, что можно использовать модели, обученные на компьютерных играх, для создания персонажей для игр на основе комиксов.\\n\\nВ работе представлены результаты реализации мобильного приложения и его тестирования. В частности, были проведены исследования на основе выборки из 1500 комиксов, которые содержат персонажей для игры. В результате было выявлено, что модели, обученные на компьютерных играх, не смогли создать персонажей, которые соответствовали бы комиксу. Также было установлено, что наиболее удачным методом для создания комиксов является использование байесовской классификации.\\n\\nНа основе полученных результатов было предложено несколько вариантов для дальнейшего развития проекта. В частности, было предложено использовать более подходящие данные для обучения моделей, а также использовать другие методы для создания комиксов.</s>', 'В данной работе представлен мобильный приложение, которое помогает игрокам D&D упростить процесс создания, хранения и управления персонажей. Наш проект включает в себя модели машинного обучения, способные автоматически генерировать уникальные предыстории и визуальные представления персонажей.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача о разложении функций на произвольные интервалы. Проанализированы существующие методы решения задачи и представлена новая методика решения.</s>', 'Данная работа посвящена разработке новых методов для автоматического мониторинга производительности. В работе рассмотрены методы для поиска интервалов изменений производительности в серии измерений. Для этого были разработаны алгоритмы поиска двоичной сегментации, скользящего окна и объединения отрезков снизу вверх. Для выбора метрики для использования в алгоритмах была проведена оценка качества метрик на искусственных и реальных данных.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача автоматического создания модульных тестов для обнаружения уязвимостей в исходных кодах приложений. Разрабатывается и реализуется программная система автоматического создания модульных тестов для обнаружения уязвимостей в исходных кодах приложений. В систему входят три компонента: модуль автоматического создания тестов для обнаружения уязвимостей в исходных кодах приложений, модуль символьного исполнения и модуль SMT-решателя. В систему также входит программа для автоматического создания тестов для обнаружения уязвимостей в исходных кодах приложений, которая использует инструменты анализа исходного кода, а также программа для анализа исходного кода, которая использует инструменты символьного исполнения. Система автоматического создания модульных тестов для обнаружения уязвимостей в исходных кодах приложений реализована на языке Python и является свободно распространяемым открытым ПО.</s>', 'Анализ программного обеспечения — это сложная задача, требующая много времени и нередко не дающая ожидаемого результата. В работе рассматривается возможность автоматизировать эту процедуру, используя символьное исполнение и модульные тесты.</s>')\n",
      "\n",
      "('В работе рассматривается проблема универсальной символьной виртуальной машины. В ходе исследования была выявлена проблема недостаточной универсальности существующих реализаций. Была предложена концепция универсальной символьной виртуальной машины, реализованная в виде программного пакета.</s>', 'В данной работе рассматривается проблема создания универсальной символьной виртуальной машины. Для решения этой проблемы были выделены основные задачи: анализ существующих символьных машин и различных техник в символьном исполнении, проектирование и разработка основных компонентов универсальной символьной виртуальной машины, подходящей для анализа языков с автоматическим управлением памятью.</s>')\n",
      "\n",
      "('В данной работе рассматривается задача выделения сочинительных связей из текста на основе нейросетевых методов. В работе описаны основные подходы к нейронным сетам и описана модель для решения задачи, на основе которой проведены эксперименты с целью определения эффективности данной модели.</s>', 'В данной работе описывается новый подход к выделению сочинительных связей. Цель работы заключается в том, чтобы построить модель, которая позволит выделить сочинительные связи в предложениях на русском языке. Для этого был использован метод, основанный на модели, учитывающей грамматические свойства сочинительных связей.</s>')\n",
      "\n",
      "('Целью данной работы является исследование факторов, влияющих на удержание курьеров, на примере сервиса доставки. В ходе исследования были описаны и проанализированы три подхода к моделированию: эвристическая модель, эмпирическая функция распределения и сравнение подходов. В результате работы была выделена эмпирическая функция распределения, которая наилучшим образом описывает данные.</s>', 'В данной работе рассматривается задача определения факторов, влияющих на удержание курьеров, на примере сервиса доставки. Для решения этой задачи был сделан выбор между двумя методами, наиболее распространенными в задачах такого рода. В результате было предложено решение, основанное на эвристической модели.</s>')\n",
      "\n",
      "('В работе рассматривается асимптотическое поведение случайного блуждания на полупространстве с ограничением на границе. В частности, рассматривается вопрос о том, при каких условиях система имеет стационарное распределение.</s>', '\\n')\n",
      "\n",
      "('В работе рассматривается асимптотическое поведение ВСБ на полупространстве с поглощением на границе.\\nВ частности, получаются оценки размерности, устойчивости и существования стационарной точки для случайных блужданий на полупространстве с поглощением на границе.</s>', '1')\n",
      "\n",
      "('В работе рассматривается задача определения числа совершенных паросочетаний в графе. Приводится верхняя оценка для задачи 0-1 перманент.</s>', 'В данной работе рассматриваются некоторые задачи, которые можно решить недетерминированно.</s>')\n",
      "\n",
      "('В данной работе рассматривается алгоритмическая сложность игры Merge game на двумерной двумерной игровой карте. Полученная версия игры Merge game может быть использована при разработке игр на 2D-карте.</s>', 'В работе рассматривается задача о поиске максимального объединения героев на двухмерной двумерной карте.</s>')\n",
      "\n",
      "('Динамическое символьное исполнение (DSL) - это способ программирования, который позволяет программистам запускать программы без необходимости предоставить исходный код. В данной работе представлены результаты разработки DSL для языков программирования Java и Kotlin.</s>', 'В данном диплом представлена разработка виртуальной машины динамического символьного исполнения для языков программирования Java и Kotlin, расширяющая уже реализованную подсистему символьного исполнения подсистемой конкретного исполнения с целью уменьшения количества ложных срабатываний и увеличения покрытия кода. Виртуальная машина реализует подход, который объединяет оба подхода, чтобы повысить эффективность и надежность анализа кода. Такая комбинация позволяет выполнять программу символьно и конкретно, уменьшая неточности чистого символьного исполнения и повышая качество анализа и покрытие анализируемой программы. Все это делает анализ более практичным и применимым для тестирования и анализа программного обеспечения.</s>')\n",
      "\n",
      "('В работе рассматривается задача об идеалах алгебры H^\\\\infty. В качестве метрики на пространстве идеалов алгебры H^\\\\infty рассматривается метрика на пространстве пространства идеалов алгебры H^\\\\infty, определенная в работе С. В. Кислякова и С. В. Руцкого. В работе доказываются теоремы о неподвижной точке для этой метрики.</s>', 'В работе рассматриваются классы Харди, в частности класс Харди Hp (с p ≥ 1) и класс Смирнова. Для них построены неравенства, обеспечивающие существование ограниченных в круге аналитических функций.</s>')\n",
      "\n",
      "('Настоящая работа посвящена разработке алгоритмов валидации вариантных пептидов. В работе рассматриваются алгоритмы валидации, написанные на основе нейронных сетей, а также алгоритмы, написанные на основе программного обеспечения, предоставленного другими исследователями. Разработанные алгоритмы валидации были протестированы на тестах, содержащих известные вариантные пептиды, и на тестах, содержащих новые вариантные пептиды. В работе рассматривается программное обеспечение, предоставленное другими исследователями, и алгоритм валидации, написанный на основе этого программного обеспечения. Разработанные алгоритмы валидации были протестированы на тестах, содержащих известные вариантные пептиды, и на тестах, содержащих новые вариантные пептиды.</s>', 'В данной работе рассматриваются различные модели для предсказания времени удерживания пептидов, основанные на глубоком и машинном обучении. Для каждой модели были сделаны предсказания на 70986 последовательностей, взятых из раковых клеток, и на 7887 последовательностей, взятых из раковых клеток с EV<0.1. Полученные результаты были сравнены друг с другом и с результатами баз данных для предсказания времени удерживания.</s>')\n",
      "\n",
      "('Работа посвящена исследованию частных случаев роста в песочных моделях. В работе рассматриваются 3 частных случая роста: функция числа обвалов, солитоны и существование паттерна. В работе представлены методы доказательства роста, а также теоремы, которые позволяют доказать рост.</s>', 'В данной работе мы рассмотрим песочные модели на подмножестве плоскости, а потому все определения будем давать для них. Введём граф, у которого вершинами будут являться клеточки плоскости, и две вершины соединены ребром, если соответствующие им клеточки имеют общую сторону. Клеточка(вершина) (i, j) это клеточка с центром (i + 0.5, j + 0.5). Рассмотрим на некотором подмножестве плоскости граф Γ с целочисленными вершинами.\\nОпределение 1.1. Состояние - это функция ϕ: Γ → Z⩾0 , которая будет вершинам графа ставить в соответствие количество песчинок в них.\\nЕсли количество песчинок в вершине больше 4, то мы называем такую вершину нестабильной и можем в ней произвести обвал.\\nПри обвале в некоторой вершине (i, j) происходит следующее: количество песчинок в данной вершине уменьшается на 4, а в смежных с ней вершинах увеличивается на 1.\\nВсе вершины Z2 ∖ Γ мы обозначаем стоками, в которых запрещено делать обвалы и находящиеся в них песчинки мы не учитываем.\\nОпределение 1.2. Релаксация - это процесс выполнения обвалов нестабильных вершин до тех пор, пока таковых не останется. Состояние, получившиеся после релаксации содержат в себе стабильные состояния, обозначается ϕ○ .\\nРелаксация не зависит от порядка, в котором происходят обвалы и конечна при конечном количестве песчи')\n",
      "\n",
      "('Данная работа посвящена разработке статического анализатора на основе символьного исполнения. Рассматривается задача обнаружения ошибок в программах на языке Java. В качестве решения для данной задачи предлагается реализация статического анализатора на основе символьного исполнения. В работе рассматривается алгоритм статического анализатора на основе символьного исполнения. В работе также рассматривается алгоритм статического анализатора на основе символьного исполнения. В работе также рассматривается алгоритм статического анализатора на основе символьного исполнения. В работе также рассматривается алгоритм статического анализатора на основе символьного исполнения. В работе также рассматривается алгоритм статического анализатора на основе символьного исполнения.</s>', 'В данной работе описан статический анализ кода на основе символьного исполнения. При помощи taint-анализа был добавлен механизм поиска уязвимостей в безопасности. Кроме того, был разработан инструмент для IntelliJ IDEA, который позволяет автоматически создавать отчёты по сгенерированным тестам.</s>')\n",
      "\n",
      "('В данной выпускной квалификационной работе рассматривается вопрос автоматической генерации модульных тестов на языке Python. В качестве средств автоматизации выбраны языки Python и Pytest. В качестве тестовых данных используются выводы, полученные из исходного кода на языке Python. В качестве тестовых данных используются выводы, полученные из исходного кода на языке Python. В качестве тестовых данных используются выводы, полученные из исходного кода на языке Python. В качестве тестовых данных используются выводы, полученные из исходного кода на языке Python.</s>', 'Работа посвящена разработке программной системы для автоматической генерации модульных тестов на языке программирования Python.</s>')\n",
      "\n",
      "('В данной работе мы рассматриваем задачу распознавания сущностей на графе знаний об организациях. В качестве входных данных используется график знаний об организациях, полученный с помощью приложения YAGO. В качестве выходных данных используется описание сущностей, полученное с помощью приложения DBpedia.\\nДля улучшения качества выходных данных используется процесс обогащения сущностей атрибутами.\\nДля решения данной задачи мы применяем методы машинного обучения, такие как:\\n\\n- ML-эволюция,\\n- графовые эмбеддинги,\\n- байесовское моделирование,\\n- социальные сети,\\n- поисковая оптимизация,\\n- модели с динамическими входными данными,\\n- модели с динамическими выходными данными.\\n\\nДанная работа позволяет улучшить качество выходных данных и, соответственно, качество результатов.</s>', 'В данной работе рассматриваются три варианта задачи распознавания сущностей на графе знаний об организациях.</s>')\n",
      "\n",
      "('В данной работе рассматриваются точки случайного распределения на границе выпуклого тела. Рассматривается задача о средней площади точек, принадлежащих выпуклому телу. В ходе работы получены следующие результаты: \\n1. Проведено исследование существования точек случайного распределения на границе выпуклого тела.\\n2. Получено доказательство непрерывности в метрике Хаусдорфа выпуклого тела.\\n3. Получена формула средней площади точек, принадлежащих выпуклому телу.</s>', 'В работе рассматривается непрерывность функционалов, определяемых на выпуклых телах в метрике Хаусдорфа.</s>')\n",
      "\n",
      "('В данной работе рассматривается глубокое обучение с подкреплением в высокочастотном трейдинге. \\nВыполняется реализация алгоритмов TRPO и PPO, а также их модификаций, которые могут быть использованы в реальном трейдинге.</s>', 'Высокочастотная торговля (High-Frequency Trading, HFT) стала неотъемлемой частью современных финансовых рынков, где десятки тысяч транзакций могут быть выполнены за доли секунды. Эта форма торговли характеризуется высокой скоростью выполнения сделок, часто измеряемой в микросекундах, и зависит от мощных вычислительных систем, передовых алгоритмов и современных технологий передачи данных. В своей стремительной природе высокочастотная торговля открывает новые возможности для получения прибыли, однако требует участия активных и адаптивных торговых алгоритмов.\\n\\nИменно здесь вступает в игру глубокое обучение с подкреплением, один из ключевых подходов в области машинного обучения. Глубокое обучение с подкреплением предоставляет возможность разработки алгоритмов, которые способны обучаться на основе опыта и взаимодействия с окружающей средой. Этот подход позволяет автоматически оптимизировать параметры торгового алгоритма на основе полученных наград и обратной связи от рынка в режиме реального времени. Внедрение глубокого обучения с подкреплением в сферу высокочастотной торговли может принести значительные преимущества, такие как повышение эффективности торговых стратегий, адаптация к изменяющимся рыночным условиям и снижение воздействия человеческого фактора на процесс принимания решений.\\n\\nЦелью данной дипломной работы является создание практического применения глубокого обучения с подкреплением для оптимизации параметров торгового алгоритма компании «Спектральные технологии» в контексте высокочастотной торговли. Мы стремимся разработать инновационный подход, который позволит автоматически оптимизировать параметры торгового алгоритма на основе полученного опыта и обратной связи от рынка в режиме реального времени.\\n\\nДля достижения этой цели перед нами стоят следующие задачи:\\n1. Выбор подходящего алгоритма обучения: исследовать различные алгоритмы глубокого обучения с подкреплением, провести сравнение эффективности их применения в контексте оптимизации параметров торгового алгоритма.\\n2. Создание пайплайна обучения: разработать эффективный и гибкий пайплайн, который позволит проводить эксперименты с различными модификациями алгоритмов. Этот пайплайн будет обеспечивать сбор и предобработку данных, обучение и оценку модели.\\n3. Построение информативных признаков для обучения: определить набор признаков, которые будут использоваться для обучения модели глубокого обучения с подкреплением. Эти признаки должны содержать достаточно информации о состоянии рынка и текущих условиях торговли, чтобы модель могла эффективно их использовать для оптимизации параметров торгового алгоритма.\\n4. Подбор хорошей архитектуры нейросети: провести исследование различных архитектур нейронных сетей, чтобы найти наиболее подходящую структуру для нашей задачи. Это включает выбор типов слоев, настроек оптимизаторов и других параметров, которые будут влиять на производительность и обучаемость модели.\\n\\nВыполнение поставленных задач представляет собой сложный и трудоемкий процесс. Достижение оптимальных результатов требует нахождения лучших практик, адаптированных к специфике высокочастотной торговли, что требует проведения обширного исследования и множества экспериментов.\\nДанная работа вносит свой вклад в область высокочастотной торговли, предлагая новые инструменты и методы для оптимизации торговых стратегий с использованием глубокого обучения с подкреплением. Результаты исследования могут быть полезны для HFT-компаний, трейдеров и исследователей, которые заинтересованы в повышении эффективности и прибыльности своих торговых операций.</s>')\n",
      "\n",
      "('В работе рассматриваются некоторые гипотезы о морфических словах. В частности, предлагается обобщение известной теоремы о морфизмах, порождающих слова Линдона, и исследуются некоторые другие вопросы.</s>', 'В данной работе изучаются некоторые гипотезы о морфических словах.</s>')\n",
      "\n",
      "('Данная работа посвящена поиску и классификации фрагментных ионов в белковых последовательностях. В работе рассматривается ряд различных подходов к проблеме, включая метод деконволюции спектров, метод максимального правдоподобия и метод сжатия ионов. Было проведено исследование, включающее в себя сравнение результатов различных алгоритмов с использованием 1000 белковых последовательностей, которые были выбраны на основе их профилей в различных базах данных.</s>', 'Данная работа посвящена исследованию поведения внутренних фрагментных ионов в масс-спектрах белков. Для каждого масс-спектра требуется понять, из какого фрагмента белка он был получен, а также разметить пики в этом масс-спектре. Пики могут быть отнесены к начальным, конечным, а также внутренним фрагментам ионов. Кроме того, необходимо проанализировать полученные результаты и попытаться найти закономерности в том, в каких позициях последовательности аминокислот чаще всего начинаются или заканчиваются внутренние ионы.</s>')\n",
      "\n",
      "('Автор производит подсчёт асимптотики числа триангуляций проективной плоскости, дефектов которых равны 1, 1 и 4. Для этого рассматривается поднятие триангуляции на сферу. Также рассматривается развёртка октаэдра на плоскость, что позволяет упростить задачу.</s>', 'Рассматриваются выпуклые триангуляции, то есть триангуляции с дефектами 1, 1 и 4. В работе изучаются триангуляции с дефектами {1, 1, 4}, для которых получается оценка снизу и сверху количества классов изометрий. Для триангуляций с дефектами {1, 1, 2, 2} получены')\n",
      "\n",
      "('Работа посвящена разработке программного средства для автоматической изоляции LLVM кода. В работе рассматривается задача символьного исполнения, а также разработан алгоритм для автоматической изоляции LLVM кода.</s>', 'В работе рассмотрена проблема автоматической изоляции кода, содержащего внешние вызовы. На основе анализа существующих решений были выбраны основные инструменты для реализации автоматической изоляции кода. Была разработана и реализована стратегия изоляции, работающая на основе моделирования символьных моделей. Также была разработана и реализована архитектура программного продукта, позволяющая воспроизводить сгенерированные им тесты. Тестирование предложенного решения на промышленном проекте Huawei показало, что решение работает успешно.</s>')\n",
      "\n",
      "('Работа посвящена разработке архитектуры и созданию мобильного приложения для создания игрового персонажа. Целью работы является разработка мобильного приложения для создания игрового персонажа.\\nПриложение создано на основе фреймворка React Native и использует сервисы API для работы с базой данных. В работе приведены решения, которые были найдены в процессе решения задачи, а также рассмотрены вопросы, которые возникли во время работы.</s>', 'Работа состоит из 4 глав:\\n• Первая глава – введение, в котором рассказывается о нашей цели, задачах и основных принципах работы.\\n• Вторая глава – предлагает решение нашей задачи. В ней описываются решения, принимаемые при создании приложения, а также указываются недостатки и ограничения, с которыми столкнулись разработчики при создании приложения.\\n• Третья глава – представляет нашу архитектуру и структуру базы данных. В ней описываются основные модули и компоненты, а также рассказывается о решениях, принимаемых при их реализации.\\n• Четвертая глава – представляет нашу реализацию. В ней описывается реализация нашего приложения, а также рассказывается о решениях, принимаемых при их реализации.</s>')\n"
     ]
    }
   ],
   "source": [
    "print(*list(zip(df[\"learnt\"], df[\"learnt_8k\"])), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Comparing learn_8k vs longlora-64k & 16k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0', 'id', 'year', 'diploma',\n",
       "       'abstract', 'study_field', 'degree', 'original_diploma_extension',\n",
       "       'raw_model', 'learnt', 'learnt_8k', 'learnt_16k'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../../../artifacts/diplomas_abstracts/baselines_with_learnt_16k.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'Д',\n",
       " '.',\n",
       " '</s>',\n",
       " '</s>',\n",
       " 'Д',\n",
       " '4',\n",
       " 'жи',\n",
       " '</s>',\n",
       " 'ции',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '∩',\n",
       " '</s>',\n",
       " 'lications',\n",
       " '0',\n",
       " 'сли',\n",
       " 'Proof',\n",
       " '</s>',\n",
       " 'Command',\n",
       " 'на',\n",
       " '6',\n",
       " ').',\n",
       " 'reach',\n",
       " '</s>',\n",
       " 'W',\n",
       " 'ч',\n",
       " '</s>',\n",
       " 'для',\n",
       " 'a',\n",
       " '∈',\n",
       " '</s>',\n",
       " '</s>',\n",
       " 'э',\n",
       " 'пара',\n",
       " 'ти',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '�',\n",
       " '9',\n",
       " '(',\n",
       " '×',\n",
       " nan,\n",
       " '7',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '</s>',\n",
       " 'сту',\n",
       " 'ти',\n",
       " 'вы',\n",
       " '</s>',\n",
       " nan,\n",
       " '�',\n",
       " '</s>',\n",
       " 'ors',\n",
       " 'бка',\n",
       " 'бер',\n",
       " '</s>',\n",
       " '[',\n",
       " 'да',\n",
       " ',',\n",
       " '</s>',\n",
       " 'problem',\n",
       " '</s>',\n",
       " 'ной',\n",
       " '</s>',\n",
       " ',',\n",
       " 'мо',\n",
       " 'би']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"learnt_16k\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700.0428571428571, 444.0285714285714, 2.842857142857143)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(list(map(len, df[\"learnt\"]))), np.mean(list(map(len, df[\"learnt_8k\"].astype(str)))), np.mean(list(map(len, df[\"learnt_16k\"].astype(str))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
