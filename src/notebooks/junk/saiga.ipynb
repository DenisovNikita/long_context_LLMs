{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:05:51.144952Z",
     "iopub.status.busy": "2024-04-30T20:05:51.144162Z",
     "iopub.status.idle": "2024-04-30T20:05:51.398124Z",
     "shell.execute_reply": "2024-04-30T20:05:51.397015Z",
     "shell.execute_reply.started": "2024-04-30T20:05:51.144918Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../utils\")\n",
    "from definitions import *\n",
    "from path_helpers import get_dataset_path, get_metric_dir_path\n",
    "from mera_helpers import construct_prompt\n",
    "from llm_helpers import get_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Try to run Saiga "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T18:44:45.456825Z",
     "iopub.status.busy": "2024-04-30T18:44:45.455903Z",
     "iopub.status.idle": "2024-04-30T18:44:55.491780Z",
     "shell.execute_reply": "2024-04-30T18:44:55.490352Z",
     "shell.execute_reply.started": "2024-04-30T18:44:45.456774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate==0.21.0 in /home/jupyter/.local/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: bitsandbytes==0.40.2 in /home/jupyter/.local/lib/python3.10/site-packages (0.40.2)\n",
      "Requirement already satisfied: peft==0.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: transformers==4.34.0 in /home/jupyter/.local/lib/python3.10/site-packages (4.34.0)\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter/.local/lib/python3.10/site-packages (from accelerate==0.21.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.10/site-packages (from accelerate==0.21.0) (24.0)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.10/site-packages (from accelerate==0.21.0) (5.7.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/jupyter/.local/lib/python3.10/site-packages (from accelerate==0.21.0) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0) (4.65.0)\n",
      "Requirement already satisfied: safetensors in /home/jupyter/.local/lib/python3.10/site-packages (from peft==0.5.0) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers==4.34.0) (0.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (2022.10.31)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.10/site-packages (from transformers==4.34.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers==4.34.0) (0.14.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (4.11.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0)\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /kernel/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jupyter/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kernel/lib/python3.10/site-packages (from requests->transformers==4.34.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests->transformers==4.34.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /kernel/lib/python3.10/site-packages (from requests->transformers==4.34.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests->transformers==4.34.0) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.22.2\n",
      "    Uninstalling huggingface-hub-0.22.2:\n",
      "      Successfully uninstalled huggingface-hub-0.22.2\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.19.0 requires huggingface-hub>=0.21.2, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "diffusers 0.27.2 requires huggingface-hub>=0.20.2, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "lm-eval 0.3.0 requires transformers>=4.36.2, but you have transformers 4.34.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.17.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade accelerate==0.21.0 \\\n",
    "  bitsandbytes==0.40.2 \\\n",
    "  peft==0.5.0 \\\n",
    "  transformers==4.34.0 \\\n",
    "  sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:04:31.381296Z",
     "iopub.status.busy": "2024-04-30T20:04:31.380005Z",
     "iopub.status.idle": "2024-04-30T20:05:17.960476Z",
     "shell.execute_reply": "2024-04-30T20:05:17.959234Z",
     "shell.execute_reply.started": "2024-04-30T20:04:31.381241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-04-30 20:04:39.995254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.80s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 1536,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.2,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T18:59:25.111936Z",
     "iopub.status.busy": "2024-04-30T18:59:25.110751Z",
     "iopub.status.idle": "2024-04-30T19:10:02.468078Z",
     "shell.execute_reply": "2024-04-30T19:10:02.466765Z",
     "shell.execute_reply.started": "2024-04-30T18:59:25.111884Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почему трава зеленая?\n",
      "Вопрос о цвете травы зависит от многих факторов, включая вид растения, условия окружающей среды, время года и т. д. Однако основной принцип заключается в том, что трава обычно зелёная из-за наличия хлорофилла.\n",
      "\n",
      "Хлорофилл - это пигмент, который находится в клетках растений и используется для фотосинтеза. Фотосинтез - это процесс, благодаря которому растения превращают солнечный свет в энергию, используя углекислый газ из воздуха и воду из почвы. Хлорофилл поглощает световые волны, которые имеют длину 430-450 нм (синий цвет) и 680-720 нм (красный цвет). Эти волны поглощаются и перерабатываются в энергию, которая используется для синтеза глюкозы - основного источника энергии для растений.\n",
      "\n",
      "Зеленый цвет травы обусловлен тем, что хлорофилл имеет максимальную поглощенную длину волны около 510-560 нм, что соответствует зеленому цвету. В то же время, другие пигменты, такие как каротин и антоцианы, также могут присутствовать в растениях и влиять на их цвет.\n",
      "\n",
      "Таким образом, трава обычно зелена из-за наличия хлорофилла, который поглощает световые волны, которые имеет зеленый цвет, и использует их для фотосинтеза.\n",
      "\n",
      "==============================\n",
      "\n",
      "Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\n",
      "Таня была маленькой девочкой, которая любила играть в футбол. Она была очень хорошей игроком и всегда стремилась стать лучшим. Её любимым объектом для тренировки был мяч.\n",
      "\n",
      "Однажды Таня решила провести собственное исследование, чтобы понять, как можно улучшить свои навыки. Она начала изучать разные виды мячей, которые она могла использовать для тренировок. В результате ей удалось найти несколько интересных вариантов.\n",
      "\n",
      "Первый мяч был очень тяжелым и имел форму куба. Он был чрезвычайно сложным для контроля, но при этом предоставлял возможность развивать силу ног. Таня решила, что этот мяч будет идеальным для тренировок на открытом воздухе.\n",
      "\n",
      "Второй мяч был очень легким и имел форму шара. Он был идеальным для игры в закрытых помещениях, так как его легко можно было перемещать. Таня решила, что этот мяч буdeт идеальным для тренировок в гимназии.\n",
      "\n",
      "Третий мяч был очень мелким и имел форму кольца. Он был идеальным для игры в мини-футбол или пинбол. Таня решила, что этот мяч будут идеальным для тренировок в парке.\n",
      "\n",
      "Четвертый мяч был очень большим и имел форму водного балла. Он был идеальным для игры в воде. Таня решила, что этот мяч будь идеальным для тренировок в бассейне.\n",
      "\n",
      "Таня решила, что каждый из этих мячей имеет свои достоинства и недостатки, и она должна использовать их в зависимости от своей текущей тренировки. Она решила, что каждый из этих мяцев может быть идеальным для тренировок в зависимости от своей текущей тренинги. Она решила, что каждый из этим мячей может быть идеальным для тренировок, в зависимости от своей текущей тренерской программы.\n",
      "\n",
      "Таня начала использовать эти мячи для тренировок и уже через некоторое время заметила, что ее навыки значительно улучшились. Она стала лучше контролировать мяч, быстрее реагировать на движения противников и эффективнее использовать свои силы.\n",
      "\n",
      "Ее успехи не остались незамеченными, и скоро Таня стала известной как талантливая футболистка. Она начала получать предложения от различных клубов, но она решила продолжать учиться и развиваться.\n",
      "\n",
      "Однажды, когда Таня играла в футбол на улице, к ней подбежала группа мальчиков и предложила ей играть в команде. Таня радовались и приняла их предложение. Они стали друзьями и вместе играли в футбол, обмениваясь опытом и помогая друг другу развиваться.\n",
      "\n",
      "Итак, история Тани и ее мячей - это история о том, как одному маленькому человеку удалось достичь своих целей благодаря желанию и трудолюбию. Именно благодаря своим мячам Таня смогла стать лучшим игроком и наслаждаться игрой, которую она любила.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\",\n",
    ")\n",
    "\n",
    "inputs = [\"Почему трава зеленая?\", \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\"]\n",
    "for inp in inputs:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "    \n",
    "    # print(model(prompt))\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    print(inp)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:05:31.419571Z",
     "iopub.status.busy": "2024-04-30T20:05:31.418827Z",
     "iopub.status.idle": "2024-04-30T20:05:31.446838Z",
     "shell.execute_reply": "2024-04-30T20:05:31.445663Z",
     "shell.execute_reply.started": "2024-04-30T20:05:31.419531Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:55:14.008627Z",
     "iopub.status.busy": "2024-04-30T19:55:14.007967Z",
     "iopub.status.idle": "2024-04-30T19:55:17.832013Z",
     "shell.execute_reply": "2024-04-30T19:55:17.830819Z",
     "shell.execute_reply.started": "2024-04-30T19:55:14.008587Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.82 s, sys: 51 ms, total: 3.87 s\n",
      "Wall time: 3.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C 4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"\"\"Задание содержит вопрос по теме Математика и 4 варианта ответа A, B, C, D, из которых только один правильный. Выберите букву правильного ответа:\n",
    "Чему равен корень из 144?\n",
    "A 14\n",
    "B 12\n",
    "C 4\n",
    "D 44\n",
    "Ответ:\"\"\"\n",
    "generate(model, tokenizer, prompt, generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:47:14.507918Z",
     "iopub.status.busy": "2024-04-30T19:47:14.506933Z",
     "iopub.status.idle": "2024-04-30T19:47:14.524735Z",
     "shell.execute_reply": "2024-04-30T19:47:14.523652Z",
     "shell.execute_reply.started": "2024-04-30T19:47:14.507868Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_new_tokens\": 1536,\n",
       "  \"no_repeat_ngram_size\": 15,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.2,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:55:42.510130Z",
     "iopub.status.busy": "2024-04-30T19:55:42.508995Z",
     "iopub.status.idle": "2024-04-30T19:55:43.311393Z",
     "shell.execute_reply": "2024-04-30T19:55:43.310330Z",
     "shell.execute_reply.started": "2024-04-30T19:55:42.510078Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 94, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 774 ms, sys: 5.22 ms, total: 779 ms\n",
      "Wall time: 777 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"\"\"Задание содержит вопрос по теме Математика и 4 варианта ответа A, B, C, D, из которых только один правильный. Выберите букву правильного ответа:\n",
    "Чему равен корень из 144?\n",
    "A 14\n",
    "B 12\n",
    "C 45\n",
    "D 44\n",
    "Ответ:\"\"\"\n",
    "data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "data = {k: v.to(model.device) for k, v in data.items()}\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **data,\n",
    "        # generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:55:45.488807Z",
     "iopub.status.busy": "2024-04-30T19:55:45.487894Z",
     "iopub.status.idle": "2024-04-30T19:55:45.505526Z",
     "shell.execute_reply": "2024-04-30T19:55:45.504371Z",
     "shell.execute_reply.started": "2024-04-30T19:55:45.488774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:47:20.056090Z",
     "iopub.status.busy": "2024-04-30T19:47:20.055137Z",
     "iopub.status.idle": "2024-04-30T19:47:20.095786Z",
     "shell.execute_reply": "2024-04-30T19:47:20.094728Z",
     "shell.execute_reply.started": "2024-04-30T19:47:20.056041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([94])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sequences[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:55:49.092271Z",
     "iopub.status.busy": "2024-04-30T19:55:49.091399Z",
     "iopub.status.idle": "2024-04-30T19:55:49.130653Z",
     "shell.execute_reply": "2024-04-30T19:55:49.129475Z",
     "shell.execute_reply.started": "2024-04-30T19:55:49.092238Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.8750, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.scores[0][0][330]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:50:11.422728Z",
     "iopub.status.busy": "2024-04-30T19:50:11.421331Z",
     "iopub.status.idle": "2024-04-30T19:50:11.449500Z",
     "shell.execute_reply": "2024-04-30T19:50:11.448416Z",
     "shell.execute_reply.started": "2024-04-30T19:50:11.422685Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[330, 365, 334, 384]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A': 12.875, 'B': 12.875, 'C': 12.5, 'D': 14.125}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokens_of_interest = [\n",
    "    tokenizer(\"A\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\"B\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\"C\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\"D\", add_special_tokens=False).input_ids[-1],\n",
    "]\n",
    "print(tokens_of_interest)\n",
    "\n",
    "probs = [output.scores[0][0][token_id].item() for token_id in tokens_of_interest]\n",
    "probs\n",
    "res = dict(zip([\"A\", \"B\", \"C\", \"D\"], probs))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:43:29.649386Z",
     "iopub.status.busy": "2024-04-30T19:43:29.648299Z",
     "iopub.status.idle": "2024-04-30T19:43:29.682263Z",
     "shell.execute_reply": "2024-04-30T19:43:29.680582Z",
     "shell.execute_reply.started": "2024-04-30T19:43:29.649328Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-4b11b9503a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "output.scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:35:43.464577Z",
     "iopub.status.busy": "2024-04-30T19:35:43.463569Z",
     "iopub.status.idle": "2024-04-30T19:35:43.485242Z",
     "shell.execute_reply": "2024-04-30T19:35:43.484114Z",
     "shell.execute_reply.started": "2024-04-30T19:35:43.464540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:50:07.368375Z",
     "iopub.status.busy": "2024-04-30T19:50:07.367118Z",
     "iopub.status.idle": "2024-04-30T19:50:07.392200Z",
     "shell.execute_reply": "2024-04-30T19:50:07.391104Z",
     "shell.execute_reply.started": "2024-04-30T19:50:07.368320Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GreedySearchDecoderOnlyOutput(sequences=tensor([[ 6369,  1225,  2676,  2573, 11078,  3139, 28786,  2890,  8378, 28788,\n",
      "          1051,  3882,  2084,  5564,  1078, 15481,   917,   839, 28705, 28781,\n",
      "         13524,   892,  2239,   946,  2433, 17835,   330, 28725,   365, 28725,\n",
      "           334, 28725,   384, 28725,  2879, 28202, 24125, 24193, 18534,  1049,\n",
      "          4086, 28723, 24311,  5744,   892,  1078,  5213, 28795,  6725, 18534,\n",
      "         22821,  2433, 17835, 28747,    13, 28909, 28773,  2953,  2101, 16227,\n",
      "          1619,   800,  7934,  2879, 28705, 28740, 28781, 28781, 28804,    13,\n",
      "         28741, 28705, 28740, 28781,    13, 28760, 28705, 28740, 28750,    13,\n",
      "         28743, 28705, 28781, 28782,    13, 28757, 28705, 28781, 28781,    13,\n",
      "         28874, 28786,  8496, 28747,   384]], device='cuda:0'), scores=(tensor([[ -7.8125,  -7.2188,   8.4375,  ...,  -2.1250, -10.0000, -11.8750]],\n",
      "       device='cuda:0'),), attentions=None, hidden_states=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(output), output)\n",
    "output_ids = output.sequences[0][len(data[\"input_ids\"][0]):]\n",
    "output_str = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:25:29.292456Z",
     "iopub.status.busy": "2024-04-30T19:25:29.291545Z",
     "iopub.status.idle": "2024-04-30T19:25:29.358078Z",
     "shell.execute_reply": "2024-04-30T19:25:29.356865Z",
     "shell.execute_reply.started": "2024-04-30T19:25:29.292424Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-313eaea31a4c>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = torch.nn.functional.softmax(transition_scores[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.nn.functional.softmax(transition_scores[0])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:05:17.963425Z",
     "iopub.status.busy": "2024-04-30T20:05:17.962352Z",
     "iopub.status.idle": "2024-04-30T20:05:17.985182Z",
     "shell.execute_reply": "2024-04-30T20:05:17.983733Z",
     "shell.execute_reply.started": "2024-04-30T20:05:17.963388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_next_token_probs(q, tokenizer, model):\n",
    "    data = tokenizer(q, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **data,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "    \n",
    "    tokens_of_interest = [\n",
    "        tokenizer(\"A\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"B\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"C\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"D\", add_special_tokens=False).input_ids[-1],\n",
    "    ]\n",
    "\n",
    "    probs = [output.scores[0][0][token_id].item() for token_id in tokens_of_interest]\n",
    "    res = dict(zip([\"A\", \"B\", \"C\", \"D\"], probs))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:05:55.126534Z",
     "iopub.status.busy": "2024-04-30T20:05:55.125065Z",
     "iopub.status.idle": "2024-04-30T20:06:02.319708Z",
     "shell.execute_reply": "2024-04-30T20:06:02.318461Z",
     "shell.execute_reply.started": "2024-04-30T20:05:55.126477Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 5.15625, 'B': 3.953125, 'C': 7.40625, 'D': 4.3125}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = calculate_next_token_probs(\"A B \", tokenizer, model)\n",
    "print(probs)\n",
    "get_answer(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutate ru metrics on Saiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:06:26.373062Z",
     "iopub.status.busy": "2024-04-30T20:06:26.371887Z",
     "iopub.status.idle": "2024-04-30T20:06:26.411079Z",
     "shell.execute_reply": "2024-04-30T20:06:26.409826Z",
     "shell.execute_reply.started": "2024-04-30T20:06:26.373027Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:06:32.836373Z",
     "iopub.status.busy": "2024-04-30T20:06:32.835584Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets...:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Splits...: 0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "Rows...:   0%|          | 0/10033 [00:00<?, ?it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 246, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 1/10033 [00:01<2:57:24,  1.06s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 235, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 2/10033 [00:02<2:52:46,  1.03s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 300, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 3/10033 [00:03<2:58:14,  1.07s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 276, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 4/10033 [00:04<2:59:04,  1.07s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 251, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 5/10033 [00:05<2:54:55,  1.05s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 238, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 6/10033 [00:06<2:52:13,  1.03s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 152, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 7/10033 [00:07<2:44:39,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 138, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 8/10033 [00:08<2:40:00,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 165, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 9/10033 [00:08<2:37:15,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 251, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 10/10033 [00:09<2:40:35,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 108, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 11/10033 [00:10<2:32:15,  1.10it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 159, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 12/10033 [00:11<2:31:14,  1.10it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 289, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 13/10033 [00:12<2:40:54,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 167, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 14/10033 [00:13<2:37:31,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 149, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 15/10033 [00:14<2:34:38,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 164, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 16/10033 [00:15<2:33:10,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 145, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 17/10033 [00:16<2:31:23,  1.10it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 204, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 18/10033 [00:17<2:34:54,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 225, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 19/10033 [00:18<2:39:12,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 198, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 20/10033 [00:19<2:40:22,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 223, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 21/10033 [00:20<2:41:50,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 110, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 22/10033 [00:21<2:33:24,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 136, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 23/10033 [00:21<2:32:07,  1.10it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 207, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 24/10033 [00:22<2:36:39,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 239, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 25/10033 [00:23<2:39:37,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 209, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 26/10033 [00:24<2:41:02,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 238, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 27/10033 [00:25<2:42:57,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 302, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 28/10033 [00:27<2:48:49,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 369, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 29/10033 [00:28<3:05:03,  1.11s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 177, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 30/10033 [00:29<2:55:04,  1.05s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 119, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 31/10033 [00:30<2:42:54,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 139, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 32/10033 [00:31<2:38:35,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 217, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 33/10033 [00:31<2:40:36,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 165, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 34/10033 [00:32<2:38:00,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 133, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 35/10033 [00:33<2:35:05,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 131, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 36/10033 [00:34<2:32:53,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 116, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 37/10033 [00:35<2:26:52,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 253, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 38/10033 [00:36<2:33:23,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 211, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 39/10033 [00:37<2:37:03,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 261, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 40/10033 [00:38<2:44:11,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 520, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 41/10033 [00:40<3:15:41,  1.18s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 217, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 42/10033 [00:41<3:05:58,  1.12s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 327, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 43/10033 [00:42<3:15:14,  1.17s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 199, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 44/10033 [00:43<3:06:39,  1.12s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 150, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 45/10033 [00:44<2:55:01,  1.05s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 232, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 46/10033 [00:45<2:52:34,  1.04s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 174, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 47/10033 [00:46<2:45:54,  1.00it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 183, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 48/10033 [00:47<2:41:28,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 309, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 49/10033 [00:48<2:47:55,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 142, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 50/10033 [00:49<2:41:43,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 122, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 51/10033 [00:49<2:33:29,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 117, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 52/10033 [00:50<2:27:15,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 147, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 53/10033 [00:51<2:27:23,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 244, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 54/10033 [00:52<2:33:39,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 132, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 55/10033 [00:53<2:32:30,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 127, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 56/10033 [00:54<2:27:13,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 173, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 57/10033 [00:55<2:28:03,  1.12it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 197, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 58/10033 [00:56<2:32:52,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 299, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 59/10033 [00:57<2:41:26,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 162, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 60/10033 [00:58<2:37:37,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 333, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 61/10033 [00:59<2:56:11,  1.06s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 295, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 62/10033 [01:00<2:57:40,  1.07s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 207, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 63/10033 [01:01<2:53:42,  1.05s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 203, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for name, dataset_meta in tqdm(HUGGINGFACE_NAME_TO_DATASET.items(), desc=\"Datasets...\"):\n",
    "        for subset, split in tqdm(zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]), desc=\"Splits...\"):\n",
    "            path = get_dataset_path(subset, name, split)\n",
    "            dataset = load_from_disk(path)\n",
    "            probs_list = []\n",
    "            a_list = []\n",
    "            for row in tqdm(dataset, desc=\"Rows...\"):\n",
    "                q = construct_prompt(row)\n",
    "                probs = calculate_next_token_probs(q, tokenizer, model)\n",
    "                probs_list.append({\n",
    "                    \"probs\": probs,\n",
    "                    \"meta\": row[\"meta\"],\n",
    "                })\n",
    "                a = get_answer(probs)\n",
    "                a_list.append({\n",
    "                    \"answer\": a,\n",
    "                    \"meta\": row[\"meta\"],\n",
    "                })\n",
    "            metric_dir_path = get_metric_dir_path(SAIGA_MISTRAL_7B_LORA, subset, name, split)\n",
    "            metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "            with open(metric_dir_path.joinpath(\"probs.jsonl\"), \"w\") as f:\n",
    "                json.dump(probs_list, f, ensure_ascii=False, indent=2)\n",
    "            with open(metric_dir_path.joinpath(\"answers.jsonl\"), \"w\") as f:\n",
    "                json.dump(a_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
