{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:05:51.144952Z",
     "iopub.status.busy": "2024-04-30T20:05:51.144162Z",
     "iopub.status.idle": "2024-04-30T20:05:51.398124Z",
     "shell.execute_reply": "2024-04-30T20:05:51.397015Z",
     "shell.execute_reply.started": "2024-04-30T20:05:51.144918Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../utils\")\n",
    "from definitions import *\n",
    "from path_helpers import get_dataset_path, get_metric_dir_path\n",
    "from mera_helpers import construct_prompt\n",
    "from llm_helpers import get_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Try to run Saiga "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T18:44:45.456825Z",
     "iopub.status.busy": "2024-04-30T18:44:45.455903Z",
     "iopub.status.idle": "2024-04-30T18:44:55.491780Z",
     "shell.execute_reply": "2024-04-30T18:44:55.490352Z",
     "shell.execute_reply.started": "2024-04-30T18:44:45.456774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate==0.21.0 in /home/jupyter/.local/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: bitsandbytes==0.40.2 in /home/jupyter/.local/lib/python3.10/site-packages (0.40.2)\n",
      "Requirement already satisfied: peft==0.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: transformers==4.34.0 in /home/jupyter/.local/lib/python3.10/site-packages (4.34.0)\n",
      "Requirement already satisfied: sentencepiece in /home/jupyter/.local/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jupyter/.local/lib/python3.10/site-packages (from accelerate==0.21.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.10/site-packages (from accelerate==0.21.0) (24.0)\n",
      "Requirement already satisfied: psutil in /kernel/lib/python3.10/site-packages (from accelerate==0.21.0) (5.7.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.21.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/jupyter/.local/lib/python3.10/site-packages (from accelerate==0.21.0) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.5.0) (4.65.0)\n",
      "Requirement already satisfied: safetensors in /home/jupyter/.local/lib/python3.10/site-packages (from peft==0.5.0) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers==4.34.0) (0.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (2022.10.31)\n",
      "Requirement already satisfied: requests in /kernel/lib/python3.10/site-packages (from transformers==4.34.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers==4.34.0) (0.14.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (4.11.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0)\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /kernel/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/jupyter/.local/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.21.0) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jupyter/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.21.0) (12.4.127)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /kernel/lib/python3.10/site-packages (from requests->transformers==4.34.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests->transformers==4.34.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /kernel/lib/python3.10/site-packages (from requests->transformers==4.34.0) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests->transformers==4.34.0) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.21.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.21.0) (1.3.0)\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.22.2\n",
      "    Uninstalling huggingface-hub-0.22.2:\n",
      "      Successfully uninstalled huggingface-hub-0.22.2\n",
      "\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 2.19.0 requires huggingface-hub>=0.21.2, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "diffusers 0.27.2 requires huggingface-hub>=0.20.2, but you have huggingface-hub 0.17.3 which is incompatible.\n",
      "lm-eval 0.3.0 requires transformers>=4.36.2, but you have transformers 4.34.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.17.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade accelerate==0.21.0 \\\n",
    "  bitsandbytes==0.40.2 \\\n",
    "  peft==0.5.0 \\\n",
    "  transformers==4.34.0 \\\n",
    "  sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:04:31.381296Z",
     "iopub.status.busy": "2024-04-30T20:04:31.380005Z",
     "iopub.status.idle": "2024-04-30T20:05:17.960476Z",
     "shell.execute_reply": "2024-04-30T20:05:17.959234Z",
     "shell.execute_reply.started": "2024-04-30T20:04:31.381241Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "2024-04-30 20:04:39.995254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:17<00:00,  8.80s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_new_tokens\": 1536,\n",
      "  \"no_repeat_ngram_size\": 15,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.2,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        generation_config=generation_config\n",
    "    )[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T18:59:25.111936Z",
     "iopub.status.busy": "2024-04-30T18:59:25.110751Z",
     "iopub.status.idle": "2024-04-30T19:10:02.468078Z",
     "shell.execute_reply": "2024-04-30T19:10:02.466765Z",
     "shell.execute_reply.started": "2024-04-30T18:59:25.111884Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почему трава зеленая?\n",
      "Вопрос о цвете травы зависит от многих факторов, включая вид растения, условия окружающей среды, время года и т. д. Однако основной принцип заключается в том, что трава обычно зелёная из-за наличия хлорофилла.\n",
      "\n",
      "Хлорофилл - это пигмент, который находится в клетках растений и используется для фотосинтеза. Фотосинтез - это процесс, благодаря которому растения превращают солнечный свет в энергию, используя углекислый газ из воздуха и воду из почвы. Хлорофилл поглощает световые волны, которые имеют длину 430-450 нм (синий цвет) и 680-720 нм (красный цвет). Эти волны поглощаются и перерабатываются в энергию, которая используется для синтеза глюкозы - основного источника энергии для растений.\n",
      "\n",
      "Зеленый цвет травы обусловлен тем, что хлорофилл имеет максимальную поглощенную длину волны около 510-560 нм, что соответствует зеленому цвету. В то же время, другие пигменты, такие как каротин и антоцианы, также могут присутствовать в растениях и влиять на их цвет.\n",
      "\n",
      "Таким образом, трава обычно зелена из-за наличия хлорофилла, который поглощает световые волны, которые имеет зеленый цвет, и использует их для фотосинтеза.\n",
      "\n",
      "==============================\n",
      "\n",
      "Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\n",
      "Таня была маленькой девочкой, которая любила играть в футбол. Она была очень хорошей игроком и всегда стремилась стать лучшим. Её любимым объектом для тренировки был мяч.\n",
      "\n",
      "Однажды Таня решила провести собственное исследование, чтобы понять, как можно улучшить свои навыки. Она начала изучать разные виды мячей, которые она могла использовать для тренировок. В результате ей удалось найти несколько интересных вариантов.\n",
      "\n",
      "Первый мяч был очень тяжелым и имел форму куба. Он был чрезвычайно сложным для контроля, но при этом предоставлял возможность развивать силу ног. Таня решила, что этот мяч будет идеальным для тренировок на открытом воздухе.\n",
      "\n",
      "Второй мяч был очень легким и имел форму шара. Он был идеальным для игры в закрытых помещениях, так как его легко можно было перемещать. Таня решила, что этот мяч буdeт идеальным для тренировок в гимназии.\n",
      "\n",
      "Третий мяч был очень мелким и имел форму кольца. Он был идеальным для игры в мини-футбол или пинбол. Таня решила, что этот мяч будут идеальным для тренировок в парке.\n",
      "\n",
      "Четвертый мяч был очень большим и имел форму водного балла. Он был идеальным для игры в воде. Таня решила, что этот мяч будь идеальным для тренировок в бассейне.\n",
      "\n",
      "Таня решила, что каждый из этих мячей имеет свои достоинства и недостатки, и она должна использовать их в зависимости от своей текущей тренировки. Она решила, что каждый из этих мяцев может быть идеальным для тренировок в зависимости от своей текущей тренинги. Она решила, что каждый из этим мячей может быть идеальным для тренировок, в зависимости от своей текущей тренерской программы.\n",
      "\n",
      "Таня начала использовать эти мячи для тренировок и уже через некоторое время заметила, что ее навыки значительно улучшились. Она стала лучше контролировать мяч, быстрее реагировать на движения противников и эффективнее использовать свои силы.\n",
      "\n",
      "Ее успехи не остались незамеченными, и скоро Таня стала известной как талантливая футболистка. Она начала получать предложения от различных клубов, но она решила продолжать учиться и развиваться.\n",
      "\n",
      "Однажды, когда Таня играла в футбол на улице, к ней подбежала группа мальчиков и предложила ей играть в команде. Таня радовались и приняла их предложение. Они стали друзьями и вместе играли в футбол, обмениваясь опытом и помогая друг другу развиваться.\n",
      "\n",
      "Итак, история Тани и ее мячей - это история о том, как одному маленькому человеку удалось достичь своих целей благодаря желанию и трудолюбию. Именно благодаря своим мячам Таня смогла стать лучшим игроком и наслаждаться игрой, которую она любила.\n",
      "\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\",\n",
    ")\n",
    "\n",
    "inputs = [\"Почему трава зеленая?\", \"Сочини длинный рассказ, обязательно упоминая следующие объекты. Дано: Таня, мяч\"]\n",
    "for inp in inputs:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "    \n",
    "    # print(model(prompt))\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    print(inp)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:05:31.419571Z",
     "iopub.status.busy": "2024-04-30T20:05:31.418827Z",
     "iopub.status.idle": "2024-04-30T20:05:31.446838Z",
     "shell.execute_reply": "2024-04-30T20:05:31.445663Z",
     "shell.execute_reply.started": "2024-04-30T20:05:31.419531Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:55:14.008627Z",
     "iopub.status.busy": "2024-04-30T19:55:14.007967Z",
     "iopub.status.idle": "2024-04-30T19:55:17.832013Z",
     "shell.execute_reply": "2024-04-30T19:55:17.830819Z",
     "shell.execute_reply.started": "2024-04-30T19:55:14.008587Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.82 s, sys: 51 ms, total: 3.87 s\n",
      "Wall time: 3.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C 4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"\"\"Задание содержит вопрос по теме Математика и 4 варианта ответа A, B, C, D, из которых только один правильный. Выберите букву правильного ответа:\n",
    "Чему равен корень из 144?\n",
    "A 14\n",
    "B 12\n",
    "C 4\n",
    "D 44\n",
    "Ответ:\"\"\"\n",
    "generate(model, tokenizer, prompt, generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:47:14.507918Z",
     "iopub.status.busy": "2024-04-30T19:47:14.506933Z",
     "iopub.status.idle": "2024-04-30T19:47:14.524735Z",
     "shell.execute_reply": "2024-04-30T19:47:14.523652Z",
     "shell.execute_reply.started": "2024-04-30T19:47:14.507868Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_new_tokens\": 1536,\n",
       "  \"no_repeat_ngram_size\": 15,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.2,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:55:42.510130Z",
     "iopub.status.busy": "2024-04-30T19:55:42.508995Z",
     "iopub.status.idle": "2024-04-30T19:55:43.311393Z",
     "shell.execute_reply": "2024-04-30T19:55:43.310330Z",
     "shell.execute_reply.started": "2024-04-30T19:55:42.510078Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 94, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 774 ms, sys: 5.22 ms, total: 779 ms\n",
      "Wall time: 777 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prompt = \"\"\"Задание содержит вопрос по теме Математика и 4 варианта ответа A, B, C, D, из которых только один правильный. Выберите букву правильного ответа:\n",
    "Чему равен корень из 144?\n",
    "A 14\n",
    "B 12\n",
    "C 45\n",
    "D 44\n",
    "Ответ:\"\"\"\n",
    "data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "data = {k: v.to(model.device) for k, v in data.items()}\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **data,\n",
    "        # generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:55:45.488807Z",
     "iopub.status.busy": "2024-04-30T19:55:45.487894Z",
     "iopub.status.idle": "2024-04-30T19:55:45.505526Z",
     "shell.execute_reply": "2024-04-30T19:55:45.504371Z",
     "shell.execute_reply.started": "2024-04-30T19:55:45.488774Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:47:20.056090Z",
     "iopub.status.busy": "2024-04-30T19:47:20.055137Z",
     "iopub.status.idle": "2024-04-30T19:47:20.095786Z",
     "shell.execute_reply": "2024-04-30T19:47:20.094728Z",
     "shell.execute_reply.started": "2024-04-30T19:47:20.056041Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([94])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.sequences[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:55:49.092271Z",
     "iopub.status.busy": "2024-04-30T19:55:49.091399Z",
     "iopub.status.idle": "2024-04-30T19:55:49.130653Z",
     "shell.execute_reply": "2024-04-30T19:55:49.129475Z",
     "shell.execute_reply.started": "2024-04-30T19:55:49.092238Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.8750, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.scores[0][0][330]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:50:11.422728Z",
     "iopub.status.busy": "2024-04-30T19:50:11.421331Z",
     "iopub.status.idle": "2024-04-30T19:50:11.449500Z",
     "shell.execute_reply": "2024-04-30T19:50:11.448416Z",
     "shell.execute_reply.started": "2024-04-30T19:50:11.422685Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[330, 365, 334, 384]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'A': 12.875, 'B': 12.875, 'C': 12.5, 'D': 14.125}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokens_of_interest = [\n",
    "    tokenizer(\"A\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\"B\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\"C\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\"D\", add_special_tokens=False).input_ids[-1],\n",
    "]\n",
    "print(tokens_of_interest)\n",
    "\n",
    "probs = [output.scores[0][0][token_id].item() for token_id in tokens_of_interest]\n",
    "probs\n",
    "res = dict(zip([\"A\", \"B\", \"C\", \"D\"], probs))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:43:29.649386Z",
     "iopub.status.busy": "2024-04-30T19:43:29.648299Z",
     "iopub.status.idle": "2024-04-30T19:43:29.682263Z",
     "shell.execute_reply": "2024-04-30T19:43:29.680582Z",
     "shell.execute_reply.started": "2024-04-30T19:43:29.649328Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-4b11b9503a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "output.scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:35:43.464577Z",
     "iopub.status.busy": "2024-04-30T19:35:43.463569Z",
     "iopub.status.idle": "2024-04-30T19:35:43.485242Z",
     "shell.execute_reply": "2024-04-30T19:35:43.484114Z",
     "shell.execute_reply.started": "2024-04-30T19:35:43.464540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:50:07.368375Z",
     "iopub.status.busy": "2024-04-30T19:50:07.367118Z",
     "iopub.status.idle": "2024-04-30T19:50:07.392200Z",
     "shell.execute_reply": "2024-04-30T19:50:07.391104Z",
     "shell.execute_reply.started": "2024-04-30T19:50:07.368320Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 GreedySearchDecoderOnlyOutput(sequences=tensor([[ 6369,  1225,  2676,  2573, 11078,  3139, 28786,  2890,  8378, 28788,\n",
      "          1051,  3882,  2084,  5564,  1078, 15481,   917,   839, 28705, 28781,\n",
      "         13524,   892,  2239,   946,  2433, 17835,   330, 28725,   365, 28725,\n",
      "           334, 28725,   384, 28725,  2879, 28202, 24125, 24193, 18534,  1049,\n",
      "          4086, 28723, 24311,  5744,   892,  1078,  5213, 28795,  6725, 18534,\n",
      "         22821,  2433, 17835, 28747,    13, 28909, 28773,  2953,  2101, 16227,\n",
      "          1619,   800,  7934,  2879, 28705, 28740, 28781, 28781, 28804,    13,\n",
      "         28741, 28705, 28740, 28781,    13, 28760, 28705, 28740, 28750,    13,\n",
      "         28743, 28705, 28781, 28782,    13, 28757, 28705, 28781, 28781,    13,\n",
      "         28874, 28786,  8496, 28747,   384]], device='cuda:0'), scores=(tensor([[ -7.8125,  -7.2188,   8.4375,  ...,  -2.1250, -10.0000, -11.8750]],\n",
      "       device='cuda:0'),), attentions=None, hidden_states=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(output), output)\n",
    "output_ids = output.sequences[0][len(data[\"input_ids\"][0]):]\n",
    "output_str = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T19:25:29.292456Z",
     "iopub.status.busy": "2024-04-30T19:25:29.291545Z",
     "iopub.status.idle": "2024-04-30T19:25:29.358078Z",
     "shell.execute_reply": "2024-04-30T19:25:29.356865Z",
     "shell.execute_reply.started": "2024-04-30T19:25:29.292424Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-313eaea31a4c>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = torch.nn.functional.softmax(transition_scores[0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = torch.nn.functional.softmax(transition_scores[0])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper inference functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:05:17.963425Z",
     "iopub.status.busy": "2024-04-30T20:05:17.962352Z",
     "iopub.status.idle": "2024-04-30T20:05:17.985182Z",
     "shell.execute_reply": "2024-04-30T20:05:17.983733Z",
     "shell.execute_reply.started": "2024-04-30T20:05:17.963388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_next_token_probs(q, tokenizer, model):\n",
    "    data = tokenizer(q, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **data,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "    \n",
    "    tokens_of_interest = [\n",
    "        tokenizer(\"A\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"B\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"C\", add_special_tokens=False).input_ids[-1],\n",
    "        tokenizer(\"D\", add_special_tokens=False).input_ids[-1],\n",
    "    ]\n",
    "\n",
    "    probs = [output.scores[0][0][token_id].item() for token_id in tokens_of_interest]\n",
    "    res = dict(zip([\"A\", \"B\", \"C\", \"D\"], probs))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:05:55.126534Z",
     "iopub.status.busy": "2024-04-30T20:05:55.125065Z",
     "iopub.status.idle": "2024-04-30T20:06:02.319708Z",
     "shell.execute_reply": "2024-04-30T20:06:02.318461Z",
     "shell.execute_reply.started": "2024-04-30T20:05:55.126477Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 5.15625, 'B': 3.953125, 'C': 7.40625, 'D': 4.3125}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = calculate_next_token_probs(\"A B \", tokenizer, model)\n",
    "print(probs)\n",
    "get_answer(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalutate ru metrics on Saiga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:06:26.373062Z",
     "iopub.status.busy": "2024-04-30T20:06:26.371887Z",
     "iopub.status.idle": "2024-04-30T20:06:26.411079Z",
     "shell.execute_reply": "2024-04-30T20:06:26.409826Z",
     "shell.execute_reply.started": "2024-04-30T20:06:26.373027Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T20:06:32.836373Z",
     "iopub.status.busy": "2024-04-30T20:06:32.835584Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets...:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Splits...: 0it [00:00, ?it/s]\u001b[A\n",
      "\n",
      "Rows...:   0%|          | 0/10033 [00:00<?, ?it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 246, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 1/10033 [00:01<2:57:24,  1.06s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 235, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 2/10033 [00:02<2:52:46,  1.03s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 300, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 3/10033 [00:03<2:58:14,  1.07s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 276, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 4/10033 [00:04<2:59:04,  1.07s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 251, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 5/10033 [00:05<2:54:55,  1.05s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 238, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 6/10033 [00:06<2:52:13,  1.03s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 152, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 7/10033 [00:07<2:44:39,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 138, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 8/10033 [00:08<2:40:00,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 165, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 9/10033 [00:08<2:37:15,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 251, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 10/10033 [00:09<2:40:35,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 108, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 11/10033 [00:10<2:32:15,  1.10it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 159, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 12/10033 [00:11<2:31:14,  1.10it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 289, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 13/10033 [00:12<2:40:54,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 167, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 14/10033 [00:13<2:37:31,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 149, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 15/10033 [00:14<2:34:38,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 164, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 16/10033 [00:15<2:33:10,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 145, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 17/10033 [00:16<2:31:23,  1.10it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 204, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 18/10033 [00:17<2:34:54,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 225, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 19/10033 [00:18<2:39:12,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 198, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 20/10033 [00:19<2:40:22,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 223, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 21/10033 [00:20<2:41:50,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 110, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 22/10033 [00:21<2:33:24,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 136, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 23/10033 [00:21<2:32:07,  1.10it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 207, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 24/10033 [00:22<2:36:39,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 239, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 25/10033 [00:23<2:39:37,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 209, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 26/10033 [00:24<2:41:02,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 238, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 27/10033 [00:25<2:42:57,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 302, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 28/10033 [00:27<2:48:49,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 369, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 29/10033 [00:28<3:05:03,  1.11s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 177, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 30/10033 [00:29<2:55:04,  1.05s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 119, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 31/10033 [00:30<2:42:54,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 139, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 32/10033 [00:31<2:38:35,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 217, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 33/10033 [00:31<2:40:36,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 165, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 34/10033 [00:32<2:38:00,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 133, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 35/10033 [00:33<2:35:05,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 131, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 36/10033 [00:34<2:32:53,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 116, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 37/10033 [00:35<2:26:52,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 253, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 38/10033 [00:36<2:33:23,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 211, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 39/10033 [00:37<2:37:03,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 261, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 40/10033 [00:38<2:44:11,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 520, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 41/10033 [00:40<3:15:41,  1.18s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 217, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 42/10033 [00:41<3:05:58,  1.12s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 327, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 43/10033 [00:42<3:15:14,  1.17s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 199, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 44/10033 [00:43<3:06:39,  1.12s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 150, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 45/10033 [00:44<2:55:01,  1.05s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 232, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 46/10033 [00:45<2:52:34,  1.04s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 174, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 47/10033 [00:46<2:45:54,  1.00it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 183, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 48/10033 [00:47<2:41:28,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 309, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 49/10033 [00:48<2:47:55,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 142, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   0%|          | 50/10033 [00:49<2:41:43,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 122, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 51/10033 [00:49<2:33:29,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 117, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 52/10033 [00:50<2:27:15,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 147, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 53/10033 [00:51<2:27:23,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 244, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 54/10033 [00:52<2:33:39,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 132, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 55/10033 [00:53<2:32:30,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 127, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 56/10033 [00:54<2:27:13,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 173, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 57/10033 [00:55<2:28:03,  1.12it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 197, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 58/10033 [00:56<2:32:52,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 299, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 59/10033 [00:57<2:41:26,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 162, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 60/10033 [00:58<2:37:37,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 333, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 61/10033 [00:59<2:56:11,  1.06s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 295, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 62/10033 [01:00<2:57:40,  1.07s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 207, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 63/10033 [01:01<2:53:42,  1.05s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 203, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 64/10033 [01:02<2:51:07,  1.03s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 95, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 65/10033 [01:03<2:38:36,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 115, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 66/10033 [01:04<2:32:20,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 115, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 67/10033 [01:05<2:26:28,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 181, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 68/10033 [01:05<2:28:02,  1.12it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 145, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 69/10033 [01:06<2:28:13,  1.12it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 125, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 70/10033 [01:07<2:24:23,  1.15it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 121, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 71/10033 [01:08<2:21:51,  1.17it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 207, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 72/10033 [01:09<2:29:11,  1.11it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 288, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 73/10033 [01:10<2:38:32,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 201, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 74/10033 [01:11<2:40:13,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 121, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 75/10033 [01:12<2:32:11,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 257, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 76/10033 [01:13<2:39:46,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 234, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 77/10033 [01:14<2:42:11,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 171, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 78/10033 [01:15<2:38:56,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 170, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 79/10033 [01:16<2:36:40,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 227, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 80/10033 [01:17<2:39:24,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 228, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 81/10033 [01:18<2:41:33,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 135, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 82/10033 [01:19<2:37:41,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 195, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 83/10033 [01:20<2:39:36,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 151, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 84/10033 [01:21<2:36:24,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 170, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 85/10033 [01:21<2:34:13,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 137, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 86/10033 [01:22<2:32:46,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 204, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 87/10033 [01:23<2:36:30,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 172, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 88/10033 [01:24<2:36:53,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 149, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 89/10033 [01:25<2:34:57,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 113, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 90/10033 [01:26<2:29:05,  1.11it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 183, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 91/10033 [01:27<2:29:17,  1.11it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 173, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 92/10033 [01:28<2:29:43,  1.11it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 272, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 93/10033 [01:29<2:39:03,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 221, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 94/10033 [01:30<2:41:01,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 263, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 95/10033 [01:31<2:46:17,  1.00s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 162, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 96/10033 [01:32<2:41:16,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 436, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 97/10033 [01:33<3:02:44,  1.10s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 300, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 98/10033 [01:34<3:03:19,  1.11s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 242, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 99/10033 [01:35<2:58:13,  1.08s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 173, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 100/10033 [01:36<2:49:43,  1.03s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 168, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 101/10033 [01:37<2:43:53,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 172, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 102/10033 [01:38<2:39:53,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 211, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 103/10033 [01:39<2:42:00,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 288, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 104/10033 [01:40<2:47:13,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 127, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 105/10033 [01:41<2:37:43,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 153, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 106/10033 [01:42<2:35:04,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 188, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 107/10033 [01:43<2:33:49,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 247, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 108/10033 [01:44<2:38:02,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 147, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 109/10033 [01:45<2:36:17,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 160, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 110/10033 [01:46<2:34:25,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 225, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 111/10033 [01:47<2:37:57,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 260, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 112/10033 [01:48<2:44:02,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 197, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 113/10033 [01:49<2:43:46,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 229, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 114/10033 [01:50<2:45:05,  1.00it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 236, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 115/10033 [01:51<2:45:22,  1.00s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 208, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 116/10033 [01:52<2:44:53,  1.00it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 178, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 117/10033 [01:53<2:40:36,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 300, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 118/10033 [01:54<2:46:50,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 305, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 119/10033 [01:55<2:52:16,  1.04s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 235, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 120/10033 [01:56<2:50:20,  1.03s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 232, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 121/10033 [01:57<2:48:33,  1.02s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 151, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 122/10033 [01:58<2:42:09,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 264, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 123/10033 [01:59<2:47:07,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 315, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 124/10033 [02:00<2:52:11,  1.04s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 169, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|          | 125/10033 [02:01<2:45:14,  1.00s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 175, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 126/10033 [02:02<2:40:30,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 161, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 127/10033 [02:03<2:37:11,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 144, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 128/10033 [02:04<2:34:28,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 160, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 129/10033 [02:05<2:33:28,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 203, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 130/10033 [02:06<2:35:58,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 197, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 131/10033 [02:07<2:37:47,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 141, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 132/10033 [02:07<2:34:29,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 288, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 133/10033 [02:09<2:41:39,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 152, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 134/10033 [02:09<2:38:06,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 202, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 135/10033 [02:10<2:40:32,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 111, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 136/10033 [02:11<2:32:08,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 112, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 137/10033 [02:12<2:26:27,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 187, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 138/10033 [02:13<2:27:44,  1.12it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 208, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 139/10033 [02:14<2:32:23,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 139, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 140/10033 [02:15<2:31:56,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 109, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 141/10033 [02:16<2:26:34,  1.12it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 143, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 142/10033 [02:17<2:26:51,  1.12it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 200, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 143/10033 [02:18<2:32:05,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 237, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 144/10033 [02:19<2:36:15,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 207, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 145/10033 [02:20<2:38:24,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 143, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 146/10033 [02:20<2:35:56,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 227, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 147/10033 [02:21<2:38:23,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 215, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 148/10033 [02:22<2:40:20,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 241, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 149/10033 [02:23<2:42:01,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 191, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   1%|▏         | 150/10033 [02:24<2:38:13,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 145, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 151/10033 [02:25<2:35:20,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 180, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 152/10033 [02:26<2:33:27,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 152, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 153/10033 [02:27<2:31:37,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 221, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 154/10033 [02:28<2:35:37,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 140, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 155/10033 [02:29<2:33:31,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 236, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 156/10033 [02:30<2:37:25,  1.05it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 137, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 157/10033 [02:31<2:35:12,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 273, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 158/10033 [02:32<2:42:17,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 130, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 159/10033 [02:33<2:37:52,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 232, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 160/10033 [02:34<2:39:55,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 204, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 161/10033 [02:35<2:40:44,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 194, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 162/10033 [02:36<2:42:28,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 325, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 163/10033 [02:37<2:58:13,  1.08s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 264, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 164/10033 [02:38<2:57:56,  1.08s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 234, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 165/10033 [02:39<2:53:51,  1.06s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 153, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 166/10033 [02:40<2:45:59,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 183, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 167/10033 [02:41<2:42:03,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 162, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 168/10033 [02:42<2:38:36,  1.04it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 221, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 169/10033 [02:43<2:40:23,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 251, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 170/10033 [02:44<2:42:28,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 173, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 171/10033 [02:45<2:39:06,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 114, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 172/10033 [02:46<2:31:35,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 171, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 173/10033 [02:47<2:30:46,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 172, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 174/10033 [02:48<2:30:49,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 128, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 175/10033 [02:48<2:25:29,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 124, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 176/10033 [02:49<2:21:52,  1.16it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 156, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 177/10033 [02:50<2:23:28,  1.14it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 142, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 178/10033 [02:51<2:25:40,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 246, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 179/10033 [02:52<2:32:11,  1.08it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 282, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 180/10033 [02:53<2:40:01,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 96, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 181/10033 [02:54<2:30:22,  1.09it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 223, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 182/10033 [02:55<2:34:16,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 280, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 183/10033 [02:56<2:41:43,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 231, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 184/10033 [02:57<2:42:41,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 206, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 185/10033 [02:58<2:42:31,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 179, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 186/10033 [02:59<2:38:41,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 147, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 187/10033 [03:00<2:34:59,  1.06it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 261, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 188/10033 [03:01<2:41:54,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 253, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 189/10033 [03:02<2:43:54,  1.00it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 170, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 190/10033 [03:03<2:39:46,  1.03it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 273, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 191/10033 [03:04<2:44:59,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 243, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 192/10033 [03:05<2:45:29,  1.01s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 697, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 193/10033 [03:07<3:32:45,  1.30s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 231, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 194/10033 [03:08<3:18:30,  1.21s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 221, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 195/10033 [03:09<3:07:55,  1.15s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 163, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 196/10033 [03:10<2:56:27,  1.08s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 294, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 197/10033 [03:11<2:56:54,  1.08s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 439, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 198/10033 [03:12<3:14:15,  1.19s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 537, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 199/10033 [03:14<3:36:02,  1.32s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 351, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 200/10033 [03:15<3:35:07,  1.31s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 405, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 201/10033 [03:17<3:38:46,  1.34s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 272, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 202/10033 [03:18<3:26:28,  1.26s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 148, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 203/10033 [03:19<3:08:42,  1.15s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 198, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 204/10033 [03:20<3:00:34,  1.10s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 258, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 205/10033 [03:21<2:58:54,  1.09s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 201, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 206/10033 [03:22<2:54:21,  1.06s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 161, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 207/10033 [03:23<2:46:18,  1.02s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 174, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 208/10033 [03:24<2:41:15,  1.02it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 216, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 209/10033 [03:25<2:41:32,  1.01it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 121, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 210/10033 [03:25<2:33:03,  1.07it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 86, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 211/10033 [03:26<2:25:19,  1.13it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 120, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 212/10033 [03:27<2:22:16,  1.15it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 151, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 213/10033 [03:28<2:23:51,  1.14it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 189, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 214/10033 [03:29<2:25:48,  1.12it/s]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 405, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 215/10033 [03:30<2:49:56,  1.04s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 230, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 216/10033 [03:31<2:48:01,  1.03s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 653, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 217/10033 [03:33<3:33:12,  1.30s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 489, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 218/10033 [03:35<3:45:16,  1.38s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 581, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 219/10033 [03:36<4:00:20,  1.47s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 993, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 220/10033 [03:39<4:59:30,  1.83s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 863, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 221/10033 [03:41<5:21:13,  1.96s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 795, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 222/10033 [03:44<5:33:53,  2.04s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 412, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 223/10033 [03:45<5:02:02,  1.85s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 459, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 224/10033 [03:46<4:46:37,  1.75s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 473, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 225/10033 [03:48<4:36:14,  1.69s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 395, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 226/10033 [03:49<4:21:31,  1.60s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 541, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 227/10033 [03:51<4:23:08,  1.61s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 365, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 228/10033 [03:52<4:10:23,  1.53s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 357, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 229/10033 [03:54<4:00:50,  1.47s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 472, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 230/10033 [03:55<4:03:47,  1.49s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 233, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 231/10033 [03:56<3:40:05,  1.35s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 119, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 232/10033 [03:57<3:14:11,  1.19s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 764, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 233/10033 [03:59<3:59:50,  1.47s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 1017, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 234/10033 [04:02<4:59:03,  1.83s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 348, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 235/10033 [04:03<4:34:18,  1.68s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 329, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 236/10033 [04:04<4:16:14,  1.57s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 804, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 237/10033 [04:07<4:48:03,  1.76s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 486, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 238/10033 [04:08<4:38:06,  1.70s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 384, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 239/10033 [04:10<4:20:26,  1.60s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 386, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 240/10033 [04:11<4:10:31,  1.53s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 187, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 241/10033 [04:12<3:40:22,  1.35s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 325, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 242/10033 [04:13<3:39:09,  1.34s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 553, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 243/10033 [04:15<3:55:05,  1.44s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 290, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 244/10033 [04:16<3:38:39,  1.34s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 478, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 245/10033 [04:18<3:48:15,  1.40s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 284, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 246/10033 [04:19<3:33:28,  1.31s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 166, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 247/10033 [04:20<3:14:15,  1.19s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 307, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 248/10033 [04:21<3:09:41,  1.16s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 413, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 249/10033 [04:22<3:20:44,  1.23s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 338, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   2%|▏         | 250/10033 [04:23<3:25:03,  1.26s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 430, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 251/10033 [04:25<3:32:39,  1.30s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 259, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 252/10033 [04:26<3:21:37,  1.24s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 340, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 253/10033 [04:27<3:25:47,  1.26s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 607, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 254/10033 [04:29<3:47:05,  1.39s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 696, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 255/10033 [04:31<4:14:45,  1.56s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 436, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 256/10033 [04:32<4:07:43,  1.52s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 238, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 257/10033 [04:33<3:43:29,  1.37s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 273, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 258/10033 [04:34<3:29:24,  1.29s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 484, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 259/10033 [04:36<3:42:05,  1.36s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 467, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 260/10033 [04:37<3:50:25,  1.41s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 363, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 261/10033 [04:39<3:47:02,  1.39s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 303, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 262/10033 [04:40<3:32:44,  1.31s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 529, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 263/10033 [04:42<3:48:49,  1.41s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 464, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 264/10033 [04:43<3:54:46,  1.44s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 392, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 265/10033 [04:44<3:52:29,  1.43s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 405, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 266/10033 [04:46<3:50:29,  1.42s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 250, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 267/10033 [04:47<3:30:57,  1.30s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 231, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 268/10033 [04:48<3:17:03,  1.21s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 388, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 269/10033 [04:49<3:26:05,  1.27s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 681, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 270/10033 [04:51<3:59:56,  1.47s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 263, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 271/10033 [04:52<3:40:47,  1.36s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 278, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 272/10033 [04:53<3:27:45,  1.28s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 282, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 273/10033 [04:55<3:19:09,  1.22s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 455, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 274/10033 [04:56<3:34:00,  1.32s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 181, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 275/10033 [04:57<3:14:28,  1.20s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 361, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 276/10033 [04:58<3:21:15,  1.24s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 474, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 277/10033 [05:00<3:36:22,  1.33s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 514, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 278/10033 [05:01<3:50:41,  1.42s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 404, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 279/10033 [05:03<3:49:02,  1.41s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 184, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 280/10033 [05:04<3:24:43,  1.26s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 131, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 281/10033 [05:05<3:07:05,  1.15s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 393, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 282/10033 [05:06<3:18:20,  1.22s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 875, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 283/10033 [05:08<4:11:19,  1.55s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 453, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 284/10033 [05:10<4:11:06,  1.55s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 290, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 285/10033 [05:11<3:48:56,  1.41s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 628, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 286/10033 [05:13<4:03:16,  1.50s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 337, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 287/10033 [05:14<3:54:21,  1.44s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 425, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 288/10033 [05:15<3:53:13,  1.44s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 266, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 289/10033 [05:17<3:35:44,  1.33s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 727, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 290/10033 [05:19<4:11:32,  1.55s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 302, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 291/10033 [05:20<3:49:53,  1.42s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 438, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 292/10033 [05:21<3:49:25,  1.41s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 582, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 293/10033 [05:23<4:02:25,  1.49s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 308, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 294/10033 [05:24<3:43:27,  1.38s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 454, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 295/10033 [05:25<3:51:32,  1.43s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 241, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 296/10033 [05:26<3:31:28,  1.30s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 489, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 297/10033 [05:28<3:43:39,  1.38s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 463, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 298/10033 [05:30<3:51:44,  1.43s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 206, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 299/10033 [05:31<3:30:35,  1.30s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 152, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 300/10033 [05:31<3:11:29,  1.18s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 286, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 301/10033 [05:33<3:07:02,  1.15s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 339, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 302/10033 [05:34<3:14:48,  1.20s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 694, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 303/10033 [05:36<3:52:32,  1.43s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 475, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 304/10033 [05:37<3:57:45,  1.47s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 297, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 305/10033 [05:38<3:39:36,  1.35s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 429, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 306/10033 [05:40<3:43:11,  1.38s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 437, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 307/10033 [05:41<3:45:17,  1.39s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 344, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 308/10033 [05:43<3:42:07,  1.37s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 414, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 309/10033 [05:44<3:43:06,  1.38s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 454, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 310/10033 [05:46<3:50:48,  1.42s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 294, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 311/10033 [05:47<3:34:44,  1.33s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 254, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 312/10033 [05:48<3:19:15,  1.23s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 425, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 313/10033 [05:49<3:28:05,  1.28s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 526, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 314/10033 [05:51<3:45:40,  1.39s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 535, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 315/10033 [05:52<3:57:02,  1.46s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 277, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 316/10033 [05:53<3:37:53,  1.35s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 432, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 317/10033 [05:55<3:41:35,  1.37s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 709, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 318/10033 [05:57<4:15:44,  1.58s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 516, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 319/10033 [05:59<4:18:10,  1.59s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 370, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 320/10033 [06:00<4:05:16,  1.52s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 457, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 321/10033 [06:01<4:06:13,  1.52s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 235, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 322/10033 [06:02<3:41:49,  1.37s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 698, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 323/10033 [06:04<4:10:37,  1.55s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 306, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 324/10033 [06:05<3:49:14,  1.42s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 258, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 325/10033 [06:07<3:32:51,  1.32s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 282, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 326/10033 [06:08<3:21:16,  1.24s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 454, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 327/10033 [06:09<3:34:58,  1.33s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 374, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 328/10033 [06:11<3:35:53,  1.33s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 317, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 329/10033 [06:12<3:24:29,  1.26s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 266, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 330/10033 [06:13<3:15:17,  1.21s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 354, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 331/10033 [06:14<3:21:08,  1.24s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 215, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 332/10033 [06:15<3:09:11,  1.17s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 344, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 333/10033 [06:16<3:17:12,  1.22s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 553, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 334/10033 [06:18<3:38:39,  1.35s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 431, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 335/10033 [06:19<3:41:44,  1.37s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 439, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 336/10033 [06:21<3:44:18,  1.39s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 406, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 337/10033 [06:22<3:44:16,  1.39s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 451, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 338/10033 [06:24<3:51:16,  1.43s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 354, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 339/10033 [06:25<3:46:43,  1.40s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 314, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 340/10033 [06:26<3:32:08,  1.31s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 236, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 341/10033 [06:27<3:17:32,  1.22s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 281, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 342/10033 [06:28<3:10:43,  1.18s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 648, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 343/10033 [06:30<3:47:27,  1.41s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 649, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 344/10033 [06:32<4:14:03,  1.57s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 435, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 345/10033 [06:34<4:06:04,  1.52s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 234, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 346/10033 [06:35<3:40:54,  1.37s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 249, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 347/10033 [06:36<3:23:27,  1.26s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 414, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 348/10033 [06:37<3:30:14,  1.30s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 452, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 349/10033 [06:39<3:41:24,  1.37s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 286, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 350/10033 [06:40<3:27:49,  1.29s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 545, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   3%|▎         | 351/10033 [06:41<3:45:56,  1.40s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 495, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 352/10033 [06:43<3:52:51,  1.44s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 235, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 353/10033 [06:44<3:31:38,  1.31s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 701, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 354/10033 [06:46<4:03:24,  1.51s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 304, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 355/10033 [06:47<3:43:56,  1.39s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 299, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 356/10033 [06:48<3:29:21,  1.30s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 694, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 357/10033 [06:50<4:01:15,  1.50s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 122, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 358/10033 [06:51<3:28:00,  1.29s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 709, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 359/10033 [06:53<4:06:00,  1.53s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 603, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 360/10033 [06:55<4:13:42,  1.57s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 160, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 361/10033 [06:55<3:41:18,  1.37s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 352, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 362/10033 [06:57<3:39:02,  1.36s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 492, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 363/10033 [06:58<3:47:57,  1.41s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 171, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 364/10033 [06:59<3:23:27,  1.26s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 261, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 365/10033 [07:00<3:14:39,  1.21s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 492, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "\n",
      "\n",
      "Rows...:   4%|▎         | 366/10033 [07:02<3:31:36,  1.31s/it]\u001b[A\u001b[ASetting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 793, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for name, dataset_meta in tqdm(HUGGINGFACE_NAME_TO_DATASET.items(), desc=\"Datasets...\"):\n",
    "        for subset, split in tqdm(zip(dataset_meta[\"subsets\"], dataset_meta[\"splits\"]), desc=\"Splits...\"):\n",
    "            path = get_dataset_path(subset, name, split)\n",
    "            dataset = load_from_disk(path)\n",
    "            probs_list = []\n",
    "            a_list = []\n",
    "            for row in tqdm(dataset, desc=\"Rows...\"):\n",
    "                q = construct_prompt(row)\n",
    "                probs = calculate_next_token_probs(q, tokenizer, model)\n",
    "                probs_list.append({\n",
    "                    \"probs\": probs,\n",
    "                    \"meta\": row[\"meta\"],\n",
    "                })\n",
    "                a = get_answer(probs)\n",
    "                a_list.append({\n",
    "                    \"answer\": a,\n",
    "                    \"meta\": row[\"meta\"],\n",
    "                })\n",
    "            metric_dir_path = get_metric_dir_path(SAIGA_MISTRAL_7B_LORA, subset, name, split)\n",
    "            metric_dir_path.mkdir(exist_ok=True, parents=True)\n",
    "            with open(metric_dir_path.joinpath(\"probs.jsonl\"), \"w\") as f:\n",
    "                json.dump(probs_list, f, ensure_ascii=False, indent=2)\n",
    "            with open(metric_dir_path.joinpath(\"answers.jsonl\"), \"w\") as f:\n",
    "                json.dump(a_list, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
