{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from definitions import *\n",
    "from llm_helpers import calculate_probs_and_get_answer\n",
    "from huggingface_helpers import get_tokenizer, get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View OpenBookQA structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '8-376',\n",
       " 'question': {'stem': 'Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as',\n",
       "  'choices': [{'text': 'Deep sea animals', 'label': 'A'},\n",
       "   {'text': 'fish', 'label': 'B'},\n",
       "   {'text': 'Long Sea Fish', 'label': 'C'},\n",
       "   {'text': 'Far Sea Animals', 'label': 'D'}]},\n",
       " 'answerKey': 'A'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(DIPLOMA_DIR_PATH.joinpath(\"OpenBookQA-V1-Sep2018/Data/Main/dev.jsonl\")) as f:\n",
    "    quiz = [json.loads(x) for x in f.readlines()]\n",
    "quiz[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                                               8-376\n",
       "Question Stem        Frilled sharks and angler fish live far beneat...\n",
       "Choices              (A) Deep sea animals (B) fish (C) Long Sea Fis...\n",
       "Complete Question    Frilled sharks and angler fish live far beneat...\n",
       "Answer Key                                                           A\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DIPLOMA_DIR_PATH.joinpath(\"OpenBookQA-V1-Sep2018/Data/Main/dev.tsv\"), sep='\\t')\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                    \tID          \tSIZE  \tMODIFIED   \n",
      "dolphin-phi:latest      \tc5761fc77240\t1.6 GB\t7 days ago\t\n",
      "gemma:2b                \tb50d6c999e59\t1.7 GB\t7 days ago\t\n",
      "gemma:7b                \ta72c7f4d0a15\t5.0 GB\t7 days ago\t\n",
      "llama2:13b              \td475bf4c50bc\t7.4 GB\t7 days ago\t\n",
      "llama2:70b              \te7f6c06ffef4\t38 GB \t7 days ago\t\n",
      "llama2:latest           \t78e26419b446\t3.8 GB\t7 days ago\t\n",
      "llama2-uncensored:latest\t44040b922233\t3.8 GB\t7 days ago\t\n",
      "llama3:70b              \tbcfb190ca3a7\t39 GB \t7 days ago\t\n",
      "llama3:latest           \t71a106a91016\t4.7 GB\t7 days ago\t\n",
      "llava:latest            \t8dd30f6b0cb1\t4.7 GB\t7 days ago\t\n",
      "mistral:latest          \t61e88e884507\t4.1 GB\t7 days ago\t\n",
      "neural-chat:latest      \t89fa737d3b85\t4.1 GB\t7 days ago\t\n",
      "orca-mini:latest        \t2dbd9f439647\t2.0 GB\t7 days ago\t\n",
      "phi:latest              \te2fd6321a5fe\t1.6 GB\t7 days ago\t\n",
      "phi3:latest             \ta2c89ceaed85\t2.3 GB\t3 days ago\t\n",
      "solar:latest            \t059fdabbe6e6\t6.1 GB\t7 days ago\t\n",
      "starling-lm:latest      \t39153f619be6\t4.1 GB\t7 days ago\t\n"
     ]
    }
   ],
   "source": [
    "! ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = LLAMA_2_7B\n",
    "tokenizer = get_tokenizer(model_name, HUGGINGFACE_MODEL_TO_REPO[model_name])\n",
    "model2 = get_model(model_name, HUGGINGFACE_MODEL_TO_REPO[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = df.iloc[0][3]\n",
    "input_ids = tokenizer(input_prompt, return_tensors=\"pt\").input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model2(input_ids=input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df28d0af629480688b4caa3d19b7442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2543431d6d0c46aaafba74bb127ad7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11fb1cb3915449fa5a50b85bbe15bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96f4c1fee2047efa2743f62e4b6a2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1097d5cf573433799f977be416426ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e4fc83fcc9435e8d1c5fa0f83c9922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "model = LlamaForCausalLM.from_pretrained(HUGGINGFACE_MODEL_TO_REPO[model_name])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs2 = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab.keys().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_all = [\n",
    "    tokenizer(x, add_special_tokens=False).input_ids[-1] for x in tokenizer.vocab.keys()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_of_interest = [\n",
    "    tokenizer(\"A\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\"B\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\"C\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\"D\", add_special_tokens=False).input_ids[-1],\n",
    "    tokenizer(\" \", add_special_tokens=False).input_ids[-1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs2.logits  # shape (batch_size, sequence_length, vocab_size)\n",
    "next_token_logits = logits[:, -1, :]  # shape (batch_size, vocab_size)\n",
    "\n",
    "next_token_logits = next_token_logits.flatten()\n",
    "next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1).cpu()  # all probs over vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = next_token_probs[tokens_all].tolist()\n",
    "res = dict(zip(tokenizer.vocab.keys(), probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as (A) Deep sea animals (B) fish (C) Long Sea Fish (D) Far Sea Animals'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token_ids=input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = list(res.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = sorted(entries, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('/.', 0.3039078116416931),\n",
       " ('?.', 0.3039078116416931),\n",
       " ('=.', 0.3039078116416931),\n",
       " ('__.', 0.3039078116416931),\n",
       " ('\\\\.', 0.3039078116416931),\n",
       " ('>.', 0.3039078116416931),\n",
       " ('-.', 0.3039078116416931),\n",
       " ('_.', 0.3039078116416931),\n",
       " (',.', 0.3039078116416931),\n",
       " (')..', 0.3039078116416931),\n",
       " ('!.', 0.3039078116416931),\n",
       " ('“.', 0.3039078116416931),\n",
       " ('}.', 0.3039078116416931),\n",
       " ('{.', 0.3039078116416931),\n",
       " ('</s>', 0.057264987379312515),\n",
       " ('\"?', 0.026555825024843216),\n",
       " ('`?', 0.026555825024843216),\n",
       " (')?', 0.026555825024843216),\n",
       " ('$?', 0.026555825024843216),\n",
       " ('/?', 0.026555825024843216),\n",
       " ('▁(', 0.019481388852000237),\n",
       " ('(', 0.019481388852000237),\n",
       " ('▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', 0.006530789192765951),\n",
       " ('Answer', 0.003926899749785662),\n",
       " ('▁Answer', 0.003926899749785662),\n",
       " ('▁.', 0.002654710318893194),\n",
       " ('.', 0.002654710318893194),\n",
       " ('▁?', 0.0019710473716259003),\n",
       " ('?', 0.0019710473716259003),\n",
       " ('and', 0.0016641627298668027),\n",
       " ('▁and', 0.0016641627298668027),\n",
       " ('(...', 0.0016302078729495406),\n",
       " ('.....', 0.0016302078729495406),\n",
       " ('!...', 0.0016302078729495406),\n",
       " ('*,', 0.001529243541881442),\n",
       " ('/,', 0.001529243541881442),\n",
       " ('...,', 0.001529243541881442),\n",
       " ('>,', 0.001529243541881442),\n",
       " ('#,', 0.001529243541881442),\n",
       " (',,', 0.001529243541881442),\n",
       " ('-,', 0.001529243541881442),\n",
       " ('`,', 0.001529243541881442),\n",
       " ('“,', 0.001529243541881442),\n",
       " ('++,', 0.001529243541881442),\n",
       " ('?,', 0.001529243541881442),\n",
       " ('!,', 0.001529243541881442),\n",
       " ('.,', 0.001529243541881442),\n",
       " ('[,', 0.001529243541881442),\n",
       " ('▁Ans', 0.0011144012678414583),\n",
       " ('F', 0.000881050480529666),\n",
       " ('▁F', 0.000881050480529666),\n",
       " ('▁[', 0.0008408165303990245),\n",
       " ('[', 0.0008408165303990245),\n",
       " ('▁-', 0.0007409926038235426),\n",
       " ('-', 0.0007409926038235426),\n",
       " ('???', 0.0007280498975887895),\n",
       " ('▁Correct', 0.0006719511584378779),\n",
       " ('\\u200b', 0.0006620598724111915),\n",
       " ('The', 0.0006548941601067781),\n",
       " ('▁The', 0.0006548941601067781),\n",
       " ('Read', 0.0005393314058892429),\n",
       " ('▁Read', 0.0005393314058892429),\n",
       " ('Ex', 0.0005388132412917912),\n",
       " ('▁Ex', 0.0005388132412917912),\n",
       " ('Please', 0.0005239372840151191),\n",
       " ('▁Please', 0.0005239372840151191),\n",
       " ('–', 0.000517316919285804),\n",
       " ('▁–', 0.000517316919285804),\n",
       " ('▁Question', 0.0004952077870257199),\n",
       " ('Question', 0.0004952077870257199),\n",
       " ('▁E', 0.00044047588016837835),\n",
       " ('E', 0.00044047588016837835),\n",
       " ('▁Which', 0.0004390964168123901),\n",
       " ('Cho', 0.0004288405179977417),\n",
       " ('▁Cho', 0.0004288405179977417),\n",
       " ('!/', 0.000410017732065171),\n",
       " ('\\\\/', 0.000410017732065171),\n",
       " ('=/', 0.000410017732065171),\n",
       " (')/', 0.000410017732065171),\n",
       " (\"('/\", 0.000410017732065171),\n",
       " ('}/', 0.000410017732065171),\n",
       " ('(\"/', 0.000410017732065171),\n",
       " (':/', 0.000410017732065171),\n",
       " ('~/', 0.000410017732065171),\n",
       " ('>/', 0.000410017732065171),\n",
       " (']/', 0.000410017732065171),\n",
       " ('▁or', 0.0003830035275314003),\n",
       " ('or', 0.0003830035275314003),\n",
       " ('...', 0.00036746147088706493),\n",
       " ('▁...', 0.00036746147088706493),\n",
       " ('▁Solution', 0.000364541367162019),\n",
       " ('▁Option', 0.00032658272539265454),\n",
       " ('Option', 0.00032658272539265454),\n",
       " ('…', 0.0003232341550756246),\n",
       " ('▁…', 0.0003232341550756246),\n",
       " ('▁Exp', 0.0002957973920274526),\n",
       " ('Exp', 0.0002957973920274526),\n",
       " ('$:', 0.0002951971546281129),\n",
       " ('}:', 0.0002951971546281129),\n",
       " ('.:', 0.0002951971546281129)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPast"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 4096]), torch.Size([128]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape, outputs[1][1][1][0][1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'past_key_values'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 4096]), 32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape, len(outputs.past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frilled sharks and angler fish live far beneath the surface of the ocean, which is why they are known as (A) Deep sea animals (B) fish (C) Long Sea Fish (D) Far Sea Animals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:57<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPast' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m q \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[i, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(q)\n\u001b[0;32m----> 4\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_probs_and_get_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/study/8-semester/diploma/long_context_LLMs/notebooks/metrics_calculation/llm_helpers.py:51\u001b[0m, in \u001b[0;36mcalculate_probs_and_get_answer\u001b[0;34m(input_prompt, tokenizer, model)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_probs_and_get_answer\u001b[39m(\n\u001b[1;32m     47\u001b[0m     input_prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     48\u001b[0m     tokenizer: transformers\u001b[38;5;241m.\u001b[39mPreTrainedTokenizerBase,\n\u001b[1;32m     49\u001b[0m     model: tp\u001b[38;5;241m.\u001b[39mUnion[transformers\u001b[38;5;241m.\u001b[39mPreTrainedModel, peft\u001b[38;5;241m.\u001b[39mpeft_model\u001b[38;5;241m.\u001b[39mPeftModelForCausalLM]\n\u001b[1;32m     50\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_token_interest_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     answer \u001b[38;5;241m=\u001b[39m get_answer(probs)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "File \u001b[0;32m~/study/8-semester/diploma/long_context_LLMs/notebooks/metrics_calculation/llm_helpers.py:16\u001b[0m, in \u001b[0;36mcalculate_token_interest_probs\u001b[0;34m(input_prompt, tokenizer, model)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids)\n\u001b[0;32m---> 16\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m  \u001b[38;5;66;03m# shape (batch_size, sequence_length, vocab_size)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# shape (batch_size, vocab_size)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m next_token_logits\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPast' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "for i in trange(len(df)):\n",
    "    q = df.iloc[i, 3]\n",
    "    print(q)\n",
    "    a = calculate_probs_and_get_answer(q, tokenizer, model)\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
